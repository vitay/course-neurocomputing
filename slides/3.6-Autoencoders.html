<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.5.57">

  <meta name="author" content="Julien Vitay">
  <title>Neurocomputing</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="img/tuc-new-large.png" data-background-opacity="1" data-background-position="top" data-background-size="30%" class="quarto-title-block center">
  <h1 class="title">Neurocomputing</h1>
  <p class="subtitle">Autoencoders</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Julien Vitay 
</div>
        <p class="quarto-title-affiliation">
            Professur für Künstliche Intelligenz - Fakultät für Informatik
          </p>
    </div>
</div>

</section>
<section id="autoencoders" class="title-slide slide level1 center">
<h1>1 - Autoencoders</h1>

</section>

<section id="labeled-vs-unlabeled-data" class="title-slide slide level1 center">
<h1>Labeled vs unlabeled data</h1>
<ul>
<li><p><strong>Supervised learning</strong> algorithms need a lot of labeled data (with <span class="math inline">\mathbf{t}</span>) in order to learn classification/regression tasks, but labeled data is very expensive to obtain (experts, crowd sourcing).</p></li>
<li><p>A “bad” algorithm trained with a lot of data will perform better than a “good” algorithm trained with few data. <em>“It is not who has the best algorithm who wins, it is who has the most data.”</em></p></li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Supervised learning</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/supervised.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p><strong>Self-taught learning</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/selftaught.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<ul>
<li>Unlabeled data is only useful for <strong>unsupervised learning</strong>, but very cheap to obtain (camera, microphone, search engines). Can we combine efficiently both approaches? <strong>Self-taught learning</strong> or <strong>semi-supervised learning</strong>.</li>
</ul>
</section>

<section id="autoencoders-1" class="title-slide slide level1 center">
<h1>Autoencoders</h1>
<ul>
<li>An <strong>autoencoder</strong> is a NN trying to learn the identity function <span class="math inline">f(\mathbf{x}) = \mathbf{x}</span> using a different number of neurons in the hidden layer than in the input layer.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/autoencoder3.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>An autoencoder minimizes the <strong>reconstruction loss</strong> between the input <span class="math inline">\mathbf{x}</span> and the reconstruction <span class="math inline">\mathbf{x'}</span>, for example the mse between the two vectors:</li>
</ul>
<p><span class="math display">
    \mathcal{L}_\text{reconstruction}(\theta) = \mathbb{E}_{\mathbf{x} \in \mathcal{D}} [ ||\mathbf{x'} - \mathbf{x}||^2 ]
</span></p>
<ul>
<li><p>An autoencoder uses <strong>unsupervised learning</strong>: the output data used for learning is the same as the input data.</p>
<ul>
<li>No need for labels!</li>
</ul></li>
</ul>
</div></div>
<ul>
<li><p>By forcing the projection of the input data on a feature space with less dimensions (<strong>latent space</strong>), the network has to extract relevant <strong>features</strong> from the training data.</p>
<ul>
<li>Dimensionality reduction, compression.</li>
</ul></li>
</ul>
</section>

<section id="result-of-training-a-sparse-autoencoder-on-natural-images" class="title-slide slide level1 center">
<h1>Result of training a sparse autoencoder on natural images</h1>
<ul>
<li><p>If the latent space has more dimensions than the input space, we need to <strong>constrain</strong> the autoencoder so that it does not simply learn the identity mapping.</p></li>
<li><p>Below is an example of a sparse autoencoder trained on natural images.</p></li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/result-autoencoder.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Inputs are taken from random natural images and cut in 10*10 patches.</p></li>
<li><p>100 features are extracted in the hidden layer.</p></li>
<li><p>The autoencoder is said <strong>sparse</strong> because it uses <strong>L1-regularization</strong> to make sure that only a few neurons are active in the hidden layer for a particular image.</p></li>
<li><p>The learned features look like what the first layer of a CNN would learn, except that there was no labels at all!</p></li>
<li><p>Can we take advantage of this to pre-train a supervised network?</p></li>
</ul>
</div></div>
<div class="footer">
<p>Olshausen and Field (1997). Sparse Coding with an Overcomplete Basis Set: A Strategy Employed by V1? Vision Research 37(23).</p>
</div>
</section>

<section id="stacked-autoencoders" class="title-slide slide level1 center">
<h1>2 - Stacked autoencoders</h1>

</section>

<section id="using-an-autoencoder-for-supervised-learning" class="title-slide slide level1 center">
<h1>Using an autoencoder for supervised learning</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/deep.svg" class="quarto-figure quarto-figure-center" style="width:60.0%"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>In supervised learning, deep neural networks suffer from many problems:</p>
<ul>
<li><p>Local minima</p></li>
<li><p>Vanishing gradients</p></li>
<li><p>Long training times</p></li>
</ul></li>
</ul>
</div></div>
<ul>
<li><p>All these problems are due to the fact that the weights are randomly initialized at the beginning of training.</p></li>
<li><p><strong>Pretraining</strong> the weights using unsupervised learning allows to start already close to a good solution:</p>
<ul>
<li><p>the network will need less steps to converge.</p></li>
<li><p>the gradients will vanish less.</p></li>
<li><p>less data is needed to learn a particular supervised task.</p></li>
</ul></li>
</ul>
</section>

<section id="stacked-autoencoders-1" class="title-slide slide level1 center">
<h1>Stacked autoencoders</h1>
<ul>
<li>Let’s try to learn a <strong>stacked autoencoder</strong> by learning <em>progressively</em> each feature vector.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/stacked-autoencoder.png" style="width:60.0%"></p>
<figcaption><a href="http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders" class="uri">http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders</a></figcaption>
</figure>
</div>
</section>

<section id="stacked-autoencoders-2" class="title-slide slide level1 center">
<h1>Stacked autoencoders</h1>
<ul>
<li>Using unlabeled data, train an autoencoder to extract first-order features, freeze the weights and remove the decoder.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/stacked1.png" style="width:35.0%"></p>
<figcaption><a href="http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders" class="uri">http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders</a></figcaption>
</figure>
</div>
</section>

<section id="stacked-autoencoders-3" class="title-slide slide level1 center">
<h1>Stacked autoencoders</h1>
<ul>
<li>Train another autoencoder on the same unlabeled data, but using the previous latent space as input/output.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/stacked2.png"></p>
<figcaption><a href="http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders" class="uri">http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders</a></figcaption>
</figure>
</div>
</section>

<section id="stacked-autoencoders-4" class="title-slide slide level1 center">
<h1>Stacked autoencoders</h1>
<ul>
<li>Repeat the operation as often as needed, and finish with a simple classifier using the labeled data.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/stacked3.png"></p>
<figcaption><a href="http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders" class="uri">http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders</a></figcaption>
</figure>
</div>
</section>

<section id="greedy-layer-wise-learning" class="title-slide slide level1 center">
<h1>Greedy layer-wise learning</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/stacked-autoencoder.png"></p>
<figcaption><a href="http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders" class="uri">http://ufldl.stanford.edu/wiki/index.php/Stacked_Autoencoders</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>This defines a <strong>stacked autoencoder</strong>, trained using <strong>Greedy layer-wise</strong> learning.</p></li>
<li><p>Each layer progressively learns more and more complex features of the input data (edges - contour - forms - objects): <strong>feature extraction</strong>.</p></li>
<li><p>This method allows to train a deep network on few labeled data: the network will not overfit, because the weights are already in the right region.</p></li>
<li><p>It solves <strong>gradient vanishing</strong>, as the weights are already close to the optimal solution and will efficiently transmit the gradient backwards.</p></li>
<li><p>One can keep the pre-trained weights fixed for the classification task or <strong>fine-tune</strong> all the weights as in a regular DNN.</p></li>
</ul>
</div></div>
</section>

<section id="application-finding-cats-on-the-internet" class="title-slide slide level1 center">
<h1>Application: Finding cats on the internet</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/catnetwork.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Andrew Ng and colleagues (Google, Stanford) used a similar technique to train a deep belief network on color images (200x200) taken from 10 million random unlabeled Youtube videos.</p></li>
<li><p>Each layer was trained greedily. They used a particular form of autoencoder called <strong>restricted Boltzmann machines</strong> (RBM) and a couple of other tricks (receptive fields, contrast normalization).</p></li>
<li><p>Training was distributed over 1000 machines (16.000 cores) and lasted for three days.</p></li>
<li><p>There was absolutely no task: the network just had to watch youtube videos.</p></li>
<li><p>After learning, they visualized what the neurons had learned.</p></li>
</ul>
</div></div>
<div class="footer">
<p>Quoc Le et al.&nbsp;(2013). Building High-level Features Using Large Scale Unsupervised Learning. ICASSP. <a href="http://ieeexplore.ieee.org/document/6639343" class="uri">http://ieeexplore.ieee.org/document/6639343</a></p>
</div>
</section>

<section id="application-finding-cats-on-the-internet-1" class="title-slide slide level1 center">
<h1>Application: Finding cats on the internet</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/catfinder.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/catneuron.jpg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<ul>
<li><p>After training, some neurons had learned to respond uniquely to faces, or to cats, without ever having been instructed to.</p></li>
<li><p>The network can then be fine-tuned for classification tasks, improving the pre-AlexNet state-of-the-art on ImageNet by 70%.</p></li>
</ul>
<div class="footer">
<p>Quoc Le et al.&nbsp;(2013). Building High-level Features Using Large Scale Unsupervised Learning. ICASSP.</p>
</div>
</section>

<section id="deep-autoencoders" class="title-slide slide level1 center">
<h1>3 - Deep autoencoders</h1>

</section>

<section id="deep-autoencoders-1" class="title-slide slide level1 center">
<h1>Deep autoencoders</h1>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li><p>Autoencoders are not restricted to a single hidden layer.</p></li>
<li><p>The <strong>encoder</strong> goes from the input space <span class="math inline">\mathbf{x}</span> to the latent space <span class="math inline">\mathbf{z}</span>.</p></li>
</ul>
<p><span class="math display">
    \mathbf{z} = g_\phi(\mathbf{x})
</span></p>
<ul>
<li>The <strong>decoder</strong> goes from the latent space <span class="math inline">\mathbf{z}</span> to the output space <span class="math inline">\mathbf{x'}</span>.</li>
</ul>
<p><span class="math display">
    \mathbf{x'} = f_\theta(\mathbf{z})
</span></p>
</div><div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/autoencoder-architecture.png"></p>
<figcaption>Source: <a href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html" class="uri">https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html</a></figcaption>
</figure>
</div>
</div></div>
<ul>
<li><p>The <strong>latent space</strong> is a <strong>bottleneck</strong> layer of lower dimensionality, learning a compressed representation of the input which has to contain enough information in order to <strong>reconstruct</strong> the input.</p></li>
<li><p>Both the encoder with weights <span class="math inline">\phi</span> and the decoder with weights <span class="math inline">\theta</span> try to minimize the <strong>reconstruction loss</strong>:</p></li>
</ul>
<p><span class="math display">
\mathcal{L}_\text{reconstruction}(\theta, \phi) = \mathbb{E}_{\mathbf{x} \in \mathcal{D}} [ ||f_\theta(g_\phi(\mathbf{x})) - \mathbf{x}||^2 ]
</span></p>
<ul>
<li>Learning is <strong>unsupervised</strong>: we only need input data.</li>
</ul>
</section>

<section id="deep-autoencoders-2" class="title-slide slide level1 center">
<h1>Deep autoencoders</h1>
<ul>
<li><p>The encoder and decoder can be anything: fully-connected, convolutional, recurrent, etc.</p></li>
<li><p>When using convolutional layers, the decoder has to <strong>upsample</strong> the latent space: max-unpooling or transposed convolutions can be used as in segmentation networks.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/convolutionalAE.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
<div class="footer">
<p>Guo X, Liu X, Zhu E, Yin J. (2017). Deep Clustering with Convolutional Autoencoders. Neural Information Processing. doi:10.1007/978-3-319-70096-0_39</p>
</div>
</section>

<section id="semi-supervised-learning" class="title-slide slide level1 center">
<h1>Semi-supervised learning</h1>
<ul>
<li>In <strong>semi-supervised</strong> or <strong>self-taught</strong> learning, we can first train an autoencoder on huge amounts of unlabeled data, and then use the latent representations as an input to a shallow classifier on a small supervised dataset.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/semisupervised-autoencoder.png"></p>
<figcaption>Source: <a href="https://doi.org/10.1117/12.2303912" class="uri">https://doi.org/10.1117/12.2303912</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/selftaught.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<ul>
<li><p>A linear classifier might even be enough if the latent space is well trained.</p></li>
<li><p>The weights of the encoder can be fine-tuned with backpropagation, or remain fixed.</p></li>
</ul>
</section>

<section id="denoising-autoencoder" class="title-slide slide level1 center">
<h1>Denoising autoencoder</h1>
<ul>
<li>A <strong>denoising autoencoder</strong> (DAE) is trained with noisy inputs (some pixels are dropped) but perfect desired outputs. It learns to suppress that noise.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/denoisingautoencoder.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/denoisingautoencoder-result.png"></p>
<figcaption>Source : <a href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html" class="uri">https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html</a></figcaption>
</figure>
</div>
<div class="footer">
<p>Vincent et al.&nbsp;(2010). “Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion”. JMLR.</p>
</div>
</section>

<section id="deep-clustering" class="title-slide slide level1 center">
<h1>Deep clustering</h1>
<ul>
<li><p><strong>Clustering</strong> algorithms (k-means, Gaussian Mixture Models, spectral clustering, etc) can be applied in the latent space to group data points into clusters.</p></li>
<li><p>If you are lucky, the clusters may even correspond to classes.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/deepclustering.jpg"></p>
<figcaption>Source: <a href="doi:10.1007/978-3-030-32520-6_55" class="uri">doi:10.1007/978-3-030-32520-6_55</a></figcaption>
</figure>
</div>
</section>

<section id="variational-autoencoders-vae" class="title-slide slide level1 center">
<h1>4 - Variational autoencoders (VAE)</h1>

</section>

<section id="motivation" class="title-slide slide level1 center">
<h1>Motivation</h1>
<ul>
<li><p>Autoencoders are <strong>deterministic</strong>: after learning, the same input <span class="math inline">\mathbf{x}</span> will generate the same latent code <span class="math inline">\mathbf{z}</span> and the same reconstruction <span class="math inline">\mathbf{\tilde{x}}</span>.</p></li>
<li><p>Sampling the latent space generally generates non-sense reconstructions, because an autoencoder only learns data samples, it does not learn the underlying <strong>probability distribution</strong>.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/autoencoder-limits.png"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" class="uri">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></figcaption>
</figure>
</div>
</section>

<section id="data-augmentation-with-autoencoders" class="title-slide slide level1 center">
<h1>Data augmentation with autoencoders</h1>
<ul>
<li><p>The main problem of supervised learning is to get enough annotated data.</p></li>
<li><p>Being able to generate <strong>new</strong> images similar to the training examples would be extremely useful (data augmentation).</p></li>
</ul>
<div class="columns">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vae-mnist.png"></p>
<figcaption>Source: <a href="https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df" class="uri">https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:55%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vae-faces.jpg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>

<section id="regularized-latent-space" class="title-slide slide level1 center">
<h1>Regularized latent space</h1>
<ul>
<li><p>In order for this to work, we need to <strong>regularize</strong> the latent space:</p>
<ul>
<li>Close points in the latent space should correspond to close images.</li>
</ul></li>
<li><p>“Classical” L1 or L2 regularization does not ensure the regularity of the latent space.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vae-latentspace.png"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" class="uri">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></figcaption>
</figure>
</div>
</section>

<section id="variational-autoencoder" class="title-slide slide level1 center">
<h1>Variational autoencoder</h1>
<ul>
<li><p>The <strong>variational autoencoder</strong> (VAE) (Kingma and Ba, 2013) solves this problem by having the encoder represent the <strong>probability distribution</strong> <span class="math inline">q_\phi(\mathbf{z}|\mathbf{x})</span> instead of a point <span class="math inline">\mathbf{z}</span> in the latent space.</p></li>
<li><p>This probability distribution is then <strong>sampled</strong> to obtain a vector <span class="math inline">\mathbf{z}</span> that will be passed to the decoder <span class="math inline">p_\theta(\mathbf{z})</span>.</p></li>
<li><p>The strong hypothesis is that the latent space follows a <strong>normal distribution</strong> with mean <span class="math inline">\mathbf{\mu_x}</span> and variance <span class="math inline">\mathbf{\sigma_x}^2</span>. <span class="math display">
  \mathbf{z} \sim \mathcal{N}(\mathbf{\mu_x}, \mathbf{\sigma_x}^2)
</span></p></li>
<li><p>The two vectors <span class="math inline">\mathbf{\mu_x}</span> and <span class="math inline">\mathbf{\sigma_x}^2</span> are the outputs of the encoder.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vae-structure.png" style="width:80.0%"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" class="uri">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></figcaption>
</figure>
</div>
<div class="footer">
<p>Kingma, D. P., and Welling, M. (2013). Auto-Encoding Variational Bayes. arXiv:1312.6114</p>
</div>
</section>

<section id="sampling-from-a-normal-distribution" class="title-slide slide level1 center">
<h1>Sampling from a normal distribution</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/normaldistribution.svg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>The normal distribution <span class="math inline">\mathcal{N}(\mu, \sigma^2)</span> is fully defined by its two parameters:</p>
<ul>
<li><p><span class="math inline">\mu</span> is the mean of the distribution.</p></li>
<li><p><span class="math inline">\sigma^2</span> is its variance.</p></li>
</ul></li>
</ul>
</div></div>
<ul>
<li>The <strong>probability density function</strong> (pdf) of the normal distribution is defined by the Gaussian function:</li>
</ul>
<p><span class="math display">
    f(x; \mu, \sigma) = \frac{1}{\sqrt{2\,\pi\,\sigma^2}} \, e^{-\displaystyle\frac{(x - \mu)^2}{2\,\sigma^2}}
</span></p>
<ul>
<li><p>A sample <span class="math inline">x</span> will likely be close to <span class="math inline">\mu</span>, with a deviation defined by <span class="math inline">\sigma^2</span>.</p></li>
<li><p>It can be obtained using a sample of the <strong>standard normal distribution</strong> <span class="math inline">\mathcal{N}(0, 1)</span>:</p></li>
</ul>
<p><span class="math display">x = \mu + \sigma \, \xi \; \; \text{with} \; \xi \sim \mathcal{N}(0, 1)</span></p>
</section>

<section id="variational-autoencoder-1" class="title-slide slide level1 center">
<h1>Variational autoencoder</h1>
<ul>
<li><p>Architecture of the VAE:</p>
<ol type="1">
<li><p>The encoder <span class="math inline">q_\phi(\mathbf{z}|\mathbf{x})</span> outputs the parameters <span class="math inline">\mathbf{\mu_x}</span> and <span class="math inline">\mathbf{\sigma_x}^2</span> of a normal distribution <span class="math inline">\mathcal{N}(\mathbf{\mu_x}, \mathbf{\sigma_x}^2)</span>.</p></li>
<li><p>We sample one vector <span class="math inline">\mathbf{z}</span> from this distribution: <span class="math inline">\mathbf{z} \sim \mathcal{N}(\mathbf{\mu_x}, \mathbf{\sigma_x}^2)</span>.</p></li>
<li><p>The decoder <span class="math inline">p_\theta(\mathbf{z})</span> reconstructs the input.</p></li>
</ol></li>
<li><p>Open questions:</p>
<ol type="1">
<li><p>Which loss should we use and how do we regularize?</p></li>
<li><p>Does backpropagation still work?</p></li>
</ol></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vae-structure.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></p>
</figure>
</div>
</section>

<section id="loss-function-of-a-vae" class="title-slide slide level1 center">
<h1>Loss function of a VAE</h1>
<ul>
<li>The <strong>loss function</strong> used in a VAE is of the form:</li>
</ul>
<p><span class="math display">
    \mathcal{L}(\theta, \phi) = \mathcal{L}_\text{reconstruction}(\theta, \phi) + \mathcal{L}_\text{regularization}(\phi)
</span></p>
<ul>
<li><p>The first term is the usual <strong>reconstruction loss</strong> of an autoencoder which depends on both the encoder and the decoder.</p></li>
<li><p>One could simply compute the <strong>mse</strong> (summed over all pixels) between the input and the reconstruction:</p></li>
</ul>
<p><span class="math display"> \mathcal{L}_\text{reconstruction}(\theta, \phi) = \mathbb{E}_{\mathbf{x} \in \mathcal{D}, \mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} [ ||p_\theta(\mathbf{z}) - \mathbf{x}||^2 ]</span></p>
<ul>
<li><p>In the expectation, <span class="math inline">\mathbf{x}</span> is sampled from the dataset <span class="math inline">\mathcal{D}</span> while <span class="math inline">\mathbf{z}</span> is sampled from the encoder <span class="math inline">q_\phi(\mathbf{z}|\mathbf{x})</span>.</p></li>
<li><p>In (Kingma et al., 2013), pixels values are normalized between 0 and 1, the decoder uses the logistic activation function for its output layer and the binary cross-entropy loss function is used:</p></li>
</ul>
<p><span class="math display"> \mathcal{L}_\text{reconstruction}(\theta, \phi) = \mathbb{E}_{\mathbf{x} \in \mathcal{D}, \mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} [ - \log p_\theta(\mathbf{z})]</span></p>
<ul>
<li>The justification comes from variational inference and evidence lower-bound optimization (ELBO) but is out of the scope of this lecture.</li>
</ul>
</section>

<section id="regularization-term" class="title-slide slide level1 center">
<h1>Regularization term</h1>
<ul>
<li>The second term is the <strong>regularization term</strong> for the latent space, which only depends on the encoder with weights <span class="math inline">\phi</span>:</li>
</ul>
<p><span class="math display">
    \mathcal{L}_\text{regularization}(\phi) = \text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || \mathcal{N}(\mathbf{0}, \mathbf{1})) = \text{KL}(\mathcal{N}(\mathbf{\mu_x}, \mathbf{\sigma_x}^2) || \mathcal{N}(\mathbf{0}, \mathbf{1}))
</span></p>
<ul>
<li><p>It is defined as the <strong>Kullback-Leibler divergence</strong> between the output of the encoder and the standard normal distribution <span class="math inline">\mathcal{N}(\mathbf{0}, \mathbf{1})</span>.</p></li>
<li><p>Think of it as a statistical “distance” between the distribution <span class="math inline">q_\phi(\mathbf{z}|\mathbf{x})</span> and the distribution <span class="math inline">\mathcal{N}(\mathbf{0}, \mathbf{1})</span>.</p></li>
<li><p>The principle is not very different from L2-regularization, where we want the weights to be as close as possible from 0.</p></li>
<li><p>Here we want the encoder to be as close as possible from <span class="math inline">\mathcal{N}(\mathbf{0}, \mathbf{1})</span>.</p></li>
</ul>
</section>

<section id="regularization-term-1" class="title-slide slide level1 center">
<h1>Regularization term</h1>
<ul>
<li><p>Why do we want the latent distributions to be close from <span class="math inline">\mathcal{N}(\mathbf{0}, \mathbf{1})</span> for <strong>all</strong> inputs <span class="math inline">\mathbf{x}</span>? <span class="math display">
  \mathcal{L}(\theta, \phi) = \mathcal{L}_\text{reconstruction}(\theta, \phi) + \text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || \mathcal{N}(\mathbf{0}, \mathbf{1}))
</span></p></li>
<li><p>By forcing the distributions to be close, we avoid “holes” in the latent space: we can move smoothly from one distribution to another without generating <strong>non-sense</strong> reconstructions.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vae-regularization.png"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" class="uri">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></figcaption>
</figure>
</div>
</section>

<section id="why-not-regularize-the-mean-and-variance" class="title-slide slide level1 center">
<h1>Why not regularize the mean and variance?</h1>
<ul>
<li>To make <span class="math inline">q_\phi(\mathbf{z}|\mathbf{x})</span> close from <span class="math inline">\mathcal{N}(\mathbf{0}, \mathbf{1})</span>, one could minimize the Euclidian distance in the <strong>parameter space</strong>:</li>
</ul>
<p><span class="math display">
    \mathcal{L}(\theta, \phi) = \mathcal{L}_\text{reconstruction}(\theta, \phi) +  (||\mathbf{\mu_x}||^2 + ||\mathbf{\sigma_x} - 1||^2)
</span></p>
<ul>
<li><p>However, this does not consider the <strong>overlap</strong> between the distributions.</p></li>
<li><p>The two pairs of distributions below have the same distance between their means (0 and 1) and the same variance (1 and 10 respectively).</p></li>
<li><p>The distributions on the left are very different from each other, but the distance in the parameter space is the same.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/naturalgradient.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></p>
</figure>
</div>
</section>

<section id="kullback-leibler-divergence" class="title-slide slide level1 center">
<h1>Kullback-Leibler divergence</h1>
<ul>
<li><p>The <strong>KL divergence</strong> between two random distributions <span class="math inline">X</span> and <span class="math inline">Y</span> measures the <strong>statistical distance</strong> between them.</p></li>
<li><p>It describes, on average, how likely a sample from <span class="math inline">X</span> could come from <span class="math inline">Y</span>:</p></li>
</ul>
<p><span class="math display">
    \text{KL}(X ||Y) = \mathbb{E}_{x \sim X}[- \log \frac{P(Y=x)}{P(X=x)}]
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/crossentropy.svg" class="quarto-figure quarto-figure-center" style="width:70.0%"></p>
</figure>
</div>
<ul>
<li><p>When the two distributions are equal almost anywhere, the KL divergence is 0. Otherwise it is positive.</p></li>
<li><p><strong>Minimizing the KL divergence between two distributions makes them close in the statistical sense</strong>.</p></li>
</ul>
</section>

<section id="kullback-leibler-divergence-1" class="title-slide slide level1 center">
<h1>Kullback-Leibler divergence</h1>
<ul>
<li>The advantage of minimizing the KL of <span class="math inline">q_\phi(\mathbf{z}|\mathbf{x})</span> with <span class="math inline">\mathcal{N}(0, 1)</span> is that the KL takes a <strong>closed form</strong> when the distributions are normal, i.e.&nbsp;there is no need to compute the expectation over all possible latent representations <span class="math inline">\mathbf{z}</span>:</li>
</ul>
<p><span class="math display">
    \mathcal{L}_\text{regularization}(\phi) = \text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || \mathcal{N}(\mathbf{0}, \mathbf{1})) = \mathbb{E}_{\mathbf{x} \in \mathcal{D}, \mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})}[- \log \frac{f_{0, 1}(\mathbf{z}|\mathbf{x})}{q_\phi(\mathbf{z}|\mathbf{x})}]
</span></p>
<ul>
<li>If <span class="math inline">\mathbf{\mu_x}</span> and <span class="math inline">\mathbf{\sigma_x}</span> have <span class="math inline">K</span> elements (dimension of the latent space), the KL can be expressed as:</li>
</ul>
<p><span class="math display">
    \mathcal{L}_\text{regularization}(\phi) = \mathbb{E}_{\mathbf{x} \in \mathcal{D}}[\dfrac{1}{2} \, \sum_{k=1}^K (\mathbf{\sigma_x^2} + \mathbf{\mu_x}^2 -1 - \log \mathbf{\sigma_x^2})]
</span></p>
<ul>
<li><p>The KL is very easy to differentiate w.r.t <span class="math inline">\mathbf{\mu_x}</span> and <span class="math inline">\mathbf{\sigma_x}</span>, i.e.&nbsp;w.r.t <span class="math inline">\phi</span>!</p></li>
<li><p>In practice, the encoder predicts the vectors <span class="math inline">\mathbf{\mu_x}</span> and <span class="math inline">\Sigma_\mathbf{x} = \log \mathbf{\sigma_x^2}</span>, so the loss becomes:</p></li>
</ul>
<p><span class="math display">
    \mathcal{L}_\text{regularization}(\phi) = \dfrac{1}{2} \, \sum_{k=1}^K (\exp \Sigma_\mathbf{x} + \mathbf{\mu_x}^2 -1 - \Sigma_\mathbf{x})
</span></p>
<div class="footer">
<p>See <a href="https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/" class="uri">https://wiseodd.github.io/techblog/2016/12/10/variational-autoencoder/</a></p>
</div>
</section>

<section id="regularization" class="title-slide slide level1 center">
<h1>Regularization</h1>
<ul>
<li><p>Regularization tends to create a “gradient” over the information encoded in the latent space.</p></li>
<li><p>A point of the latent space sampled between the means of two encoded distributions should be decoded in an image in between the two training images.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vae-regularization2.png" class="quarto-figure quarto-figure-center" style="width:65.0%"></p>
</figure>
</div>
</section>

<section id="reparameterization-trick" class="title-slide slide level1 center">
<h1>Reparameterization trick</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>The second problem is that backpropagation does not work through the sampling operation.</p></li>
<li><p>It is easy to backpropagate the gradient of the loss function through the decoder until the sample <span class="math inline">\mathbf{z}</span>.</p></li>
<li><p>But how do you backpropagate to the outputs of the encoder: <span class="math inline">\mathbf{\mu_x}</span> and <span class="math inline">\mathbf{\sigma_x}</span>?</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vae-structure.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Modifying slightly <span class="math inline">\mathbf{\mu_x}</span> or <span class="math inline">\mathbf{\sigma_x}</span> may not change at all the sample <span class="math inline">\mathbf{z} \sim \mathcal{N}(\mathbf{\mu_x}, \mathbf{\sigma_x}^2)</span>, so you cannot estimate any gradient.</li>
</ul>
<p><span class="math display">\frac{\partial \mathbf{z}}{\partial \mathbf{\mu_x}} = \; ?</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/normaldistribution.svg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>

<section id="reparameterization-trick-1" class="title-slide slide level1 center">
<h1>Reparameterization trick</h1>
<ul>
<li>Backpropagation does not work through a <strong>sampling</strong> operation, because it is not differentiable.</li>
</ul>
<p><span class="math display">\mathbf{z} \sim \mathcal{N}(\mathbf{\mu_x}, \mathbf{\sigma_x}^2)</span></p>
<ul>
<li>The <strong>reparameterization trick</strong> consists in taking a sample <span class="math inline">\xi</span> out of <span class="math inline">\mathcal{N}(0, 1)</span> and reconstruct <span class="math inline">\mathbf{z}</span> with:</li>
</ul>
<p><span class="math display">\mathbf{z} = \mathbf{\mu_x} + \mathbf{\sigma_x} \, \xi \qquad \text{with} \qquad \xi \sim \mathcal{N}(0, 1)</span></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vae-reparameterization.png" style="width:80.0%"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" class="uri">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></figcaption>
</figure>
</div>
</section>

<section id="reparameterization-trick-2" class="title-slide slide level1 center">
<h1>Reparameterization trick</h1>
<ul>
<li>The sampled value <span class="math inline">\xi \sim \mathcal{N}(0, 1)</span> becomes just another input to the neural network.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vae-reparameterization2.png" style="width:60.0%"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73" class="uri">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></figcaption>
</figure>
</div>
<ul>
<li>It allows to transform <span class="math inline">\mathbf{\mu_x}</span> and <span class="math inline">\mathbf{\sigma_x}</span> into a sample <span class="math inline">\mathbf{z}</span> of <span class="math inline">\mathcal{N}(\mathbf{\mu_x}, \mathbf{\sigma_x}^2)</span>:</li>
</ul>
<p><span class="math display">\mathbf{z} = \mathbf{\mu_x} + \mathbf{\sigma_x} \, \xi</span></p>
<ul>
<li><p>We do not need to backpropagate through <span class="math inline">\xi</span>, as there is no parameter to learn!</p></li>
<li><p>The neural network becomes differentiable end-to-end, backpropagation will work.</p></li>
</ul>
</section>

<section id="variational-autoencoder-2" class="title-slide slide level1 center">
<h1>Variational autoencoder</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>A variational autoencoder is an autoencoder where the latent space represents a probability distribution <span class="math inline">q_\phi(\mathbf{z} | \mathbf{x})</span> using the mean <span class="math inline">\mathbf{\mu_x}</span> and standard deviation <span class="math inline">\mathbf{\sigma_x}</span> of a normal distribution.</p></li>
<li><p>The latent space can be sampled to generate new images using the decoder <span class="math inline">p_\theta(\mathbf{z})</span>.</p></li>
<li><p>KL regularization and the reparameterization trick are essential to VAE.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vae-structure.svg"></p>
<figcaption>Source: <a href="https://ijdykeman.github.io/ml/2016/12/21/cvae.html" class="uri">https://ijdykeman.github.io/ml/2016/12/21/cvae.html</a></figcaption>
</figure>
</div>
</div></div>
<p><span class="math display">\begin{aligned}
    \mathcal{L}(\theta, \phi) &amp;= \mathcal{L}_\text{reconstruction}(\theta, \phi) + \mathcal{L}_\text{regularization}(\phi) \\
    &amp;= \mathbb{E}_{\mathbf{x} \in \mathcal{D}, \xi \sim \mathcal{N}(0, 1)} [ - \log p_\theta(\mathbf{\mu_x} + \mathbf{\sigma_x} \, \xi) + \dfrac{1}{2} \, \sum_{k=1}^K (\mathbf{\sigma_x^2} + \mathbf{\mu_x}^2 -1 - \log \mathbf{\sigma_x^2})] \\
\end{aligned}</span></p>
</section>

<section id="variational-autoencoder-3" class="title-slide slide level1 center">
<h1>Variational autoencoder</h1>
<ul>
<li>The two main applications of VAEs in <strong>unsupervised learning</strong> are:</li>
</ul>
<ol type="1">
<li><p><strong>Dimensionality reduction</strong>: projecting high dimensional data (images) onto a smaller space, for example a 2D space for visualization.</p></li>
<li><p><strong>Generative modeling</strong>: generating samples from the same distribution as the training data (data augmentation, deep fakes) by sampling on the manifold.</p></li>
</ol>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vae_mnist.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vae_digits_manifold.png" style="width:80.0%"></p>
<figcaption>Source: <a href="https://blog.keras.io/building-autoencoders-in-keras.html" class="uri">https://blog.keras.io/building-autoencoders-in-keras.html</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="deepfake" class="title-slide slide level1 center">
<h1>DeepFake</h1>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/JbzVhzNaTdI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<p><a href="https://github.com/iperov/DeepFaceLab" class="uri">https://github.com/iperov/DeepFaceLab</a></p>
</section>

<section id="deepfake-1" class="title-slide slide level1 center">
<h1>DeepFake</h1>
<ul>
<li>During training, <strong>one</strong> encoder and <strong>two</strong> decoders learns to reproduce the face of each person.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/deepfakes_01.png" class="quarto-figure quarto-figure-center" style="width:40.0%"></p>
</figure>
</div>
<ul>
<li>When generating the deepfake, the decoder of person B is used on the latent representation of person A.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/deepfakes_02.png" class="quarto-figure quarto-figure-center" style="width:40.0%"></p>
</figure>
</div>
<div class="footer">
<p><a href="https://www.alanzucconi.com/2018/03/14/understanding-the-technology-behind-deepfakes/" class="uri">https://www.alanzucconi.com/2018/03/14/understanding-the-technology-behind-deepfakes/</a></p>
</div>
</section>

<section id="beta-vae" class="title-slide slide level1 center">
<h1><span class="math inline">\beta</span>-VAE</h1>
<ul>
<li>VAE does not use a regularization parameter to balance the reconstruction and regularization losses. What happens if you do?</li>
</ul>
<p><span class="math display">\begin{aligned}
    \mathcal{L}(\theta, \phi) &amp;= \mathcal{L}_\text{reconstruction}(\theta, \phi) + \beta \, \mathcal{L}_\text{regularization}(\phi) \\
    &amp;= \mathbb{E}_{\mathbf{x} \in \mathcal{D}, \xi \sim \mathcal{N}(0, 1)} [ - \log p_\theta(\mathbf{\mu_x} + \mathbf{\sigma_x} \, \xi) + \dfrac{\beta}{2} \, \sum_{k=1}^K (\mathbf{\sigma_x^2} + \mathbf{\mu_x}^2 -1 - \log \mathbf{\sigma_x^2})] \\
\end{aligned}</span></p>
<ul>
<li><p>Using <span class="math inline">\beta &gt; 1</span> puts emphasis on learning statistically independent latent factors.</p></li>
<li><p>The <span class="math inline">\beta</span>-VAE allows to <strong>disentangle</strong> the latent variables, i.e.&nbsp;manipulate them individually to vary only one aspect of the image (pose, color, gender, etc.).</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/betavae-results.png" class="quarto-figure quarto-figure-center" style="width:70.0%"></p>
</figure>
</div>
<div class="footer">
<p>Higgins et al.&nbsp;(2016). beta-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework. <a href="https://openreview.net/forum?id=Sy2fzU9gl" class="uri">https://openreview.net/forum?id=Sy2fzU9gl</a></p>
</div>
</section>

<section id="vq-vae" class="title-slide slide level1 center">
<h1>VQ-VAE</h1>
<ul>
<li>Deepmind researchers proposed VQ-VAE-2, a hierarchical VAE using vector-quantized priors able to generate high-resolution images.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vqvae.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vqvae-results.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vqvae-results.gif" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<div class="footer">
<p>Razavi A, Oord A van den, Vinyals O. 2019. Generating Diverse High-Fidelity Images with VQ-VAE-2. arXiv:190600446</p>
</div>
</section>

<section id="conditional-variational-autoencoder-cvae" class="title-slide slide level1 center">
<h1>Conditional variational autoencoder (CVAE)</h1>
<ul>
<li>What if we provide the labels to the encoder and the decoder during training?</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cvae-structure.svg" style="width:70.0%"></p>
<figcaption>Source: <a href="https://ijdykeman.github.io/ml/2016/12/21/cvae.html" class="uri">https://ijdykeman.github.io/ml/2016/12/21/cvae.html</a></figcaption>
</figure>
</div>
<div class="footer">
<p>Sohn, K., Lee, H., and Yan, X. (2015). “Learning Structured Output Representation using Deep Conditional Generative Models,” NIPS 28.</p>
</div>
</section>

<section id="conditional-variational-autoencoder-cvae-1" class="title-slide slide level1 center">
<h1>Conditional variational autoencoder (CVAE)</h1>
<ul>
<li>When trained with labels, the <strong>conditional variational autoencoder</strong> (CVAE) becomes able to sample many images of the same class.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cvae-generation.svg" style="width:80.0%"></p>
<figcaption>Source: <a href="https://ijdykeman.github.io/ml/2016/12/21/cvae.html" class="uri">https://ijdykeman.github.io/ml/2016/12/21/cvae.html</a></figcaption>
</figure>
</div>
<div class="footer">
<p>Sohn, K., Lee, H., and Yan, X. (2015). “Learning Structured Output Representation using Deep Conditional Generative Models,” NIPS 28.</p>
</div>
</section>

<section id="cvae-on-mnist" class="title-slide slide level1 center">
<h1>CVAE on MNIST</h1>
<ul>
<li>CVAE allows to sample as many samples of a given class as we want: <strong>data augmentation</strong>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cvae-mnist.png" style="width:50.0%"></p>
<figcaption>Source: <a href="https://ijdykeman.github.io/ml/2016/12/21/cvae.html" class="uri">https://ijdykeman.github.io/ml/2016/12/21/cvae.html</a></figcaption>
</figure>
</div>
</section>

<section id="cvae-on-shapes" class="title-slide slide level1 center">
<h1>CVAE on shapes</h1>
<ul>
<li>The condition does not need to be a label, it can be a shape or another image (passed through another encoder).</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cvae-shape.png" style="width:50.0%"></p>
<figcaption>Source: <a href="https://hci.iwr.uni-heidelberg.de/content/variational-u-net-conditional-appearance-and-shape-generation" class="uri">https://hci.iwr.uni-heidelberg.de/content/variational-u-net-conditional-appearance-and-shape-generation</a></figcaption>
</figure>
</div>
</section>

<section id="variational-inference-optional" class="title-slide slide level1 center">
<h1>5 - Variational inference (optional)</h1>

</section>

<section id="learning-probability-distributions-from-samples" class="title-slide slide level1 center">
<h1>Learning probability distributions from samples</h1>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p>The input data <span class="math inline">X</span> comes from an unknown distribution <span class="math inline">P(X)</span>. The training set <span class="math inline">\mathcal{D}</span> is formed by <strong>samples</strong> of that distribution.</p></li>
<li><p>Learning the distribution of the data means learning a <strong>parameterized distribution</strong> <span class="math inline">p_\theta(X)</span> that is as close as possible from the true distribution <span class="math inline">P(X)</span>.</p></li>
<li><p>The parameterized distribution could be a family of known distributions (e.g.&nbsp;normal) or a neural network with a softmax output layer.</p></li>
</ul>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/kerneldensityestimation.png"></p>
<figcaption>Source: <a href="https://machinelearningmastery.com/probability-density-estimation/" class="uri">https://machinelearningmastery.com/probability-density-estimation/</a></figcaption>
</figure>
</div>
</div></div>
<ul>
<li>This means that we want to minimize the KL between the two distributions:</li>
</ul>
<p><span class="math display">\min_\theta \, \text{KL}(P(X) || p_\theta(X)) = \mathbb{E}_{x \sim P(X)} [- \log \dfrac{p_\theta(X=x)}{P(X=x)}]</span></p>
<ul>
<li>The problem is that we do not know <span class="math inline">P(X)</span> as it is what we want to learn, so we cannot estimate the KL directly.</li>
</ul>
</section>

<section id="supervised-learning" class="title-slide slide level1 center">
<h1>Supervised learning</h1>
<div class="columns">
<div class="column" style="width:65%;">
<ul>
<li><p>In supervised learning, we are learning the <strong>conditional probability</strong> <span class="math inline">P(T | X)</span> of the targets given the inputs, i.e.&nbsp;what is the probability of having the label <span class="math inline">T=t</span> given the input <span class="math inline">X=x</span>.</p></li>
<li><p>A NN with a softmax output layer represents the parameterized distribution <span class="math inline">p_\theta(T | X)</span>.</p></li>
<li><p>The KL between the two distributions is:</p></li>
</ul>
<p><span class="math display">\text{KL}(P(T | X) || p_\theta(T | X)) = \mathbb{E}_{x, t \sim \mathcal{D}} [- \log \dfrac{p_\theta(T=t | X=x)}{P(T=t | X=x)}]</span></p>
</div><div class="column" style="width:35%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/mlp.svg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<ul>
<li>With the properties of the log, we know that the KL is the cross-entropy minus the entropy of the data:</li>
</ul>
<p><span class="math display">\begin{aligned}
\text{KL}(P(T | X) || p_\theta(T | X)) &amp;= \mathbb{E}_{x, t \sim \mathcal{D}} [- \log p_\theta(T=t | X=x)]  - \mathbb{E}_{x, t \sim \mathcal{D}} [- \log P(T=t | X=x)] \\
&amp;\\
    &amp; = H(P(T | X), p_\theta(T |X)) - H(P(T|X)) \\
\end{aligned}</span></p>
</section>

<section id="supervised-learning-1" class="title-slide slide level1 center">
<h1>Supervised learning</h1>
<ul>
<li>Kullback-Leibler divergence between the model and the data:</li>
</ul>
<p><span class="math display">\begin{aligned}
\text{KL}(P(T | X) || p_\theta(T | X)) &amp; = H(P(T | X), p_\theta(T |X)) - H(P(T|X)) \\
\end{aligned}</span></p>
<ul>
<li>When we minimize the KL by applying gradient descent on the parameters <span class="math inline">\theta</span>, only the cross-entropy will change, as the data does not depends on the model:</li>
</ul>
<p><span class="math display">\begin{aligned}
\nabla_\theta \, \text{KL}(P(T | X) || p_\theta(T | X))  &amp; = \nabla_\theta \, H(P(T | X), p_\theta(T |X)) - \nabla_\theta \,  H(P(T|X)) \\
    &amp;\\
     &amp; = \nabla_\theta \, H(P(T | X), p_\theta(T |X)) \\
     &amp; \\
     &amp; = \nabla_\theta \, \mathbb{E}_{x, t \sim \mathcal{D}} [-  \log p_\theta(T=t | X=x) ]\\
\end{aligned}</span></p>
<ul>
<li><p>Minimizing the cross-entropy (negative log likelihood) of the model on the data is the same as minimizing the KL between the two distributions in supervised learning!</p></li>
<li><p>We were actually minimizing the KL all along.</p></li>
</ul>
</section>

<section id="maximum-likelihood-estimation" class="title-slide slide level1 center">
<h1>Maximum likelihood estimation</h1>
<ul>
<li>When trying to learn the distribution <span class="math inline">P(X)</span> of the data directly, we could use the same trick:</li>
</ul>
<p><span class="math display">\nabla_\theta \, \text{KL}(P(X) || p_\theta(X)) = \nabla_\theta \, H(P(X), p_\theta(X))  = \nabla_\theta \, \mathbb{E}_{x \sim X} [- \log p_\theta(X=x)]</span></p>
<p>i.e.&nbsp;maximize the log-likelihood of the model on the data <span class="math inline">X</span>.</p>
<ul>
<li>If we use <span class="math inline">N</span> data samples to estimate the expectation, we notice that:</li>
</ul>
<p><span class="math display">
\mathbb{E}_{x \sim X} [\log p_\theta(X=x)] \approx \dfrac{1}{N} \, \sum_{i=1}^N \log p_\theta(X=x_i) = \dfrac{1}{N} \, \log \prod_{i=1}^N p_\theta(X=x_i) = \dfrac{1}{N} \, \log L(\theta)
</span></p>
<p>is indeed the log-likelihood of the model on the data that we maximized in <strong>maximum likelihood estimation</strong>.</p>
</section>

<section id="curse-of-dimensionality" class="title-slide slide level1 center">
<h1>Curse of dimensionality</h1>
<ul>
<li>The problem is that images are <strong>highly-dimensional</strong> (one dimension per pixel), so we would need astronomical numbers of samples to estimate the gradient (once): <strong>curse of dimensionality</strong>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cursedimensionality.png"></p>
<figcaption>Source: <a href="https://dibyaghosh.com/blog/probability/highdimensionalgeometry.html" class="uri">https://dibyaghosh.com/blog/probability/highdimensionalgeometry.html</a></figcaption>
</figure>
</div>
<ul>
<li><p>MLE does not work well in high-dimensional spaces.</p></li>
<li><p>We need to work in a much lower-dimensional space.</p></li>
</ul>
</section>

<section id="manifolds" class="title-slide slide level1 center">
<h1>Manifolds</h1>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p>Images are not random samples of the pixel space: <strong>natural images</strong> are embedded in a much lower-dimensional space called a <strong>manifold</strong>.</p></li>
<li><p>A manifold is a <strong>locally Euclidian</strong> topological space of lower dimension.</p></li>
<li><p>The surface of the earth is locally flat and 2D, but globally spherical and 3D.</p></li>
<li><p>If we have a <strong>generative model</strong> telling us how a point on the manifold <span class="math inline">z</span> maps to the image space (<span class="math inline">P(X | z)</span>), we would only need to learn the distribution of the data in the lower-dimensional <strong>latent space</strong>.</p></li>
</ul>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/manifold.png"></p>
<figcaption>Source: <a href="https://en.wikipedia.org/wiki/Manifold" class="uri">https://en.wikipedia.org/wiki/Manifold</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="generative-model" class="title-slide slide level1 center">
<h1>Generative model</h1>
<div class="columns">
<div class="column" style="width:70%;">
<ul>
<li><p>The low-dimensional <strong>latent variables</strong> <span class="math inline">z</span> are the actual cause for the observations <span class="math inline">X</span>.</p></li>
<li><p>Given a sample <span class="math inline">z</span> on the manifold, we can train a <strong>generative model</strong> <span class="math inline">p_\theta(X | z)</span> to recreate the input <span class="math inline">X</span>.</p></li>
<li><p><span class="math inline">p_\theta(X | z)</span> is the <strong>decoder</strong>: given a latent representation <span class="math inline">z</span>, what is the corresponding observation <span class="math inline">X</span>?</p></li>
</ul>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/latentvariable.png"></p>
<figcaption>Source: <a href="https://blog.evjang.com/2016/08/variational-bayes.html" class="uri">https://blog.evjang.com/2016/08/variational-bayes.html</a></figcaption>
</figure>
</div>
</div></div>
<ul>
<li>If we learn the distribution <span class="math inline">p_\theta(z)</span> of the manifold (latent space), we can infer the distribution of the data <span class="math inline">p_\theta(X)</span> using that model:</li>
</ul>
<p><span class="math display">p_\theta(X) = \mathbb{E}_{z \sim p_\theta(z)} [p_\theta(X | z)] = \int_z p_\theta(X | z) \, p_\theta(z) \, dz</span></p>
<ul>
<li>Problem: we do not know <span class="math inline">p_\theta(z)</span>, as the only data we see is <span class="math inline">X</span>: <span class="math inline">z</span> is called a <strong>latent variable</strong> because it explains the data but is hidden.</li>
</ul>
</section>

<section id="variational-inference" class="title-slide slide level1 center">
<h1>Variational inference</h1>
<ul>
<li>To estimate <span class="math inline">p_\theta(z)</span>, we could again marginalize over <span class="math inline">X</span>:</li>
</ul>
<p><span class="math display">p_\theta(z) = \mathbb{E}_{x \sim p_\theta(X)} [p_\theta(z | x)] = \int_x p_\theta(z | x) \, p_\theta(x) \, dx</span></p>
<ul>
<li><p><span class="math inline">p_\theta(z | x)</span> is the <strong>encoder</strong>: given an input <span class="math inline">x \sim p_\theta(X)</span>, what is its latent representation <span class="math inline">z</span>?</p></li>
<li><p>The Bayes rule tells us:</p></li>
</ul>
<p><span class="math display">p_\theta(z | x) = p_\theta(x |z) \, \dfrac{p_\theta(z)}{p_\theta(x)}</span></p>
<ul>
<li><p>The posterior probability (encoder) <span class="math inline">p_\theta(z | X)</span> depends on the model (decoder) <span class="math inline">p_\theta(X|z)</span>, the prior (assumption) <span class="math inline">p_\theta(z)</span> and the evidence (data) <span class="math inline">p_\theta(X)</span>.</p></li>
<li><p>We get:</p></li>
</ul>
<p><span class="math display">p_\theta(z) = \mathbb{E}_{x \sim p_\theta(X)} [p_\theta(x |z) \, \dfrac{p_\theta(z)}{p_\theta(x)}]</span></p>
</section>

<section id="variational-inference-1" class="title-slide slide level1 center">
<h1>Variational inference</h1>
<ul>
<li>The posterior is <strong>untractable</strong> as it would require to integrate over all possible inputs <span class="math inline">x \sim p_\theta(X)</span>:</li>
</ul>
<p><span class="math display">p_\theta(z) = \mathbb{E}_{x \sim p_\theta(X)} [p_\theta(x |z) \, \dfrac{p_\theta(z)}{p_\theta(x)}] = \int_x p_\theta(x |z) \, p_\theta(z) \, dx</span></p>
<ul>
<li><strong>Variational inference</strong> proposes to approximate the true encoder <span class="math inline">p_\theta(z | x)</span> by another parameterized distribution <span class="math inline">q_\phi(z|x)</span>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/VAE-graphical-model.png" style="width:60.0%"></p>
<figcaption>Source: <a href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html" class="uri">https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html</a></figcaption>
</figure>
</div>
<ul>
<li><p>The decoder <span class="math inline">p_\theta(x |z)</span> generates observations <span class="math inline">x</span> from a latent representation <span class="math inline">x</span> with parameters <span class="math inline">\theta</span>.</p></li>
<li><p>The encoder <span class="math inline">q_\phi(z|x)</span> estimates the latent representation <span class="math inline">z</span> of a generated observation <span class="math inline">x</span>. It should approximate <span class="math inline">p_\theta(z | x)</span> with parameters <span class="math inline">\phi</span>.</p></li>
</ul>
</section>

<section id="variational-inference-2" class="title-slide slide level1 center">
<h1>Variational inference</h1>
<ul>
<li>To make <span class="math inline">q_\phi(z| X)</span> close from <span class="math inline">p_\theta(z | X)</span>, we minimize their KL divergence:</li>
</ul>
<p><span class="math display">\begin{aligned}
\text{KL}(q_\phi(z|X) || p_\theta(z | X) ) &amp;= \mathbb{E}_{z \sim q_\phi(z|X)} [- \log \dfrac{p_\theta(z | X)}{q_\phi(z|X)}]\\
\end{aligned}</span></p>
<ul>
<li><p>Note that we sample the latent representations from the learned encoder <span class="math inline">q_\phi(z|X)</span> (imagination).</p></li>
<li><p>As <span class="math inline">p_\theta(z | X) = p_\theta(X |z) \, \dfrac{p_\theta(z)}{p_\theta(X)}</span>, we get:</p></li>
</ul>
<p><span class="math display">\begin{aligned}
\text{KL}(q_\phi(z|X) || p_\theta(z | X) ) &amp;= \mathbb{E}_{z \sim q_\phi(z|X)} [- \log \dfrac{p_\theta(X | z) \, p_\theta(z)}{q_\phi(z|X) \, p_\theta(X)}]\\
&amp;=\mathbb{E}_{z \sim q_\phi(z|X)} [- \log \dfrac{p_\theta(z)}{q_\phi(z|X)}] - \mathbb{E}_{z \sim q_\phi(z|X)} [- \log p_\theta(X)]  \\
&amp;+ \mathbb{E}_{z \sim q_\phi(z|X)} [- \log p_\theta(X | z)]\\
\end{aligned}</span></p>
<ul>
<li><span class="math inline">p_\theta(X)</span> does not depend on <span class="math inline">z</span>, so its expectation w.r.t <span class="math inline">z</span> is constant:</li>
</ul>
<p><span class="math display">\begin{aligned}
\text{KL}(q_\phi(z|X) || p_\theta(z | X) ) &amp;= \text{KL}(q_\phi(z|X) || p_\theta(z)) + \log p_\theta(X) + \mathbb{E}_{z \sim q_\phi(z|X)} [- \log p_\theta(X | z)]\\
\end{aligned}</span></p>
</section>

<section id="evidence-lower-bound" class="title-slide slide level1 center">
<h1>Evidence lower bound</h1>
<ul>
<li>We rearrange the terms:</li>
</ul>
<p><span class="math display">\begin{aligned}
\log p_\theta(X) - \text{KL}(q_\phi(z|X) || p_\theta(z | X) ) &amp;=  - \mathbb{E}_{z \sim q_\phi(z|X)} [- \log p_\theta(X | z)] - \text{KL}(q_\phi(z|X) || p_\theta(z))\\
\end{aligned}</span></p>
<ul>
<li><p>Training the <strong>encoder</strong> means that we <strong>minimize</strong> <span class="math inline">\text{KL}(q_\phi(z|X) || p_\theta(z | X) )</span>.</p></li>
<li><p>Training the <strong>decoder</strong> means that we <strong>maximize</strong> <span class="math inline">\log p_\theta(X)</span> (log-likelihood of the model).</p></li>
<li><p>Training the encoder and decoder together means that we <strong>maximize</strong>:</p></li>
</ul>
<p><span class="math display"> \text{ELBO}(\theta, \phi) = \log p_\theta(X) - \text{KL}(q_\phi(z|X) || p_\theta(z | X) )</span></p>
<ul>
<li>The KL divergence is always positive or equal to 0, so we have:</li>
</ul>
<p><span class="math display">\text{ELBO}(\theta, \phi) \leq \log p_\theta(X)</span></p>
<ul>
<li>This term is called the <strong>evidence lower bound</strong> (ELBO): by maximizing it, we also maximize the untractable evidence <span class="math inline">\log p_\theta(X)</span>, which is what we want to do.</li>
</ul>
</section>

<section id="variational-inference-3" class="title-slide slide level1 center">
<h1>Variational inference</h1>
<ul>
<li>The trick is that the right-hand term of the equation gives us a tractable definition of the ELBO term:</li>
</ul>
<p><span class="math display">\begin{aligned}
\text{ELBO}(\theta, \phi) &amp;=  \log p_\theta(X) - \text{KL}(q_\phi(z|X) || p_\theta(z | X) ) \\
&amp;\\
&amp;= - \mathbb{E}_{z \sim q_\phi(z|X)} [- \log p_\theta(X | z)] - \text{KL}(q_\phi(z|X) || p_\theta(z))
\end{aligned}</span></p>
<ul>
<li>What happens when we <strong>minimize</strong> the negative ELBO?</li>
</ul>
<p><span class="math display"> \mathcal{L}(\theta, \phi) = - \text{ELBO}(\theta, \phi) = \mathbb{E}_{z \sim q_\phi(z|X)} [- \log p_\theta(X | z)] + \text{KL}(q_\phi(z|X) || p_\theta(z))</span></p>
<ul>
<li><p><span class="math inline">\mathbb{E}_{z \sim q_\phi(z|X)} [- \log p_\theta(X | z)]</span> is the <strong>reconstruction loss</strong> of the decoder <span class="math inline">p_\theta(X | z)</span>:</p>
<ul>
<li>Given a sample <span class="math inline">z</span> of the encoder <span class="math inline">q_\phi(z|X)</span>, minimize the negative log-likelihood of the reconstruction <span class="math inline">p_\theta(X | z)</span>.</li>
</ul></li>
<li><p><span class="math inline">\text{KL}(q_\phi(z|X) || p_\theta(z))</span> is the <strong>regularization loss</strong> for the encoder:</p>
<ul>
<li>The latent distribution <span class="math inline">q_\phi(z|X)</span> should be too far from the <strong>prior</strong> <span class="math inline">p_\theta(z)</span>.</li>
</ul></li>
</ul>
</section>

<section id="variational-autoencoders" class="title-slide slide level1 center">
<h1>Variational autoencoders</h1>
<ul>
<li><strong>Variational autoencoders</strong> use <span class="math inline">\mathcal{N}(0, 1)</span> as a prior for the latent space, but any other prior could be used.</li>
</ul>
<p><span class="math display">\begin{aligned}
    \mathcal{L}(\theta, \phi) &amp;= \mathcal{L}_\text{reconstruction}(\theta, \phi) + \mathcal{L}_\text{regularization}(\phi) \\
    &amp;\\
    &amp;= \mathbb{E}_{\mathbf{x} \in \mathcal{D}, \mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} [ - \log p_\theta(\mathbf{z})] + \text{KL}(q_\phi(\mathbf{z}|\mathbf{x}) || \mathcal{N}(\mathbf{0}, \mathbf{1}))\\
\end{aligned}</span></p>
<ul>
<li><p>The reparameterization trick and the fact that the KL between normal distributions has a closed form allow us to use backpropagation end-to-end.</p></li>
<li><p>The encoder <span class="math inline">q_\phi(z|X)</span> and decoder <span class="math inline">p_\theta(X | z)</span> are neural networks in a VAE, but other parametrized distributions can be used (e.g.&nbsp;in physics).</p></li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/VAE-graphical-model.png"></p>
<figcaption>Source: <a href="https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html" class="uri">https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vae-reparameterization2.png"></p>
<figcaption>Source: <a href="https://ijdykeman.github.io/ml/2016/12/21/cvae.html" class="uri">https://ijdykeman.github.io/ml/2016/12/21/cvae.html</a></figcaption>
</figure>
</div>
</div></div>

<div class="quarto-auto-generated-content">
<div class="footer footer-default">

</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false,"theme":"whiteboard"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>