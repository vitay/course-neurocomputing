<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.5.57">

  <meta name="author" content="Julien Vitay">
  <title>Neurocomputing</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="img/tuc-new-large.png" data-background-opacity="1" data-background-position="top" data-background-size="30%" class="quarto-title-block center">
  <h1 class="title">Neurocomputing</h1>
  <p class="subtitle">Basics in mathematics</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Julien Vitay 
</div>
        <p class="quarto-title-affiliation">
            Professur für Künstliche Intelligenz - Fakultät für Informatik
          </p>
    </div>
</div>

</section>
<section id="outline" class="title-slide slide level1 center">
<h1>Outline</h1>
<ol type="1">
<li><p>Linear algebra</p></li>
<li><p>Calculus</p></li>
<li><p>Probability theory</p></li>
<li><p>Statistics</p></li>
<li><p>Information theory</p></li>
</ol>
</section>

<section id="linear-algebra" class="title-slide slide level1 center">
<h1>1 - Linear algebra</h1>

</section>

<section id="mathematical-objects" class="title-slide slide level1 center">
<h1>Mathematical objects</h1>
<ul>
<li><p><strong>Scalars</strong> <span class="math inline">x</span> are 0-dimensional values. They can either take real values (<span class="math inline">x \in \Re</span>, e.g.&nbsp;<span class="math inline">x = 1.4573</span>, floats in CS) or natural values (<span class="math inline">x \in \mathbb{N}</span>, e.g.&nbsp;<span class="math inline">x = 3</span>, integers in CS).</p></li>
<li><p><strong>Vectors</strong> <span class="math inline">\mathbf{x}</span> are 1-dimensional arrays of length <span class="math inline">d</span>.</p></li>
<li><p>The bold notation <span class="math inline">\mathbf{x}</span> will be used in this course, but you may also be accustomed to the arrow notation <span class="math inline">\overrightarrow{x}</span> used on the blackboard. When using real numbers, the <strong>vector space</strong> with <span class="math inline">d</span> dimensions is noted <span class="math inline">\Re^d</span>, so we can note <span class="math inline">\mathbf{x} \in \Re^d</span>.</p></li>
<li><p>Vectors are typically represented vertically to outline their <span class="math inline">d</span> elements <span class="math inline">x_1, x_2, \ldots, x_d</span>:</p></li>
</ul>
<p><span class="math display">\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix}</span></p>
</section>

<section id="mathematical-objects-1" class="title-slide slide level1 center">
<h1>Mathematical objects</h1>
<ul>
<li><p><strong>Matrices</strong> <span class="math inline">A</span> are 2-dimensional arrays of size (or shape) <span class="math inline">m \times n</span> (<span class="math inline">m</span> rows, <span class="math inline">n</span> columns, <span class="math inline">A \in \Re^{m \times n}</span>).</p></li>
<li><p>They are represented by a capital letter to distinguish them from scalars (classically also in bold <span class="math inline">\mathbf{A}</span> but not here). The element <span class="math inline">a_{ij}</span> of a matrix <span class="math inline">A</span> is the element on the <span class="math inline">i</span>-th row and <span class="math inline">j</span>-th column.</p></li>
</ul>
<p><span class="math display">A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix}</span></p>
<ul>
<li><strong>Tensors</strong> <span class="math inline">\mathcal{A}</span> are arrays with more than two dimensions. We will not really do math on these objects, but they are useful internally (hence the name of the <code>tensorflow</code> library).</li>
</ul>
</section>

<section id="vectors" class="title-slide slide level1 center">
<h1>Vectors</h1>
<ul>
<li><p>A vector can be thought of as the <strong>coordinates of a point</strong> in an Euclidean space (such the 2D space), relative to the origin.</p></li>
<li><p>A vector space relies on two fundamental operations, which are that:</p></li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Vectors can be added:</li>
</ul>
<p><span class="math display">\mathbf{x} + \mathbf{y} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix} + \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_d \end{bmatrix} = \begin{bmatrix} x_1 + y_1 \\ x_2 + y_2 \\ \vdots \\ x_d + y_d \end{bmatrix}</span></p>
<ul>
<li>Vectors can be multiplied by a scalar:</li>
</ul>
<p><span class="math display">a \, \mathbf{x} = a \, \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix} = \begin{bmatrix} a \, x_1 \\ a \, x_2 \\ \vdots \\ a \, x_d \end{bmatrix}</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vectorspace.png"></p>
<figcaption>Source: <a href="https://mathinsight.org/image/vector_2d_add" class="uri">https://mathinsight.org/image/vector_2d_add</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="properties-of-vector-spaces" class="title-slide slide level1 center">
<h1>Properties of vector spaces</h1>
<ul>
<li><p>These two operations generate a lot of nice properties (see <a href="https://en.wikipedia.org/wiki/Vector_space" class="uri">https://en.wikipedia.org/wiki/Vector_space</a> for a full list), including:</p>
<ul>
<li>associativity:</li>
</ul>
<p><span class="math display">\mathbf{x} + (\mathbf{y} + \mathbf{z}) = (\mathbf{x} + \mathbf{y}) + \mathbf{z}</span></p>
<ul>
<li>commutativity:</li>
</ul>
<p><span class="math display">\mathbf{x} + \mathbf{y} = \mathbf{y} + \mathbf{x}</span></p>
<ul>
<li>the existence of a zero vector</li>
</ul>
<p><span class="math display">\mathbf{x} + \mathbf{0} = \mathbf{x}</span></p>
<ul>
<li>inversion:</li>
</ul>
<p><span class="math display">\mathbf{x} + (-\mathbf{x}) = \mathbf{0}</span></p>
<ul>
<li>distributivity:</li>
</ul>
<p><span class="math display">a \, (\mathbf{x} + \mathbf{y}) = a \, \mathbf{x} + a \, \mathbf{y}</span></p></li>
</ul>
</section>

<section id="norm-of-a-vector" class="title-slide slide level1 center">
<h1>Norm of a vector</h1>
<div class="columns">
<div class="column" style="width:15%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vector_norms.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:85%;">
<ul>
<li>Vectors have a <strong>norm</strong> (or length) <span class="math inline">||\mathbf{x}||</span>. The most intuitive one (if you know the Pythagoras theorem) is the <strong>Euclidean norm</strong> or <span class="math inline">L^2</span>-norm, which sums the square of each element:</li>
</ul>
<p><span class="math display">||\mathbf{x}||_2 = \sqrt{x_1^2 + x_2^2 + \ldots + x_d^2}</span></p>
<ul>
<li>Other norms exist, distinguished by the subscript. The <strong><span class="math inline">L^1</span>-norm</strong> (also called the Manhattan norm) sums the absolute value of each element:</li>
</ul>
<p><span class="math display">||\mathbf{x}||_1 = |x_1| + |x_2| + \ldots + |x_d|</span></p>
<ul>
<li>The <strong>p-norm</strong> generalizes the Euclidean norm to other powers <span class="math inline">p</span>:</li>
</ul>
<p><span class="math display">||\mathbf{x}||_p = (|x_1|^p + |x_2|^p + \ldots + |x_d|^p)^{\frac{1}{p}}</span></p>
<ul>
<li>The <strong>infinity norm</strong> (or maximum norm) <span class="math inline">L^\infty</span> returns the maximum element of the vector:</li>
</ul>
<p><span class="math display">||\mathbf{x}||_\infty = \max(|x_1|, |x_2|, \ldots, |x_d|)</span></p>
</div></div>
<div class="footer">
<p><a href="https://en.wikipedia.org/wiki/Norm_(mathematics)" class="uri">https://en.wikipedia.org/wiki/Norm_(mathematics)</a></p>
</div>
</section>

<section id="dot-product" class="title-slide slide level1 center">
<h1>Dot product</h1>
<ul>
<li>One important operation for vectors is the <strong>dot product</strong> (also called scalar product or inner product) between two vectors:</li>
</ul>
<p><span class="math display">\langle \mathbf{x} \cdot \mathbf{y} \rangle = \langle \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix} \cdot \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_d \end{bmatrix} \rangle = x_1 \, y_1 + x_2 \, y_2 + \ldots + x_d \, y_d</span></p>
<ul>
<li><p>The dot product basically sums one by one the product of the elements of each vector. The angular brackets are sometimes omitted (<span class="math inline">\mathbf{x} \cdot \mathbf{y}</span>) but we will use them in this course for clarity.</p></li>
<li><p>One can notice immediately that the dot product is <strong>symmetric</strong>:</p></li>
</ul>
<p><span class="math display">\langle \mathbf{x} \cdot \mathbf{y} \rangle = \langle \mathbf{y} \cdot \mathbf{x} \rangle</span></p>
<p>and <strong>linear</strong>:</p>
<p><span class="math display">\langle (a \, \mathbf{x} + b\, \mathbf{y}) \cdot \mathbf{z} \rangle = a\, \langle \mathbf{x} \cdot \mathbf{z} \rangle + b \, \langle \mathbf{y} \cdot \mathbf{z} \rangle</span></p>
</section>

<section id="dot-product-1" class="title-slide slide level1 center">
<h1>Dot product</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>The dot product is an indirect measurement of the <strong>angle</strong> <span class="math inline">\theta</span> between two vectors:</li>
</ul>
<p><span class="math display">\langle \mathbf{x} \cdot \mathbf{y} \rangle = ||\mathbf{x}||_2 \, ||\mathbf{y}||_2 \, \cos(\theta)</span></p>
<ul>
<li><p>If you normalize the two vectors by dividing them by their norm (which is a scalar), we indeed have the cosine of the angle between them</p></li>
<li><p>The higher the normalized dot product, the more the two vectors point towards the same direction (<strong>cosine distance</strong> between two vectors).</p></li>
</ul>
<p><span class="math display">\langle \displaystyle\frac{\mathbf{x}}{||\mathbf{x}||_2} \cdot \frac{\mathbf{y}}{||\mathbf{y}||_2} \rangle =  \cos(\theta)</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dot_product_projection_unit_vector.png"></p>
<figcaption>Source: <a href="https://mathinsight.org/image/dot_product_projection_unit_vector" class="uri">https://mathinsight.org/image/dot_product_projection_unit_vector</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="matrices" class="title-slide slide level1 center">
<h1>Matrices</h1>
<ul>
<li>Matrices are derived from vectors, so most of the previous properties will be true. Let’s consider this 4x3 matrix:</li>
</ul>
<p><span class="math display">A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33} \\
a_{41} &amp; a_{42} &amp; a_{43} \\
\end{bmatrix}</span></p>
<ul>
<li>Each column of the matrix is a vector with 4 elements:</li>
</ul>
<p><span class="math display">\mathbf{a}_1 = \begin{bmatrix}
a_{11} \\
a_{21} \\
a_{31} \\
a_{41} \\
\end{bmatrix} \qquad
\mathbf{a}_2 = \begin{bmatrix}
a_{12} \\
a_{22} \\
a_{32} \\
a_{42} \\
\end{bmatrix} \qquad
\mathbf{a}_3 = \begin{bmatrix}
a_{13} \\
a_{23} \\
a_{33} \\
a_{43} \\
\end{bmatrix} \qquad
</span></p>
<ul>
<li>A <span class="math inline">m \times n</span> matrix is therefore a collection of <span class="math inline">n</span> vectors of size <span class="math inline">m</span> put side by side column-wise:</li>
</ul>
<p><span class="math display">A = \begin{bmatrix}
\mathbf{a}_1 &amp; \mathbf{a}_2 &amp; \mathbf{a}_3\\
\end{bmatrix}</span></p>
</section>

<section id="properties-of-matrix-spaces" class="title-slide slide level1 center">
<h1>Properties of matrix spaces</h1>
<ul>
<li>All properties of the vector spaces (associativity, commutativity, distributivity) also apply to matrices, as additions and multiplications with a scalar are defined.</li>
</ul>
<p><span class="math display">\alpha \, A + \beta \, B = \begin{bmatrix}
\alpha\, a_{11} + \beta \, b_{11} &amp; \alpha\, a_{12} + \beta \, b_{12} &amp; \alpha\, a_{13} + \beta \, b_{13} \\
\alpha\, a_{21} + \beta \, b_{21} &amp; \alpha\, a_{22} + \beta \, b_{22} &amp; \alpha\, a_{23} + \beta \, b_{23} \\
\alpha\, a_{31} + \beta \, b_{31} &amp; \alpha\, a_{32} + \beta \, b_{32} &amp; \alpha\, a_{33} + \beta \, b_{33} \\
\alpha\, a_{41} + \beta \, b_{41} &amp; \alpha\, a_{42} + \beta \, b_{42} &amp; \alpha\, a_{43} + \beta \, b_{43} \\
\end{bmatrix}</span></p>
<p><strong>Note:</strong> Beware, you can only add matrices of the same dimensions <span class="math inline">m\times n</span>. You cannot add a <span class="math inline">2\times 3</span> matrix to a <span class="math inline">5 \times 4</span> one.</p>
</section>

<section id="transposition" class="title-slide slide level1 center">
<h1>Transposition</h1>
<ul>
<li>The <strong>transpose</strong> <span class="math inline">A^T</span> of a <span class="math inline">m \times n</span> matrix <span class="math inline">A</span> is a <span class="math inline">n \times m</span> matrix, where the row and column indices are swapped:</li>
</ul>
<p><span class="math display">A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix}, \qquad
A^T = \begin{bmatrix}
a_{11} &amp; a_{21} &amp; \cdots &amp; a_{m1} \\
a_{12} &amp; a_{22} &amp; \cdots &amp; a_{m2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{1n} &amp; a_{2n} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix}
</span></p>
<ul>
<li>This is also true for vectors, which become horizontal after transposition:</li>
</ul>
<p><span class="math display">\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix}, \qquad
\mathbf{x}^T = \begin{bmatrix} x_1 &amp; x_2 &amp; \ldots &amp; x_d \end{bmatrix}
</span></p>
</section>

<section id="matrix-multiplication" class="title-slide slide level1 center">
<h1>Matrix multiplication</h1>
<ul>
<li>If <span class="math inline">A</span> is a <span class="math inline">m\times n</span> matrix and <span class="math inline">B</span> a <span class="math inline">n \times p</span> matrix:</li>
</ul>
<p><span class="math display">
A=\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix},\quad
B=\begin{bmatrix}
b_{11} &amp; b_{12} &amp; \cdots &amp; b_{1p} \\
b_{21} &amp; b_{22} &amp; \cdots &amp; b_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
b_{n1} &amp; b_{n2} &amp; \cdots &amp; b_{np} \\
\end{bmatrix}
</span></p>
<p>we can multiply them to obtain a <span class="math inline">m \times p</span> matrix:</p>
<p><span class="math display">
C = A \times B =\begin{bmatrix}
c_{11} &amp; c_{12} &amp; \cdots &amp; c_{1p} \\
c_{21} &amp; c_{22} &amp; \cdots &amp; c_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
c_{m1} &amp; c_{m2} &amp; \cdots &amp; c_{mp} \\
\end{bmatrix}
</span></p>
<p>where each element <span class="math inline">c_{ij}</span> is the dot product of the <span class="math inline">i</span>th row of <span class="math inline">A</span> and <span class="math inline">j</span>th column of <span class="math inline">B</span>:</p>
<p><span class="math display">c_{ij} = \langle A_{i, :} \cdot B_{:, j} \rangle = a_{i1}b_{1j} + a_{i2}b_{2j} +\cdots + a_{in}b_{nj}= \sum_{k=1}^n a_{ik}b_{kj}</span></p>
<p><strong>Note:</strong> <span class="math inline">n</span>, the number of columns of <span class="math inline">A</span> and rows of <span class="math inline">B</span>, must be the same!</p>
</section>

<section id="matrix-multiplication-1" class="title-slide slide level1 center">
<h1>Matrix multiplication</h1>
<ul>
<li>The element <span class="math inline">c_{ij}</span> of <span class="math inline">C = A \times B</span> is the dot product between the <span class="math inline">i</span>th row of <span class="math inline">A</span> and the <span class="math inline">j</span>th column of <span class="math inline">B</span>.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/matrixmultiplication.jpg" style="width:50.0%"></p>
<figcaption>Source: <a href="https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Book%3A_Mathematical_Methods_in_Chemistry_(Levitus)/15%3A_Matrices/15.03%3A_Matrix_Multiplication" class="uri">https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Book%3A_Mathematical_Methods_in_Chemistry_(Levitus)/15%3A_Matrices/15.03%3A_Matrix_Multiplication</a> CC BY-NC-SA; Marcia Levitus</figcaption>
</figure>
</div>
</section>

<section id="matrix-vector-multiplication" class="title-slide slide level1 center">
<h1>Matrix-vector multiplication</h1>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Thinking of vectors as <span class="math inline">n \times 1</span> matrices, we can multiply a matrix <span class="math inline">m \times n</span> with a vector:</li>
</ul>
<p><span class="math display">
\mathbf{y} = A \times \mathbf{x} = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix} \times \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{bmatrix}
</span></p>
<ul>
<li><p>The result <span class="math inline">\mathbf{y}</span> is a vector of size <span class="math inline">m</span>.</p></li>
<li><p>In that sense, a matrix <span class="math inline">A</span> can transform a vector of size <span class="math inline">n</span> into a vector of size <span class="math inline">m</span>:</p>
<ul>
<li><span class="math inline">A</span> represents a <strong>projection</strong> from <span class="math inline">\Re^n</span> to <span class="math inline">\Re^m</span>.</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/projection.png"></p>
<figcaption>Source: <a href="https://en.wikipedia.org/wiki/Homogeneous_coordinate" class="uri">https://en.wikipedia.org/wiki/Homogeneous_coordinate</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="dot-product-2" class="title-slide slide level1 center">
<h1>Dot product</h1>
<ul>
<li>Note that the <strong>dot product</strong> between two vectors of size <span class="math inline">n</span> is the matrix multiplication between the transpose of the first vector and the second one:</li>
</ul>
<p><span class="math display">\mathbf{x}^T \times \mathbf{y} = \begin{bmatrix} x_1 &amp; x_2 &amp; \ldots &amp; x_n \end{bmatrix} \times \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} = x_1 \, y_1 + x_2 \, y_2 + \ldots + x_n \, y_n = \langle \mathbf{x} \cdot \mathbf{y} \rangle</span></p>
</section>

<section id="matrix-inversion" class="title-slide slide level1 center">
<h1>Matrix inversion</h1>
<ul>
<li>Square matrices of size <span class="math inline">n \times n</span> can be inverted. The <strong>inverse</strong> <span class="math inline">A^{-1}</span> of a matrix <span class="math inline">A</span> is defined by:</li>
</ul>
<p><span class="math display">A \times A^{-1} = A^{-1} \times A = I</span></p>
<p>where <span class="math inline">I</span> is the identity matrix (a matrix with ones on the diagonal and 0 otherwise).</p>
<ul>
<li>Matrix inversion allows to solve linear systems of equations. Given the problem:</li>
</ul>
<p><span class="math display">
\begin{cases}
    a_{11} \, x_1 + a_{12} \, x_2 + \ldots + a_{1n} \, x_n = b_1 \\
    a_{21} \, x_1 + a_{22} \, x_2 + \ldots + a_{2n} \, x_n = b_2 \\
    \ldots \\
    a_{n1} \, x_1 + a_{n2} \, x_2 + \ldots + a_{nn} \, x_n = b_n \\
\end{cases}
</span></p>
<p>which is equivalent to:</p>
<p><span class="math display">A \times \mathbf{x} = \mathbf{b}</span></p>
<ul>
<li>We can multiply both sides to the left with <span class="math inline">A^{-1}</span> (if it exists) and obtain:</li>
</ul>
<p><span class="math display">\mathbf{x} = A^{-1} \times \mathbf{b}</span></p>
</section>

<section id="calculus" class="title-slide slide level1 center">
<h1>2 - Calculus</h1>

</section>

<section id="univariate-functions" class="title-slide slide level1 center">
<h1>Univariate functions</h1>
<ul>
<li>A <strong>univariate function</strong> <span class="math inline">f</span> associates to any real number <span class="math inline">x \in \Re</span> (or a subset of <span class="math inline">\Re</span> called the support of the function) another (unique) real number <span class="math inline">f(x)</span>:</li>
</ul>
<p><span class="math display">
\begin{align}
f\colon \quad \Re &amp;\to \Re\\
x &amp;\mapsto f(x),\end{align}
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/function.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</section>

<section id="multivariate-functions" class="title-slide slide level1 center">
<h1>Multivariate functions</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>A <strong>multivariate function</strong> <span class="math inline">f</span> associates to any vector <span class="math inline">\mathbf{x} \in \Re^n</span> (or a subset) a real number <span class="math inline">f(\mathbf{x})</span>:</li>
</ul>
<p><span class="math display">
\begin{align}
f\colon \quad \Re^n &amp;\to \Re\\
\mathbf{x} &amp;\mapsto f(\mathbf{x}),\end{align}
</span></p>
<ul>
<li><p>The variables of the function are the elements of the vector.</p></li>
<li><p>For low-dimensional vector spaces, it is possible to represent each element explicitly, for example:</p></li>
</ul>
<p><span class="math display">
\begin{align}
f\colon \quad\Re^3 &amp;\to \Re\\
x, y, z &amp;\mapsto f(x, y, z),\end{align}
</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/multivariatefunction.png"></p>
<figcaption>Source: <a href="https://en.wikipedia.org/wiki/Function_of_several_real_variables" class="uri">https://en.wikipedia.org/wiki/Function_of_several_real_variables</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="vector-fields" class="title-slide slide level1 center">
<h1>Vector fields</h1>
<ul>
<li><strong>Vector fields</strong> associate to any vector <span class="math inline">\mathbf{x} \in \Re^n</span> (or a subset) another vector (possibly of different size):</li>
</ul>
<p><span class="math display">
\begin{align}
\overrightarrow{f}\colon \quad \Re^n &amp;\to \Re^m\\
\mathbf{x} &amp;\mapsto \overrightarrow{f}(\mathbf{x}),\end{align}
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vectorfield.png" style="width:30.0%"></p>
<figcaption>Source: <a href="https://en.wikipedia.org/wiki/Vector_field" class="uri">https://en.wikipedia.org/wiki/Vector_field</a></figcaption>
</figure>
</div>
<p><strong>Note:</strong> The matrix-vector multiplication <span class="math inline">\mathbf{y} = A \times \mathbf{x}</span> is a linear vector field, mapping any vector <span class="math inline">\mathbf{x}</span> into another vector <span class="math inline">\mathbf{y}</span>.</p>
</section>

<section id="differentiation" class="title-slide slide level1 center">
<h1>Differentiation</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>Differential calculus deals with the <strong>derivative</strong> of a function, a process called differentiation.</p></li>
<li><p>The derivative <span class="math inline">f'(x)</span> or <span class="math inline">\displaystyle\frac{d f(x)}{dx}</span> of a univariate function <span class="math inline">f(x)</span> is defined as the local <em>slope</em> of the tangent to the function for a given value of <span class="math inline">x</span>:</p></li>
</ul>
<p><span class="math display">f'(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}</span></p>
<ul>
<li>The line passing through the points <span class="math inline">(x, f(x))</span> and <span class="math inline">(x + h, f(x + h))</span> becomes tangent to the function when <span class="math inline">h</span> becomes very small.</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/derivative-approx.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>

<section id="differentiation-1" class="title-slide slide level1 center">
<h1>Differentiation</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>The sign of the derivative tells you how the function behaves locally:</p>
<ul>
<li><p>If the derivative is positive, increasing a little bit <span class="math inline">x</span> increases the function <span class="math inline">f(x)</span>, so the function is <strong>locally increasing</strong>.</p></li>
<li><p>If the derivative is negative, increasing a little bit <span class="math inline">x</span> decreases the function <span class="math inline">f(x)</span>, so the function is <strong>locally decreasing</strong>.</p></li>
</ul></li>
<li><p>It basically allows you to measure the local influence of <span class="math inline">x</span> on <span class="math inline">f(x)</span>: if I change a little bit the value <span class="math inline">x</span>, what happens to <span class="math inline">f(x)</span>? This will be very useful in machine learning.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/derivative-approx.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>

<section id="extrema" class="title-slide slide level1 center">
<h1>Extrema</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>A special case is when the derivative is equal to 0 in <span class="math inline">x</span>. <span class="math inline">x</span> is then called an <strong>extremum</strong> (or optimum) of the function, i.e.&nbsp;it can be a maximum or minimum.</p></li>
<li><p>You can tell whether an extremum is a maximum or a minimum by looking at its second-order derivative:</p>
<ul>
<li><p>If <span class="math inline">f''(x) &gt; 0</span>, the extremum is a <strong>minimum</strong>.</p></li>
<li><p>If <span class="math inline">f''(x) &lt; 0</span>, the extremum is a <strong>maximum</strong>.</p></li>
<li><p>If <span class="math inline">f''(x) = 0</span>, the extremum is a <strong>saddle point</strong>.</p></li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/optimization-example.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>

<section id="gradients" class="title-slide slide level1 center">
<h1>Gradients</h1>
<ul>
<li>The derivative of a <strong>multivariate function</strong> <span class="math inline">f(\mathbf{x})</span> is a vector of partial derivatives called the <strong>gradient of the function</strong> <span class="math inline">\nabla_\mathbf{x} \, f(\mathbf{x})</span>:</li>
</ul>
<p><span class="math display">
    \nabla_\mathbf{x} \, f(\mathbf{x}) = \begin{bmatrix}
        \displaystyle\frac{\partial f(\mathbf{x})}{\partial x_1} \\
        \displaystyle\frac{\partial f(\mathbf{x})}{\partial x_2} \\
        \ldots \\
        \displaystyle\frac{\partial f(\mathbf{x})}{\partial x_n} \\
    \end{bmatrix}
</span></p>
<ul>
<li>The subscript to the <span class="math inline">\nabla</span> operator denotes <em>with respect to</em> (w.r.t) which variable the differentiation is done.</li>
</ul>
</section>

<section id="partial-derivatives" class="title-slide slide level1 center">
<h1>Partial derivatives</h1>
<ul>
<li>A <strong>partial derivative</strong> w.r.t. to particular variable (or element of the vector) is simply achieved by differentiating the function while considering all other variables to be <strong>constant</strong>. For example the function:</li>
</ul>
<p><span class="math display">f(x, y) = x^2 + 3 \, x \, y + 4 \, x \, y^2 - 1</span></p>
<p>can be partially differentiated w.r.t. <span class="math inline">x</span> and <span class="math inline">y</span> as:</p>
<p><span class="math display">\begin{cases}
\displaystyle\frac{\partial f(x, y)}{\partial x} = 2 \, x + 3\, y + 4 \, y^2 \\
\\
\displaystyle\frac{\partial f(x, y)}{\partial y} = 3 \, x + 8\, x \, y
\end{cases}
</span></p>
</section>

<section id="jacobian" class="title-slide slide level1 center">
<h1>Jacobian</h1>
<ul>
<li>The gradient can be generalized to <strong>vector fields</strong>, where the <strong>Jacobian</strong> or <strong>Jacobi matrix</strong> is a matrix containing all partial derivatives.</li>
</ul>
<p><span class="math display">
J = \begin{bmatrix}
    \dfrac{\partial \mathbf{f}}{\partial x_1} &amp; \cdots &amp; \dfrac{\partial \mathbf{f}}{\partial x_n} \end{bmatrix}
= \begin{bmatrix}
    \dfrac{\partial f_1}{\partial x_1} &amp; \cdots &amp; \dfrac{\partial f_1}{\partial x_n}\\
    \vdots &amp; \ddots &amp; \vdots\\
    \dfrac{\partial f_m}{\partial x_1} &amp; \cdots &amp; \dfrac{\partial f_m}{\partial x_n} \end{bmatrix}
</span></p>
</section>

<section id="analytical-properties" class="title-slide slide level1 center">
<h1>Analytical properties</h1>
<ul>
<li>Differentiation is linear, which means that if we define the function:</li>
</ul>
<p><span class="math display">h(x) = a \, f(x) + b \, g(x)</span></p>
<p>its derivative is:</p>
<p><span class="math display">h'(x) = a \, f'(x) + b \, g'(x)</span></p>
<ul>
<li>A product of functions can also be differentiated analytically (product rule):</li>
</ul>
<p><span class="math display">(f(x) \times g(x))' = f'(x) \times g(x) + f(x) \times g'(x)</span></p>
<p><strong>Example:</strong></p>
<p><span class="math display">f(x) = x^2 \, e^x</span></p>
<p><span class="math display">f'(x) = 2 \, x \, e^x + x^2 \cdot e^x</span></p>
</section>

<section id="chain-rule" class="title-slide slide level1 center">
<h1>Chain rule</h1>
<ul>
<li>A very important concept for neural networks is the <strong>chain rule</strong>, which tells how to differentiate <strong>function compositions</strong> (functions of a function) of the form:</li>
</ul>
<p><span class="math display">(f \circ g) (x) = f(g(x))</span></p>
<ul>
<li>The derivative of <span class="math inline">f \circ g</span> is:</li>
</ul>
<p><span class="math display">(f \circ g)' (x) = (f' \circ g) (x) \times g'(x)</span></p>
<ul>
<li>The chain rule may be more understandable using Leibniz’s notation:</li>
</ul>
<p><span class="math display">\frac{d (f \circ g) (x)}{dx} = \frac{d f (g (x))}{d g(x)} \times \frac{d g (x)}{dx}</span></p>
<ul>
<li>By posing <span class="math inline">y = g(x)</span> as an intermediary variable, it becomes:</li>
</ul>
<p><span class="math display">\frac{d f(y)}{dx} = \frac{d f(y)}{dy} \times \frac{dy}{dx}</span></p>
</section>

<section id="chain-rule-1" class="title-slide slide level1 center">
<h1>Chain rule</h1>
<ul>
<li>The function :</li>
</ul>
<p><span class="math display">h(x) = \frac{1}{2 \, x + 1}</span></p>
<p>is the function composition of <span class="math inline">g(x) = 2 \, x + 1</span> and <span class="math inline">f(x) = \displaystyle\frac{1}{x}</span>, whose derivatives are known:</p>
<p><span class="math display">g'(x) = 2</span> <span class="math display">f'(x) = -\displaystyle\frac{1}{x^2}</span></p>
<ul>
<li>Its derivative according to the <strong>chain rule</strong> is:</li>
</ul>
<p><span class="math display">h'(x) = f'(g(x)) \times g'(x) = -\displaystyle\frac{1}{(2 \, x + 1)^2} \times 2</span></p>
</section>

<section id="chain-rule-2" class="title-slide slide level1 center">
<h1>Chain rule</h1>
<ul>
<li>The chain rule also applies to partial derivatives:</li>
</ul>
<p><span class="math display">
    \displaystyle\frac{\partial f \circ g (x, y)}{\partial x} = \frac{\partial f \circ g (x, y)}{\partial g (x, y)} \times \frac{\partial g (x, y)}{\partial x}
</span></p>
<p>and gradients:</p>
<p><span class="math display">
    \nabla_\mathbf{x} \, f \circ g (\mathbf{x}) = \nabla_{g(\mathbf{x})} \, f \circ g (\mathbf{x}) \times \nabla_\mathbf{x} \, g (\mathbf{x})
</span></p>
</section>

<section id="integrals" class="title-slide slide level1 center">
<h1>Integrals</h1>
<ul>
<li>The opposite operation of differentation is <strong>integration</strong>. Given a function <span class="math inline">f(x)</span>, we search a function <span class="math inline">F(x)</span> whose <em>derivative</em> is <span class="math inline">f(x)</span>:</li>
</ul>
<p><span class="math display">F'(x) = f(x)</span></p>
<ul>
<li>The <strong>integral</strong> of <span class="math inline">f</span> is noted:</li>
</ul>
<p><span class="math display">F(x) = \int f(x) \, dx</span></p>
<p><span class="math inline">dx</span> being an infinitesimal interval (similar to <span class="math inline">h</span> in the definition of the derivative).</p>
<ul>
<li>There are tons of formal definitions of integrals (Riemann, Lebesgue, Darboux…) and we will not get into details here as we will not use integrals a lot.</li>
</ul>
</section>

<section id="integrals-1" class="title-slide slide level1 center">
<h1>Integrals</h1>
<ul>
<li><p>The most important to understand for now is maybe that the integral of a function is the <strong>area under the curve</strong>.</p></li>
<li><p>The area under the curve of a function <span class="math inline">f</span> on the interval <span class="math inline">[a, b]</span> is:</p></li>
</ul>
<p><span class="math display">\mathcal{S} = \int_a^b f(x) \, dx</span></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/riemann-sum1.svg" style="width:50.0%"></p>
<figcaption>Source: <a href="https://www.math24.net/riemann-sums-definite-integral/" class="uri">https://www.math24.net/riemann-sums-definite-integral/</a></figcaption>
</figure>
</div>
</section>

<section id="integrals-2" class="title-slide slide level1 center">
<h1>Integrals</h1>
<ul>
<li><p>One way to approximate this surface is to split the interval <span class="math inline">[a, b]</span> into <span class="math inline">n</span> intervals of width <span class="math inline">dx</span> with the points <span class="math inline">x_1, x_2, \ldots, x_n</span>.</p></li>
<li><p>This defines <span class="math inline">n</span> rectangles of width <span class="math inline">dx</span> and height <span class="math inline">f(x_i)</span>, so their surface is <span class="math inline">f(x_i) \, dx</span>.</p></li>
<li><p>The area under the curve can then be approximated by the sum of the surfaces of all these rectangles.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/riemann-sum.svg" style="width:50.0%"></p>
<figcaption>Source: <a href="https://www.math24.net/riemann-sums-definite-integral/" class="uri">https://www.math24.net/riemann-sums-definite-integral/</a></figcaption>
</figure>
</div>
</section>

<section id="integrals-3" class="title-slide slide level1 center">
<h1>Integrals</h1>
<ul>
<li>When <span class="math inline">n \to \infty</span>, or equivalently <span class="math inline">dx \to 0</span>, the sum of these rectangular areas (called the Riemann sum) becomes exactly the area under the curve. This is the definition of the definite integral:</li>
</ul>
<p><span class="math display">\int_a^b f(x) \, dx = \lim_{dx \to 0} \sum_{i=1}^n f(x_i) \, dx</span></p>
<ul>
<li>Very roughly speaking, the integral can be considered as the equivalent of a sum for continuous functions.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/riemann-sum.svg" style="width:40.0%"></p>
<figcaption>Source: <a href="https://www.math24.net/riemann-sums-definite-integral/" class="uri">https://www.math24.net/riemann-sums-definite-integral/</a></figcaption>
</figure>
</div>
</section>

<section id="probability-theory" class="title-slide slide level1 center">
<h1>3 - Probability theory</h1>

</section>

<section id="discrete-probability-distributions" class="title-slide slide level1 center">
<h1>Discrete probability distributions</h1>
<div class="columns">
<div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dice.svg" style="width:70.0%"></p>
<figcaption>Credit: <a href="https://commons.wikimedia.org/wiki/File:2-Dice-Icon.svg" class="uri">https://commons.wikimedia.org/wiki/File:2-Dice-Icon.svg</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:70%;">
<ul>
<li><p>Let’s note <span class="math inline">X</span> a <strong>discrete random variable</strong> with <span class="math inline">n</span> realizations (or outcomes) <span class="math inline">x_1, \ldots, x_n</span>.</p></li>
<li><p>The <strong>probability</strong> that <span class="math inline">X</span> takes the value <span class="math inline">x_i</span> is defined by the <em>relative frequency of occurrence</em>, i.e.&nbsp;the proportion of samples having the value <span class="math inline">x_i</span>, when the total number <span class="math inline">N</span> of samples tends to infinity:</p></li>
</ul>
<p><span class="math display">
    P(X = x_i) = \frac{\text{Number of favorable cases}}{\text{Total number of samples}}
</span></p>
</div></div>
<ul>
<li><p>The set of probabilities <span class="math inline">\{P(X = x_i)\}_{i=1}^n</span> define the <strong>probability distribution</strong> for the random variable (or probability mass function, pmf).</p></li>
<li><p>By definition, we have <span class="math inline">0 \leq P(X = x_i) \leq 1</span> and the probabilities <strong>have</strong> to respect:</p></li>
</ul>
<p><span class="math display">
    \sum_{i=1}^n P(X = x_i) = 1
</span></p>
</section>

<section id="mathematical-expectation-and-variance" class="title-slide slide level1 center">
<h1>Mathematical expectation and variance</h1>
<ul>
<li>An important metric for a random variable is its <strong>mathematical expectation</strong> or expected value, i.e.&nbsp;its “mean” realization weighted by the probabilities:</li>
</ul>
<p><span class="math display">
    \mathbb{E}[X] = \sum_{i=1}^n P(X = x_i) \, x_i
</span></p>
<ul>
<li>The expectation does not even need to be a valid realization:</li>
</ul>
<p><span class="math display">
    \mathbb{E}[\text{Coin}] = \frac{1}{2} \, 0 + \frac{1}{2} \, 1 = 0.5
</span></p>
<p><span class="math display">
    \mathbb{E}[\text{Dice}] = \frac{1}{6} \, (1 + 2 + 3 + 4 + 5 + 6) = 3.5
</span></p>
<ul>
<li>We can also compute the mathematical expectation of <strong>functions of</strong> a random variable:</li>
</ul>
<p><span class="math display">
    \mathbb{E}[f(X)] = \sum_{i=1}^n P(X = x_i) \, f(x_i)
</span></p>
</section>

<section id="mathematical-expectation-and-variance-1" class="title-slide slide level1 center">
<h1>Mathematical expectation and variance</h1>
<ul>
<li>The <strong>variance</strong> of a random variable is the squared deviation around the mean:</li>
</ul>
<p><span class="math display">
    \text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \sum_{i=1}^n P(X = x_i) \, (x_i - \mathbb{E}[X])^2
</span></p>
<ul>
<li>Variance of a coin:</li>
</ul>
<p><span class="math display">
    \text{Var}(\text{Coin}) = \frac{1}{2} \, (0 - 0.5)^2 + \frac{1}{2} \, (1 - 0.5)^2 = 0.25
</span></p>
<ul>
<li>Variance of a dice:</li>
</ul>
<p><span class="math display">
    \text{Var}(\text{Dice}) = \frac{1}{6} \, ((1-3.5)^2 + (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (6-3.5)^2) = \frac{105}{36}
</span></p>
</section>

<section id="continuous-probability-distributions" class="title-slide slide level1 center">
<h1>Continuous probability distributions</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/normaldistribution.png"></p>
<figcaption>Source: <a href="https://en.wikipedia.org/wiki/Normal_distribution" class="uri">https://en.wikipedia.org/wiki/Normal_distribution</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li><p><strong>Continuous random variables</strong> can take an infinity of continuous values, e.g.&nbsp;<span class="math inline">\Re</span> or some subset.</p></li>
<li><p>The closed set of values they can take is called the <strong>support</strong> <span class="math inline">\mathcal{D}_X</span> of the probability distribution.</p></li>
<li><p>The probability distribution is described by a <strong>probability density function</strong> (pdf) <span class="math inline">f(x)</span>.</p></li>
<li><p>The pdf of a distribution must be positive (<span class="math inline">f(x) \geq 0 \, \forall x \in \mathcal{D}_X</span>) and its integral must be equal to 1:</p></li>
</ul>
<p><span class="math display">
    \int_{x \in \mathcal{D}_X} f(x) \, dx = 1
</span></p>
</div></div>
<ul>
<li>The pdf does not give the probability of taking a particular value <span class="math inline">x</span> (it is 0), but allows to get the probability that a value lies in a specific interval:</li>
</ul>
<p><span class="math display">
    P(a \leq X \leq b) = \int_{a}^b f(x) \, dx
</span></p>
<ul>
<li>One can however think of the pdf as the <strong>likelihood</strong> that a value <span class="math inline">x</span> comes from that distribution.</li>
</ul>
</section>

<section id="expectation-and-variance-of-continuous-distributions" class="title-slide slide level1 center">
<h1>Expectation and variance of continuous distributions</h1>
<ul>
<li>The mathematical expectation is now defined by an integral instead of a sum:</li>
</ul>
<p><span class="math display">
    \mathbb{E}[X] = \int_{x \in \mathcal{D}_X} f(x) \, x \, dx
</span></p>
<p>the variance:</p>
<p><span class="math display">
    \text{Var}(X) = \int_{x \in \mathcal{D}_X} f(x) \, (x - \mathbb{E}[X])^2 \, dx
</span></p>
<p>or a function of the random variable:</p>
<p><span class="math display">
    \mathbb{E}[g(X)] = \int_{x \in \mathcal{D}_X} f(x) \, g(x) \, dx
</span></p>
<ul>
<li>Note that the expectation operator is <strong>linear</strong>:</li>
</ul>
<p><span class="math display">
    \mathbb{E}[a \, X + b \, Y] = a \, \mathbb{E}[X] + b \, \mathbb{E}[Y]
</span></p>
</section>

<section id="some-parameterized-probability-distributions" class="title-slide slide level1 center">
<h1>Some parameterized probability distributions</h1>
<ul>
<li><p>Probability distributions can in principle have any form: <span class="math inline">f(x)</span> is unknown.</p></li>
<li><p>However, specific parameterized distributions can be very useful: their pmf/pdf is fully determined by a couple of parameters.</p></li>
<li><p>The <strong>Bernouilli</strong> distribution is a binary (discrete, 0 or 1) distribution with a parameter <span class="math inline">p</span> specifying the probability to obtain the outcome 1:</p></li>
</ul>
<p><span class="math display">
    P(X = 1) = p \; \text{and} \; P(X=0) = 1 - p
</span> <span class="math display">P(X=x) = p^x \, (1-p)^{1-x}</span> <span class="math display">\mathbb{E}[X] = p</span></p>
<ul>
<li>The <strong>Multinouilli</strong> or <strong>categorical</strong> distribution is a discrete distribution with <span class="math inline">k</span> realizations. Each realization <span class="math inline">x_i</span> is associated with a parameter <span class="math inline">p_i &gt;0</span> representing its probability. We have <span class="math inline">\sum_i p_i = 1</span>.</li>
</ul>
<p><span class="math display">P(X = x_i) = p_i</span></p>
<ul>
<li>Knowing <span class="math inline">p</span> or the <span class="math inline">p_i</span> tells us everything about the discrete distributions.</li>
</ul>
</section>

<section id="the-uniform-distribution" class="title-slide slide level1 center">
<h1>The uniform distribution</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/uniformdistribution.png"></p>
<figcaption>Credit: <a href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)" class="uri">https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>The <strong>uniform distribution</strong> has an equal and constant probability of returning values between <span class="math inline">a</span> and <span class="math inline">b</span>, never outside this range.</p></li>
<li><p>It is parameterized by two parameters:</p>
<ul>
<li><p>the start of the range <span class="math inline">a</span>.</p></li>
<li><p>the end of the range <span class="math inline">b</span>.</p></li>
</ul></li>
<li><p>Its support is <span class="math inline">[a, b]</span>.</p></li>
</ul>
</div></div>
<ul>
<li>The pdf of the uniform distribution <span class="math inline">\mathcal{U}(a, b)</span> is defined on <span class="math inline">[a, b]</span> as:</li>
</ul>
<p><span class="math display">
    f(x; a, b) = \frac{1}{b - a}
</span></p>
<ul>
<li>Knowing <span class="math inline">a</span> and <span class="math inline">b</span> completely defines the distribution.</li>
</ul>
</section>

<section id="the-normal-or-gaussian-distribution" class="title-slide slide level1 center">
<h1>The normal or Gaussian distribution</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/normaldistribution.png"></p>
<figcaption>Credit: <a href="https://en.wikipedia.org/wiki/Normal_distribution" class="uri">https://en.wikipedia.org/wiki/Normal_distribution</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>For continuous distributions, the <strong>normal distribution</strong> is the most frequently encountered one.</p></li>
<li><p>It is parameterized by two parameters:</p>
<ul>
<li><p>the mean <span class="math inline">\mu</span>.</p></li>
<li><p>the variance <span class="math inline">\sigma^2</span> (or standard deviation <span class="math inline">\sigma</span>).</p></li>
</ul></li>
<li><p>Its support is <span class="math inline">\Re</span>.</p></li>
</ul>
</div></div>
<ul>
<li>The pdf of the normal distribution <span class="math inline">\mathcal{N}(\mu, \sigma)</span> is defined on <span class="math inline">\Re</span> as:</li>
</ul>
<p><span class="math display">
    f(x; \mu, \sigma) = \frac{1}{\sqrt{2\,\pi\,\sigma^2}} \, e^{-\displaystyle\frac{(x - \mu)^2}{2\,\sigma^2}}
</span></p>
<ul>
<li>Knowing <span class="math inline">\mu</span> and <span class="math inline">\sigma</span> completely defines the distribution.</li>
</ul>
</section>

<section id="the-exponential-distribution" class="title-slide slide level1 center">
<h1>The exponential distribution</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/exponentialdistribution.png"></p>
<figcaption>Credit: <a href="https://en.wikipedia.org/wiki/Exponential_distribution" class="uri">https://en.wikipedia.org/wiki/Exponential_distribution</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>The <strong>exponential distribution</strong> is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate.</p></li>
<li><p>It is parameterized by one parameter:</p>
<ul>
<li>the rate <span class="math inline">\lambda</span>.</li>
</ul></li>
<li><p>Its support is <span class="math inline">\Re^+</span> (<span class="math inline">x &gt; 0</span>).</p></li>
</ul>
</div></div>
<ul>
<li>The pdf of the exponential distribution is defined on <span class="math inline">\Re^+</span> as:</li>
</ul>
<p><span class="math display">
    f(x; \lambda) = \lambda \, e^{-\lambda \, x}
</span></p>
<ul>
<li>Knowing <span class="math inline">\lambda</span> completely defines the distribution.</li>
</ul>
</section>

<section id="joint-probabilities" class="title-slide slide level1 center">
<h1>Joint probabilities</h1>
<ul>
<li><p>Let’s now suppose that we have two random variables <span class="math inline">X</span> and <span class="math inline">Y</span> with different probability distributions <span class="math inline">P(X)</span> and <span class="math inline">P(Y)</span>.</p></li>
<li><p>The <strong>joint probability</strong> <span class="math inline">P(X, Y)</span> denotes the probability of observing the realizations <span class="math inline">x</span> <strong>and</strong> <span class="math inline">y</span> at the same time:</p></li>
</ul>
<p><span class="math display">P(X=x, Y=y)</span></p>
<ul>
<li>If the random variables are <strong>independent</strong>, we have:</li>
</ul>
<p><span class="math display">P(X=x, Y=y) = P(X=x) \, P(Y=y)</span></p>
<ul>
<li>If you know the joint probability, you can compute the <strong>marginal probability distribution</strong> of each variable:</li>
</ul>
<p><span class="math display">P(X=x) = \sum_y P(X=x, Y=y)</span></p>
<ul>
<li>The same is true for continuous probability distributions:</li>
</ul>
<p><span class="math display">
    f(x) = \int f(x, y) \, dy
</span></p>
</section>

<section id="conditional-probabilities" class="title-slide slide level1 center">
<h1>Conditional probabilities</h1>
<ul>
<li><p>Some useful information between two random variables is the <strong>conditional probability</strong>.</p></li>
<li><p><span class="math inline">P(X=x | Y=y)</span> is the conditional probability that <span class="math inline">X=x</span>, <strong>given</strong> that <span class="math inline">Y=y</span> is observed.</p></li>
<li><p><span class="math inline">Y=y</span> is not random anymore: it is a <strong>fact</strong> (at least theoretically).</p></li>
<li><p>You wonder what happens to the probability distribution of <span class="math inline">X</span> now that you know the value of <span class="math inline">Y</span>.</p></li>
<li><p>Conditional probabilities are linked to the joint probability by:</p></li>
</ul>
<p><span class="math display">
    P(X=x | Y=y) = \frac{P(X=x, Y=y)}{P(Y=y)}
</span></p>
<ul>
<li><p>If <span class="math inline">X</span> and <span class="math inline">Y</span> are <strong>independent</strong>, we have <span class="math inline">P(X=x | Y=y) = P(X=x)</span> (knowing <span class="math inline">Y</span> does not change anything to the probability distribution of <span class="math inline">X</span>).</p></li>
<li><p>We can use the same notation for the complete probability distributions:</p></li>
</ul>
<p><span class="math display">
    P(X | Y) = \frac{P(X, Y)}{P(Y)}
</span></p>
</section>

<section id="joint-and-conditional-probabilities-using-a-venn-diagram" class="title-slide slide level1 center">
<h1>Joint and conditional probabilities: using a Venn diagram</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/conditionalprobability.png"></p>
<figcaption>Credit: <a href="https://www.elevise.co.uk/g-e-m-h-5-u.html" class="uri">https://www.elevise.co.uk/g-e-m-h-5-u.html</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li><p>You ask 50 people whether they like cats or dogs:</p>
<ul>
<li>18 like both cats and dogs.</li>
<li>21 like only dogs.</li>
<li>5 like only cats.</li>
<li>6 like none of them.</li>
</ul></li>
<li><p>We consider loving cats and dogs as random variables (and that our sample size is big enough to use probabilities…)</p></li>
<li><p>We have <span class="math inline">P(\text{dog}) = \dfrac{18+21}{50}</span> and <span class="math inline">P(\text{cat}) = \dfrac{18+5}{50}</span>.</p></li>
<li><p>Among the 23 who love cats, which proportion also loves dogs?</p></li>
</ul>
</div></div>
<ul>
<li><p>The joint probability of loving both cats and dogs is <span class="math inline">P(\text{cat}, \text{dog}) = \dfrac{18}{50}</span>.</p></li>
<li><p>The conditional probability of loving dogs given one loves cats is:</p></li>
</ul>
<p><span class="math display">P(\text{dog} | \text{cat}) = \dfrac{P(\text{cat}, \text{dog})}{P(\text{cat})} = \dfrac{\dfrac{18}{50}}{\dfrac{23}{50}} = \dfrac{18}{23}</span></p>
</section>

<section id="bayes-rule" class="title-slide slide level1 center">
<h1>Bayes’ rule</h1>
<ul>
<li>Noticing that the definition of conditional probabilities is symmetric:</li>
</ul>
<p><span class="math display">
    P(X, Y) = P(X | Y) \, P(Y) = P(Y | X) \, P(X)
</span></p>
<p>we can obtain the <strong>Bayes’ rule</strong>:</p>
<p><span class="math display">
    P(Y | X) = \frac{P(X|Y) \, P(Y)}{P(X)}
</span></p>
<ul>
<li><p>It is very useful when you already know <span class="math inline">P(X|Y)</span> and want to obtain <span class="math inline">P(Y|X)</span> (<strong>Bayesian inference</strong>).</p>
<ul>
<li><p><span class="math inline">P(Y | X)</span> is called the <strong>posterior probability</strong>.</p></li>
<li><p><span class="math inline">P(X | Y)</span> is called the <strong>likelihood</strong>.</p></li>
<li><p><span class="math inline">P(Y)</span> is called the <strong>prior probability</strong> (belief).</p></li>
<li><p><span class="math inline">P(X)</span> is called the <strong>model evidence</strong> or <strong>marginal likelihood</strong>.</p></li>
</ul></li>
</ul>
</section>

<section id="bayes-rule-example" class="title-slide slide level1 center">
<h1>Bayes’ rule : example</h1>
<ul>
<li>Let’s consider a disease <span class="math inline">D</span> (binary random variable) and a medical test <span class="math inline">T</span> (also binary). The disease affects 10% of the general population:</li>
</ul>
<p><span class="math display">P(D=1)= 0.1 \qquad \qquad P(D=0)=0.9</span></p>
<ul>
<li>When a patient has the disease, the test is positive 80% of the time:</li>
</ul>
<p><span class="math display">P(T=1 | D=1) = 0.8 \qquad \qquad P(T=0 | D=1) = 0.2</span></p>
<ul>
<li>When a patient does not have the disease, the test is still positive 10% of the time:</li>
</ul>
<p><span class="math display">P(T=1 | D=0) = 0.1 \qquad \qquad P(T=0 | D=0) = 0.9</span></p>
<ul>
<li>Given that the test is positive, what is the probability that the patient is ill?</li>
</ul>
</section>

<section id="bayes-rule-example-1" class="title-slide slide level1 center">
<h1>Bayes’ rule : example</h1>
<p><span class="math display">
\begin{aligned}
    P(D=1|T=1) &amp;= \frac{P(T=1 | D=1) \, P(D=1)}{P(T=1)} \\
               &amp;\\
               &amp;= \frac{P(T=1 | D=1) \, P(D=1)}{P(T=1 | D=1) \, P(D=1) + P(T=1 | D=0) \, P(D=0)} \\
               &amp;\\
               &amp;= \frac{0.8 \times 0.1}{0.8 \times 0.1 + 0.1 \times 0.9} \\
               &amp;\\
               &amp; = 0.47 \\
\end{aligned}
</span></p>
</section>

<section id="statistics" class="title-slide slide level1 center">
<h1>4 - Statistics</h1>

</section>

<section id="random-sampling-monte-carlo-sampling" class="title-slide slide level1 center">
<h1>Random sampling / Monte Carlo sampling</h1>
<ul>
<li><p>In ML, we will deal with random variables whose exact probability distribution is unknown, but we are interested in their expectation or variance anyway.</p></li>
<li><p><strong>Random sampling</strong> or <strong>Monte Carlo sampling</strong> (MC) consists of taking <span class="math inline">N</span> samples <span class="math inline">x_i</span> out of the distribution <span class="math inline">X</span> (discrete or continuous) and computing the <strong>sample average</strong>: <span class="math display">
  \mathbb{E}[X] = \mathbb{E}_{x \sim X} [x] \approx \frac{1}{N} \, \sum_{i=1}^N x_i
</span></p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/normaldistribution.svg" class="quarto-figure quarto-figure-center" style="width:50.0%"></p>
</figure>
</div>
<ul>
<li>More samples will be obtained where <span class="math inline">f(x)</span> is high (<span class="math inline">x</span> is probable), so the average of the sampled data will be close to the expected value of the distribution.</li>
</ul>
</section>

<section id="random-sampling-monte-carlo-sampling-1" class="title-slide slide level1 center">
<h1>Random sampling / Monte Carlo sampling</h1>
<p><strong>Law of big numbers</strong></p>
<blockquote>
<p>As the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean.</p>
</blockquote>
<p>MC estimates are only correct when:</p>
<ul>
<li><p>the samples are <strong>i.i.d</strong> (independent and identically distributed):</p>
<ul>
<li><p>independent: the samples must be unrelated with each other.</p></li>
<li><p>identically distributed: the samples must come from the same distribution <span class="math inline">X</span>.</p></li>
</ul></li>
<li><p>the number of samples is large enough. Usually <span class="math inline">N &gt; 30</span> for simple distributions.</p></li>
</ul>
</section>

<section id="random-sampling-monte-carlo-sampling-2" class="title-slide slide level1 center">
<h1>Random sampling / Monte Carlo sampling</h1>
<ul>
<li>One can estimate any function of the random variable with random sampling:</li>
</ul>
<p><span class="math display">
    \mathbb{E}[f(X)] = \mathbb{E}_{x \sim X} [f(x)] \approx \frac{1}{N} \, \sum_{i=1}^N f(x_i)
</span></p>
<ul>
<li>Example of Monte Carlo sampling to estimate <span class="math inline">\pi/4</span>:</li>
</ul>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="4">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/montecarlo-1.jpeg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/montecarlo-2.jpeg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/montecarlo-3.jpeg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/montecarlo-4.jpeg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div>
</div>
</div>
<div class="footer">
<p>Credit <a href="https://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694" class="uri">https://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694</a></p>
</div>
</section>

<section id="central-limit-theorem" class="title-slide slide level1 center">
<h1>Central limit theorem</h1>
<ul>
<li><p>Suppose we have an unknown distribution <span class="math inline">X</span> with expected value <span class="math inline">\mu = \mathbb{E}[X]</span> and variance <span class="math inline">\sigma^2</span>.</p></li>
<li><p>We can take randomly <span class="math inline">N</span> samples from <span class="math inline">X</span> to compute the sample average:</p></li>
</ul>
<p><span class="math display">
    S_N = \frac{1}{N} \, \sum_{i=1}^N x_i
</span></p>
<ul>
<li>The <strong>Central Limit Theorem</strong> (CLT) states that:</li>
</ul>
<blockquote>
<p>The distribution of sample averages is normally distributed with mean <span class="math inline">\mu</span> and variance <span class="math inline">\frac{\sigma^2}{N}</span>.</p>
</blockquote>
<p><span class="math display">S_N \sim \mathcal{N}(\mu, \frac{\sigma}{\sqrt{N}})</span></p>
</section>

<section id="central-limit-theorem-1" class="title-slide slide level1 center">
<h1>Central limit theorem</h1>
<ul>
<li><p>If we perform the sampling multiple times, even with few samples, the average of the sampling averages will be very close to the expected value.</p></li>
<li><p>The more samples we get, the smaller the variance of the estimates.</p></li>
<li><p>Although the distribution <span class="math inline">X</span> can be anything, the sampling averages are normally distributed.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/IllustrationCentralTheorem.png" style="width:70.0%"></p>
<figcaption>Credit: <a href="https://en.wikipedia.org/wiki/Central_limit_theorem" class="uri">https://en.wikipedia.org/wiki/Central_limit_theorem</a></figcaption>
</figure>
</div>
</section>

<section id="estimators" class="title-slide slide level1 center">
<h1>Estimators</h1>
<ul>
<li>CLT shows that the sampling average is an <strong>unbiased estimator</strong> of the expected value of a distribution:</li>
</ul>
<p><span class="math display">\mathbb{E}(S_N) = \mathbb{E}(X)</span></p>
<ul>
<li><p>An estimator is a random variable used to measure parameters of a distribution (e.g.&nbsp;its expectation). The problem is that estimators can generally be <strong>biased</strong>.</p></li>
<li><p>Take the example of a thermometer <span class="math inline">M</span> measuring the temperature <span class="math inline">T</span>. <span class="math inline">T</span> is a random variable (normally distributed with <span class="math inline">\mu=20</span> and <span class="math inline">\sigma=10</span>) and the measurements <span class="math inline">M</span> relate to the temperature with the relation:</p></li>
</ul>
<p><span class="math display">
    M = 0.95 \, T + 0.65
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/estimators-temperature.png" class="quarto-figure quarto-figure-center" style="width:70.0%"></p>
</figure>
</div>
</section>

<section id="estimators-1" class="title-slide slide level1 center">
<h1>Estimators</h1>
<ul>
<li><p>The thermometer is not perfect, but do random measurements allow us to estimate the expected value of the temperature?</p></li>
<li><p>We could repeatedly take 100 random samples of the thermometer and see how the distribution of sample averages look like:</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/estimators-temperature2.png" class="quarto-figure quarto-figure-center" style="width:40.0%"></p>
</figure>
</div>
<ul>
<li>But, as the expectation is linear, we actually have:</li>
</ul>
<p><span class="math display">
    \mathbb{E}[M] = \mathbb{E}[0.95 \, T + 0.65] = 0.95 \, \mathbb{E}[T] + 0.65 = 19.65 \neq \mathbb{E}[T]
</span></p>
<ul>
<li>The thermometer is a <strong>biased estimator</strong> of the temperature.</li>
</ul>
</section>

<section id="estimators-2" class="title-slide slide level1 center">
<h1>Estimators</h1>
<ul>
<li><p>Let’s note <span class="math inline">\theta</span> a parameter of a probability distribution <span class="math inline">X</span> that we want to estimate (it does not have to be its mean).</p></li>
<li><p>An <strong>estimator</strong> <span class="math inline">\hat{\theta}</span> is a random variable mapping the sample space of <span class="math inline">X</span> to a set of sample estimates.</p></li>
<li><p>The <strong>bias</strong> of an estimator is the mean error made by the estimator:</p></li>
</ul>
<p><span class="math display">
    \mathcal{B}(\hat{\theta}) = \mathbb{E}[\hat{\theta} - \theta] = \mathbb{E}[\hat{\theta}] - \theta
</span></p>
<ul>
<li>The <strong>variance</strong> of an estimator is the deviation of the samples around the expected value:</li>
</ul>
<p><span class="math display">
    \text{Var}(\hat{\theta}) = \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}] )^2]
</span></p>
<ul>
<li><p>Ideally, we would like estimators with:</p>
<ul>
<li><p><strong>low bias</strong>: the estimations are correct on average (= equal to the true parameter).</p></li>
<li><p><strong>low variance</strong>: we do not need many estimates to get a correct estimate (CLT: <span class="math inline">\frac{\sigma}{\sqrt{N}}</span>)</p></li>
</ul></li>
</ul>
</section>

<section id="estimators-bias-and-variance" class="title-slide slide level1 center">
<h1>Estimators: bias and variance</h1>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/biasvariance3.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<ul>
<li><p>Unfortunately, the perfect estimator does not exist.</p></li>
<li><p>Estimators will have a bias and a variance:</p>
<ul>
<li><p><strong>Bias</strong>: the estimated values will be wrong, and the policy not optimal.</p></li>
<li><p><strong>Variance</strong>: we will need a lot of samples (trial and error) to have correct estimates.</p></li>
</ul></li>
<li><p>One usually talks of a <strong>bias/variance</strong> trade-off: if you have a small bias, you will have a high variance, or vice versa.</p></li>
<li><p>In machine learning, bias corresponds to underfitting, variance to overfitting.</p></li>
</ul>
</div></div>
</section>

<section id="information-theory" class="title-slide slide level1 center">
<h1>5 - Information theory</h1>

</section>

<section id="information" class="title-slide slide level1 center">
<h1>Information</h1>
<ul>
<li><p><strong>Information theory</strong> (Claude Shannon) asks how much information is contained in a probability distribution.</p></li>
<li><p>Information is related to <strong>surprise</strong> or <strong>uncertainty</strong>: are the outcomes of a random variable surprising?</p>
<ul>
<li><p>Almost certain outcomes (<span class="math inline">P \sim 1</span>) are not surprising because they happen all the time.</p></li>
<li><p>Almost impossible outcomes (<span class="math inline">P \sim 0</span>) are very surprising because they are very rare.</p></li>
</ul></li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/selfinformation.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>A useful measurement of how surprising is an outcome <span class="math inline">x</span> is the <strong>self-information</strong>:</li>
</ul>
<p><span class="math display">
    I (x) = - \log P(X = x)
</span></p>
<ul>
<li><p>Depending on which log is used, self-information has different units:</p>
<ul>
<li><p><span class="math inline">\log_2</span>: bits or shannons.</p></li>
<li><p><span class="math inline">\log_e = \ln</span>: nats.</p></li>
</ul></li>
<li><p>But it is just a rescaling, the base never matters.</p></li>
</ul>
</div></div>
</section>

<section id="entropy" class="title-slide slide level1 center">
<h1>Entropy</h1>
<ul>
<li>The <strong>entropy</strong> (or Shannon entropy) of a probability distribution is the expected value of the self-information of its outcomes:</li>
</ul>
<p><span class="math display">
    H(X) = \mathbb{E}_{x \sim X} [I(x)] = \mathbb{E}_{x \sim X} [- \log P(X = x)]
</span></p>
<ul>
<li>It measures the <strong>uncertainty</strong>, <strong>randomness</strong> or <strong>information content</strong> of the random variable.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>In the discrete case:</li>
</ul>
<p><span class="math display">
    H(X) = - \sum_x P(x) \, \log P(x)
</span></p>
<ul>
<li>In the continuous case:</li>
</ul>
<p><span class="math display">
    H(X) = - \int_x f(x) \, \log f(x) \, dx
</span></p>
<ul>
<li><p>The entropy of a Bernouilli variable is maximal when both outcomes are <strong>equiprobable</strong>.</p></li>
<li><p>If a variable is <strong>deterministic</strong>, its entropy is minimal and equal to zero.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/entropy-binomial.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>

<section id="joint-and-conditional-entropies" class="title-slide slide level1 center">
<h1>Joint and conditional entropies</h1>
<ul>
<li>The <strong>joint entropy</strong> of two random variables <span class="math inline">X</span> and <span class="math inline">Y</span> is defined by:</li>
</ul>
<p><span class="math display">
    H(X, Y) = \mathbb{E}_{x \sim X, y \sim Y} [- \log P(X=x, Y=y)]
</span></p>
<ul>
<li>The <strong>conditional entropy</strong> of two random variables <span class="math inline">X</span> and <span class="math inline">Y</span> is defined by:</li>
</ul>
<p><span class="math display">
    H(X | Y) = \mathbb{E}_{x \sim X, y \sim Y} [- \log P(X=x | Y=y)]  = \mathbb{E}_{x \sim X, y \sim Y} [- \log \frac{P(X=x , Y=y)}{P(Y=y)}]
</span></p>
<ul>
<li>If the variables are <strong>independent</strong>, we have:</li>
</ul>
<p><span class="math display">
    H(X, Y) = H(X) + H(Y) \qquad \text{or} \qquad H(X | Y) = H(X)
</span></p>
<ul>
<li>Both are related by:</li>
</ul>
<p><span class="math display">
    H(X | Y) = H(X, Y) - H(Y)
</span></p>
<ul>
<li>The equivalent of Bayes’ rule is:</li>
</ul>
<p><span class="math display">
    H(Y |X) = H(X |Y) + H(Y) - H(X)
</span></p>
</section>

<section id="mutual-information" class="title-slide slide level1 center">
<h1>Mutual Information</h1>
<ul>
<li>The most important information measurement between two variables is the <strong>mutual information</strong> MI (or information gain):</li>
</ul>
<p><span class="math display">
    I(X, Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)
</span></p>
<ul>
<li><p>It measures how much information the variable <span class="math inline">X</span> holds on <span class="math inline">Y</span>:</p>
<ul>
<li>If the two variables are <strong>independent</strong>, the MI is 0 : <span class="math inline">X</span> is as random, whether you know <span class="math inline">Y</span> or not.</li>
</ul>
<p><span class="math display">
      I (X, Y) = 0
  </span></p>
<ul>
<li>If the two variables are <strong>dependent</strong>, knowing <span class="math inline">Y</span> gives you information on <span class="math inline">X</span>, which becomes less random, i.e.&nbsp;less uncertain / surprising.</li>
</ul>
<p><span class="math display">
      I (X, Y) &gt; 0
  </span></p></li>
<li><p>If you can fully predict <span class="math inline">X</span> when you know <span class="math inline">Y</span>, it becomes deterministic (<span class="math inline">H(X|Y)=0</span>) so the mutual information is maximal (<span class="math inline">I(X, Y) = H(X)</span>).</p></li>
</ul>
</section>

<section id="cross-entropy" class="title-slide slide level1 center">
<h1>Cross-entropy</h1>
<ul>
<li>The <strong>cross-entropy</strong> between two distributions <span class="math inline">X</span> and <span class="math inline">Y</span> is defined as:</li>
</ul>
<p><span class="math display">
    H(X, Y) = \mathbb{E}_{x \sim X}[- \log P(Y=x)]
</span></p>
<ul>
<li><p>Beware that the notation <span class="math inline">H(X, Y)</span> is the same as the joint entropy, but it is a different concept!</p></li>
<li><p>The cross-entropy measures the <strong>negative log-likelihood</strong> that a sample <span class="math inline">x</span> taken from the distribution <span class="math inline">X</span> could also come from the distribution <span class="math inline">Y</span>.</p></li>
<li><p>More exactly, it measures how many bits of information one would need to distinguish the two distributions <span class="math inline">X</span> and <span class="math inline">Y</span>.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/crossentropy.svg" class="quarto-figure quarto-figure-center" style="width:80.0%"></p>
</figure>
</div>
</section>

<section id="cross-entropy-1" class="title-slide slide level1 center">
<h1>Cross-entropy</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/crossentropy.svg" class="quarto-figure quarto-figure-center" style="width:80.0%"></p>
</figure>
</div>
<p><span class="math display">
    H(X, Y) = \mathbb{E}_{x \sim X}[- \log P(Y=x)]
</span></p>
<ul>
<li><p>If the two distributions are the same <em>almost anywhere</em>, one cannot distinguish samples from the two distributions:</p>
<ul>
<li>The cross-entropy is the same as the entropy of <span class="math inline">X</span>.</li>
</ul></li>
<li><p>If the two distributions are completely different, one can tell whether a sample <span class="math inline">Z</span> comes from <span class="math inline">X</span> or <span class="math inline">Y</span>:</p>
<ul>
<li>The cross-entropy is higher than the entropy of <span class="math inline">X</span>.</li>
</ul></li>
</ul>
</section>

<section id="kullback-leibler-divergence" class="title-slide slide level1 center">
<h1>Kullback-Leibler divergence</h1>
<ul>
<li>In practice, the <strong>Kullback-Leibler divergence</strong> <span class="math inline">\text{KL}(X ||Y)</span> is a better measurement of the similarity (statistical distance) between two probability distributions:</li>
</ul>
<p><span class="math display">
    \text{KL}(X ||Y) = \mathbb{E}_{x \sim X}[- \log \frac{P(Y=x)}{P(X=x)}]
</span></p>
<ul>
<li>It is linked to the cross-entropy by:</li>
</ul>
<p><span class="math display">
    \text{KL}(X ||Y) = H(X, Y) - H(X)
</span></p>
<ul>
<li><p>If the two distributions are the same <em>almost anywhere</em>:</p>
<ul>
<li>The KL divergence is zero.</li>
</ul></li>
<li><p>If the two distributions are different:</p>
<ul>
<li>The KL divergence is positive.</li>
</ul></li>
<li><p>Minimizing the KL between two distributions is the same as making the two distributions “equal”.</p></li>
<li><p>Again, the KL is not a metric, as it is not symmetric.</p></li>
</ul>

<div class="quarto-auto-generated-content">
<div class="footer footer-default">

</div>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false,"theme":"whiteboard"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/vitay\.github\.io\/course-neurocomputing\/");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>