<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-0626ff4d7a71b55c8707dcae1d04a9b6.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.39">

  <meta name="author" content="Julien Vitay">
  <title>Neurocomputing</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #24292e;  }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #24292e; } /* Normal */
    code span.al { color: #ff5555; font-weight: bold; } /* Alert */
    code span.an { color: #6a737d; } /* Annotation */
    code span.at { color: #d73a49; } /* Attribute */
    code span.bn { color: #005cc5; } /* BaseN */
    code span.bu { color: #d73a49; } /* BuiltIn */
    code span.cf { color: #d73a49; } /* ControlFlow */
    code span.ch { color: #032f62; } /* Char */
    code span.cn { color: #005cc5; } /* Constant */
    code span.co { color: #6a737d; } /* Comment */
    code span.cv { color: #6a737d; } /* CommentVar */
    code span.do { color: #6a737d; } /* Documentation */
    code span.dt { color: #d73a49; } /* DataType */
    code span.dv { color: #005cc5; } /* DecVal */
    code span.er { color: #ff5555; text-decoration: underline; } /* Error */
    code span.ex { color: #d73a49; font-weight: bold; } /* Extension */
    code span.fl { color: #005cc5; } /* Float */
    code span.fu { color: #6f42c1; } /* Function */
    code span.im { color: #032f62; } /* Import */
    code span.in { color: #6a737d; } /* Information */
    code span.kw { color: #d73a49; } /* Keyword */
    code span.op { color: #24292e; } /* Operator */
    code span.ot { color: #6f42c1; } /* Other */
    code span.pp { color: #d73a49; } /* Preprocessor */
    code span.re { color: #6a737d; } /* RegionMarker */
    code span.sc { color: #005cc5; } /* SpecialChar */
    code span.ss { color: #032f62; } /* SpecialString */
    code span.st { color: #032f62; } /* String */
    code span.va { color: #e36209; } /* Variable */
    code span.vs { color: #032f62; } /* VerbatimString */
    code span.wa { color: #ff5555; } /* Warning */
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-31e5e70a95169d1719a5a4fd7b5514a9.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="img/tuc-new-large.png" data-background-opacity="1" data-background-position="top" data-background-size="30%" class="quarto-title-block center">
  <h1 class="title">Neurocomputing</h1>
  <p class="subtitle">Object detection</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Julien Vitay 
</div>
        <p class="quarto-title-affiliation">
            Professur für Künstliche Intelligenz - Fakultät für Informatik
          </p>
    </div>
</div>

</section>
<section id="object-detection" class="title-slide slide level1 center">
<h1>1 - Object detection</h1>

</section>

<section id="object-recognition-vs.-object-detection" class="title-slide slide level1 center">
<h1>Object recognition vs.&nbsp;object detection</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dnn_classification_vs_detection.png"></p>
<figcaption>Source: <a href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4" class="uri">https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4</a></figcaption>
</figure>
</div>
</section>

<section id="object-detection-with-heatmaps" class="title-slide slide level1 center">
<h1>Object detection with heatmaps</h1>
<ul>
<li><p>A naive and very expensive method is to use a trained CNN as a high-level filter.</p></li>
<li><p>The CNN is trained on small images and convolved on bigger images.</p></li>
<li><p>The output is a heatmap of the probability that a particular object is present.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/objectdetection.png" style="width:70.0%"></p>
<figcaption>Source: <a href="https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4" class="uri">https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4</a></figcaption>
</figure>
</div>
</section>

<section id="pascal-visual-object-classes-challenge" class="title-slide slide level1 center">
<h1>PASCAL Visual Object Classes Challenge</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/pascal-cow.jpg"></p>
<figcaption>Source: <a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2008/" class="uri">http://host.robots.ox.ac.uk/pascal/VOC/voc2008/</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>The main dataset for object detection is the <strong>PASCAL</strong> Visual Object Classes Challenge:</p>
<ul>
<li><p>20 classes</p></li>
<li><p>~10K images</p></li>
<li><p>~25K annotated objects</p></li>
</ul></li>
<li><p>It is both a:</p>
<ul>
<li><p><strong>Classification</strong> problem, as one has to recognize an object.</p></li>
<li><p><strong>Regression</strong> problem, as one has to predict the coordinates <span class="math inline">(x, y, w, h)</span> of the bounding box.</p></li>
</ul></li>
</ul>
</div></div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/localization.png"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e" class="uri">https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e</a></figcaption>
</figure>
</div>
</section>

<section id="ms-coco-dataset-common-objects-in-context" class="title-slide slide level1 center">
<h1>MS COCO dataset (Common Objects in COntext)</h1>
<div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/mscoco.png" style="width:80.0%"></p>
<figcaption>Source: <a href="http://cocodataset.org" class="uri">http://cocodataset.org</a></figcaption>
</figure>
</div>
</div>
<ul>
<li><p>330K images, 80 labels.</p></li>
<li><p>Also contains data for semantic segmentation, caption generation, etc.</p></li>
</ul>
</section>

<section id="r-cnn-regions-with-cnn-features" class="title-slide slide level1 center">
<h1>R-CNN : Regions with CNN features</h1>
<div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/rcnn.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></p>
</figure>
</div>
</div>
<ol type="1">
<li><p>Bottom-up region proposals (selective search) by searching bounding boxes based on pixel info.</p></li>
<li><p>Feature extraction using a pre-trained CNN (AlexNet).</p></li>
<li><p>Classification using a SVM (object or not; if yes, which one?)</p></li>
<li><p>If an object is found, linear regression on the region proposal to generate tighter bounding box coordinates.</p></li>
</ol>
<div class="footer">
<p>Girshick, Donahue, Darrell and Malik (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. CVPR.</p>
</div>
<div class="callout callout-note no-icon callout-style-simple">
<div class="callout-body">
<div class="callout-content">
<p>Selective search: <a href="https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf" class="uri">https://ivi.fnwi.uva.nl/isis/publications/2013/UijlingsIJCV2013/UijlingsIJCV2013.pdf</a></p>
</div>
</div>
</div>
</section>

<section id="r-cnn-regions-with-cnn-features-1" class="title-slide slide level1 center">
<h1>R-CNN : Regions with CNN features</h1>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li>Each region proposal is processed by the CNN, followed by a SVM and a bounding box regressor.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/rcnn-detail.png"></p>
<figcaption>Source: <a href="https://courses.cs.washington.edu/courses/cse590v/14au/cse590v_wk1_rcnn.pdf" class="uri">https://courses.cs.washington.edu/courses/cse590v/14au/cse590v_wk1_rcnn.pdf</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:55%;">
<ul>
<li>The CNN is pre-trained on ImageNet and fine-tuned on Pascal VOC.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/rcnn-training.png"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e" class="uri">https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="fast-r-cnn" class="title-slide slide level1 center">
<h1>Fast R-CNN</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/fast-rcnn.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>The main drawback of R-CNN is that each of the 2000 region proposals have to go through the CNN: extremely slow.</p></li>
<li><p>The idea behind <strong>Fast R-CNN</strong> is to extract region proposals in higher feature maps and to use transfer learning.</p></li>
</ul>
</div></div>
<ul>
<li><p>The network first processes the whole image with several convolutional and max pooling layers to produce a feature map.</p></li>
<li><p>Each object proposal is projected to the feature map, where a region of interest (RoI) pooling layer extracts a fixed-length feature vector.</p></li>
<li><p>Each feature vector is fed into a sequence of FC layers that finally branch into two sibling output layers:</p>
<ul>
<li>a softmax probability estimate over the K classes plus a catch-all “background” class.</li>
<li>a regression layer that outputs four real-valued numbers for each class.</li>
</ul></li>
<li><p>The loss function to minimize is a composition of different losses and penalty terms:</p></li>
</ul>
<p><span class="math display">
    \mathcal{L}(\theta) = \lambda_1 \, \mathcal{L}_\text{classification}(\theta) + \lambda_2 \, \mathcal{L}_\text{regression}(\theta) + \lambda_3 \, \mathcal{L}_\text{regularization}(\theta)
</span></p>
<div class="footer">
<p>Girschick (2015). Fast R-CNN. arxiv:1504.08083</p>
</div>
</section>

<section id="faster-r-cnn" class="title-slide slide level1 center">
<h1>Faster R-CNN</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/faster-rcnn.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Both R-CNN and Fast R-CNN use selective search to find out the region proposals: slow and time-consuming.</p></li>
<li><p>Faster R-CNN introduces an object detection algorithm that lets the network learn the region proposals.</p></li>
<li><p>The image is passed through a pretrained CNN to obtain a convolutional feature map.</p></li>
<li><p>A separate network is used to predict the region proposals.</p></li>
<li><p>The predicted region proposals are then reshaped using a RoI pooling layer which is then used to classify the object and predict the bounding box.</p></li>
</ul>
</div></div>
<div class="footer">
<p>Ren et al.&nbsp;(2015). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. arXiv:1506.01497</p>
</div>
</section>

<section id="yolo" class="title-slide slide level1 center">
<h1>2 - YOLO</h1>

</section>

<section id="yolo-you-only-look-once" class="title-slide slide level1 center">
<h1>YOLO (You Only Look Once)</h1>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/yolo.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:35%;">
<ul>
<li><p>(Fast(er)) R-CNN perform classification for each region proposal sequentially: slow.</p></li>
<li><p>YOLO applies a single neural network to the full image to predict all possible boxes and the corresponding classes.</p></li>
<li><p>YOLO divides the image into a SxS grid of cells.</p></li>
</ul>
</div></div>
<ul>
<li><p>Each grid cell predicts a single object, with the corresponding <span class="math inline">C</span> <strong>class probabilities</strong> (softmax).</p></li>
<li><p>It also predicts the coordinates of <span class="math inline">B</span> possible <strong>bounding boxes</strong> (x, y, w, h) as well as a box <strong>confidence score</strong>.</p></li>
<li><p>The SxSxB predicted boxes are then pooled together to form the final prediction.</p></li>
</ul>
<div class="footer">
<p>Redmon et al.&nbsp;(2015). You Only Look Once: Unified, Real-Time Object Detection. arxiv:1506.02640</p>
</div>
</section>

<section id="yolo-you-only-look-once-1" class="title-slide slide level1 center">
<h1>YOLO (You Only Look Once)</h1>
<ul>
<li>The yellow box predicts the presence of a <strong>person</strong> (the class) as well as a candidate <strong>bounding box</strong> (it may be bigger than the grid cell itself).</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/yolo1.jpeg" style="width:80.0%"></p>
<figcaption>Source: <a href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" class="uri">https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088</a></figcaption>
</figure>
</div>
</section>

<section id="yolo-you-only-look-once-2" class="title-slide slide level1 center">
<h1>YOLO (You Only Look Once)</h1>
<ul>
<li>We will suppose here that each grid cell proposes 2 bounding boxes.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/yolo2.jpeg" style="width:50.0%"></p>
<figcaption>Source: <a href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" class="uri">https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088</a></figcaption>
</figure>
</div>
</section>

<section id="yolo-you-only-look-once-3" class="title-slide slide level1 center">
<h1>YOLO (You Only Look Once)</h1>
<ul>
<li><p>Each grid cell predicts a probability for each of the 20 classes, and two bounding boxes (4 coordinates and a confidence score per bounding box).</p></li>
<li><p>This makes C + B * 5 = 30 values to predict for each cell.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/yolo3.jpeg" style="width:70.0%"></p>
<figcaption>Source: <a href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" class="uri">https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088</a></figcaption>
</figure>
</div>
</section>

<section id="yolo-cnn-architecture" class="title-slide slide level1 center">
<h1>YOLO : CNN architecture</h1>
<ul>
<li><p>YOLO uses a CNN with 24 convolutional layers and 4 max-pooling layers to obtain a 7x7 grid.</p></li>
<li><p>The last convolution layer outputs a tensor with shape (7, 7, 1024). The tensor is then flattened and passed through 2 fully connected layers.</p></li>
<li><p>The output is a tensor of shape (7, 7, 30), i.e.&nbsp;7x7 grid cells, 20 classes and 2 boundary box predictions per cell.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/yolo-cnn.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></p>
</figure>
</div>
<div class="footer">
<p>Redmon et al.&nbsp;(2015). You Only Look Once: Unified, Real-Time Object Detection. arxiv:1506.02640</p>
</div>
</section>

<section id="yolo-confidence-score" class="title-slide slide level1 center">
<h1>YOLO : confidence score</h1>
<ul>
<li><p>The 7x7 grid cells predict 2 bounding boxes each: maximum of 98 bounding boxes on the whole image.</p></li>
<li><p>Only the bounding boxes with the <strong>highest class confidence score</strong> are kept.</p></li>
</ul>
<p><span class="math display">
    \text{class confidence score = box confidence score * class probability}
</span></p>
<ul>
<li>In practice, the class confidence score should be above 0.25 to be retained.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/yolo4.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></p>
</figure>
</div>
<div class="footer">
<p>Redmon et al.&nbsp;(2015). You Only Look Once: Unified, Real-Time Object Detection. arxiv:1506.02640</p>
</div>
</section>

<section id="yolo-intersection-over-union-iou" class="title-slide slide level1 center">
<h1>YOLO : Intersection over Union (IoU)</h1>
<ul>
<li><p>To ensure specialization, only one bounding box per grid cell should be responsible for detecting an object.</p></li>
<li><p>During learning, we select the bounding box with the biggest overlap with the object.</p></li>
<li><p>This can be measured by the <strong>Intersection over the Union</strong> (IoU).</p></li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/iou1.jpg"></p>
<figcaption>Source: <a href="https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/" class="uri">https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/iou2.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>

<section id="yolo-loss-functions" class="title-slide slide level1 center">
<h1>YOLO : loss functions</h1>
<ul>
<li><p>The output of the network is a 7x7x30 tensor, representing for each cell:</p>
<ul>
<li><p>the probability that an object of a given class is present.</p></li>
<li><p>the position of two bounding boxes.</p></li>
<li><p>the confidence that the proposed bounding boxes correspond to a real object (the IoU).</p></li>
</ul></li>
<li><p>We are going to combine three different loss functions:</p></li>
</ul>
<ol type="1">
<li><p>The <strong>categorization loss</strong>: each cell should predict the correct class.</p></li>
<li><p>The <strong>localization loss</strong>: error between the predicted boundary box and the ground truth for each object.</p></li>
<li><p>The <strong>confidence loss</strong>: do the predicted bounding boxes correspond to real objects?</p></li>
</ol>
<div class="footer">
<p>Redmon et al.&nbsp;(2015). You Only Look Once: Unified, Real-Time Object Detection. arxiv:1506.02640</p>
</div>
</section>

<section id="yolo-classification-loss" class="title-slide slide level1 center">
<h1>YOLO : classification loss</h1>
<ul>
<li><p>The classification loss is the <strong>mse</strong> between:</p>
<ul>
<li><p><span class="math inline">\hat{p}_i(c)</span>: the one-hot encoded class <span class="math inline">c</span> of the object present under each cell <span class="math inline">i</span>, and</p></li>
<li><p><span class="math inline">p_i(c)</span>: the predicted class probabilities of cell <span class="math inline">i</span>.</p></li>
</ul></li>
</ul>
<p><span class="math display">
    \mathcal{L}_\text{classification}(\theta) =  \sum_{i=0}^{S^2} \mathbb{1}_i^\text{obj} \sum_{c \in \text{classes}} (p_i(c) - \hat{p}_i(c))^2
</span></p>
<p>where <span class="math inline">\mathbb{1}_i^\text{obj}</span> is 1 when there actually is an object behind the cell <span class="math inline">i</span>, 0 otherwise (background).</p>
<ul>
<li><p>They could also have used the cross-entropy loss, but the output layer is not a softmax layer.</p></li>
<li><p>Using mse is also more compatible with the other losses.</p></li>
</ul>
<div class="footer">
<p>Redmon et al.&nbsp;(2015). You Only Look Once: Unified, Real-Time Object Detection. arxiv:1506.02640</p>
</div>
</section>

<section id="yolo-localization-loss" class="title-slide slide level1 center">
<h1>YOLO : localization loss</h1>
<ul>
<li><p>For all bounding boxes matching a real object, we want to minimize the <strong>mse</strong> between:</p>
<ul>
<li><p><span class="math inline">(\hat{x}_i, \hat{y}_i, \hat{w}_i, \hat{h}_i)</span>: the coordinates of the ground truth bounding box, and</p></li>
<li><p><span class="math inline">(x_i, y_i, w_i, h_i)</span>: the coordinates of the predicted bounding box.</p></li>
</ul></li>
</ul>
<p><span class="math display">
    \mathcal{L}_\text{localization}(\theta) = \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{obj} [ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2]  + \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{obj} [ (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2]
</span></p>
<p>where <span class="math inline">\mathbb{1}_{ij}^\text{obj}</span> is 1 when the bounding box <span class="math inline">j</span> of cell <span class="math inline">i</span> “matches” with an object (IoU).</p>
<ul>
<li><p>The root square of the width and height of the bounding boxes is used.</p></li>
<li><p>This allows to penalize more the errors on small boxes than on big boxes.</p></li>
</ul>
<div class="footer">
<p>Redmon et al.&nbsp;(2015). You Only Look Once: Unified, Real-Time Object Detection. arxiv:1506.02640</p>
</div>
</section>

<section id="yolo-confidence-loss" class="title-slide slide level1 center">
<h1>YOLO : confidence loss</h1>
<ul>
<li><p>Finally, we need to learn the confidence score of each bounding box, by minimizing the <strong>mse</strong> between:</p>
<ul>
<li><p><span class="math inline">C_i</span>: the predicted confidence score of cell <span class="math inline">i</span>, and</p></li>
<li><p><span class="math inline">\hat{C}_i</span>: the IoU between the ground truth bounding box and the predicted one.</p></li>
</ul></li>
</ul>
<p><span class="math display">
    \mathcal{L}_\text{confidence}(\theta) = \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{obj} (C_{ij} - \hat{C}_{ij})^2  + \lambda^\text{noobj} \, \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{noobj} (C_{ij} - \hat{C}_{ij})^2
</span></p>
<ul>
<li><p>Two cases are considered:</p>
<ol type="1">
<li><p>There was a real object at that location (<span class="math inline">\mathbb{1}_{ij}^\text{obj} = 1</span>): the confidences should be updated fully.</p></li>
<li><p>There was no real object (<span class="math inline">\mathbb{1}_{ij}^\text{noobj} = 1</span>): the confidences should only be moderately updated (<span class="math inline">\lambda^\text{noobj} = 0.5</span>)</p></li>
</ol></li>
<li><p>This is to deal with <strong>class imbalance</strong>: there are much more cells on the background than on real objects.</p></li>
</ul>
<div class="footer">
<p>Redmon et al.&nbsp;(2015). You Only Look Once: Unified, Real-Time Object Detection. arxiv:1506.02640</p>
</div>
</section>

<section id="yolo-loss-function" class="title-slide slide level1 center">
<h1>YOLO : loss function</h1>
<ul>
<li>Put together, the loss function to minimize is:</li>
</ul>
<p><span class="math display">
\begin{align}
    \mathcal{L}(\theta) &amp; = \mathcal{L}_\text{classification}(\theta) + \lambda_\text{coord} \, \mathcal{L}_\text{localization}(\theta) + \mathcal{L}_\text{confidence}(\theta) \\
              &amp; = \sum_{i=0}^{S^2} \mathbb{1}_i^\text{obj} \sum_{c \in \text{classes}} (p_i(c) - \hat{p}_i(c))^2 \\
              &amp; + \lambda_\text{coord} \, \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{obj} [ (x_i - \hat{x}_i)^2 + (y_i - \hat{y}_i)^2] \\
              &amp; + \lambda_\text{coord} \, \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{obj} [ (\sqrt{w_i} - \sqrt{\hat{w}_i})^2 + (\sqrt{h_i} - \sqrt{\hat{h}_i})^2] \\
              &amp; + \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{obj} (C_{ij} - \hat{C}_{ij})^2  \\
              &amp; + \lambda^\text{noobj} \, \sum_{i=0}^{S^2} \sum_{j=0}^{B} \mathbb{1}_{ij}^\text{noobj} (C_{ij} - \hat{C}_{ij})^2 \\
\end{align}
</span></p>
<div class="footer">
<p>Redmon et al.&nbsp;(2015). You Only Look Once: Unified, Real-Time Object Detection. arxiv:1506.02640</p>
</div>
</section>

<section>
<section id="yolo-training-on-pascal-voc" class="title-slide slide level1 center">
<h1>YOLO : Training on PASCAL VOC</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/yolo-result.png" class="quarto-figure quarto-figure-center" style="width:70.0%"></p>
</figure>
</div>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/yolo-result2.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>YOLO was trained on PASCAL VOC (natural images) but generalizes well to other datasets (paintings…).</p></li>
<li><p>Runs real-time (60 fps) on a NVIDIA Titan X.</p></li>
<li><p>Faster and more accurate versions of YOLO have been developed: YOLO9000, YOLOv3, YOLOv4, YOLOv5…</p></li>
</ul>
</div></div>
</section>
<section class="slide level2">

<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/MPU2HistivI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<div class="footer">
<p><a href="https://pjreddie.com/darknet/yolo/" class="uri">https://pjreddie.com/darknet/yolo/</a></p>
</div>
</section></section>
<section id="other-object-detectors" class="title-slide slide level1 center">
<h1>3 - Other object detectors</h1>

</section>

<section id="ssd-single-shot-detector" class="title-slide slide level1 center">
<h1>SSD: Single-Shot Detector</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/ssd.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>The idea of SSD is similar to YOLO, but:</p>
<ul>
<li>faster</li>
<li>more accurate</li>
<li>not limited to 98 objects per scene</li>
<li>multi-scale</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Contrary to YOLO, all convolutional layers are used to predict a bounding box, not just the final tensor.</p>
<ul>
<li>Skip connections.</li>
</ul></li>
<li><p>This allows to detect boxes at multiple scales (pyramid).</p></li>
</ul>
</div></div>
<div class="footer">
<p>Liu et al.&nbsp;(2016) SSD: Single Shot MultiBox Detector. arXiv:1512.02325</p>
</div>
</section>

<section id="r-cnns-on-rgb-d-images" class="title-slide slide level1 center">
<h1>R-CNNs on RGB-D images</h1>
<ul>
<li><p>It is also possible to use <strong>depth</strong> information (e.g.&nbsp;from a Kinect) as an additional channel of the R-CNN.</p></li>
<li><p>The depth information provides more information on the structure of the object, allowing to disambiguate certain situations (segmentation).</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/rcnn-rgbd.png" class="quarto-figure quarto-figure-center" style="width:100.0%"></p>
</figure>
</div>
<div class="footer">
<p>Gupta et al.&nbsp;(2014). Learning Rich Features from RGB-D Images for Object Detection and Segmentation, ECCV 2014.</p>
</div>
</section>

<section id="voxelnet" class="title-slide slide level1 center">
<h1>VoxelNet</h1>
<ul>
<li>Lidar point clouds can also be used for detecting objects, for example <strong>VoxelNet</strong> trained on the KITTI dataset.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/voxelnet.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
<div class="footer">
<p>Zhou Y, Tuzel O. (2017). VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection. arXiv:171106396</p>
</div>
</section>

<section id="voxelnet-1" class="title-slide slide level1 center">
<h1>VoxelNet</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/voxelnet-result.png" style="width:100.0%"></p>
<figcaption>Source: <a href="https://medium.com/@SmartLabAI/3d-object-detection-from-lidar-data-with-deep-learning-95f6d400399a" class="uri">https://medium.com/@SmartLabAI/3d-object-detection-from-lidar-data-with-deep-learning-95f6d400399a</a></figcaption>
</figure>
</div>
</section>

<section id="metrics" class="title-slide slide level1 center">
<h1>4 - Metrics</h1>

</section>

<section id="metrics-for-object-detection" class="title-slide slide level1 center">
<h1>Metrics for object detection</h1>
<ul>
<li><p>How do we measure the “accuracy” of an object detector? The output is both a classification and a regression.</p></li>
<li><p>Not only must the predicted class be correct, but the predicted bounding box must overlap with the ground truth, i.e.&nbsp;have an high IoU.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/mAP-basic.png"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/map-mean-average-precision-might-confuse-you-5956f1bfa9e2" class="uri">https://towardsdatascience.com/map-mean-average-precision-might-confuse-you-5956f1bfa9e2</a></figcaption>
</figure>
</div>
</section>

<section id="metrics-for-object-detection-1" class="title-slide slide level1 center">
<h1>Metrics for object detection</h1>
<ul>
<li><p>The accuracy of an object detector depends on a threshold for the IoU, for example 0.5.</p></li>
<li><p>A prediction is correct if the predicted class is correct <strong>and</strong> the IoU is above the threshold.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/mAP-basic2.webp"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/map-mean-average-precision-might-confuse-you-5956f1bfa9e2" class="uri">https://towardsdatascience.com/map-mean-average-precision-might-confuse-you-5956f1bfa9e2</a></figcaption>
</figure>
</div>
</section>

<section id="precision-and-recall" class="title-slide slide level1 center">
<h1>Precision and recall</h1>
<ul>
<li>For a given class (e.g.&nbsp;“human”), we can compute the binary <strong>precision</strong> and <strong>recall</strong> values:</li>
</ul>
<p><span class="math display">
    P = \frac{\text{TP}}{\text{TP} + \text{FP}} \;\;
    R = \frac{\text{TP}}{\text{TP} + \text{FN}}
</span></p>
<ul>
<li>P = when something is detected, is it correct? R = if something exists, is it detected?</li>
</ul>
<div class="columns">
<div class="column">
<ul>
<li>In the image on the right, we have one TP, one FN, zero FP and an undefined number of TN:</li>
</ul>
<p><span class="math display">
    P = \frac{\text{1}}{\text{1} + \text{0}} = 1\;\;
    R = \frac{\text{1}}{\text{1} + \text{1}} = 0.5
</span></p>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/mAP-basic3.webp"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/map-mean-average-precision-might-confuse-you-5956f1bfa9e2" class="uri">https://towardsdatascience.com/map-mean-average-precision-might-confuse-you-5956f1bfa9e2</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="map-mean-average-precision" class="title-slide slide level1 center">
<h1>mAP: mean average precision</h1>
<div class="columns">
<div class="column" style="width:70%;">
<ul>
<li>Let’s now compute the <strong>precision-recall curve</strong> over 7 images, with 15 ground truth boxes and 24 predictions.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/meanAP-1.png"></p>
<figcaption>Source: <a href="https://github.com/rafaelpadilla/Object-Detection-Metrics" class="uri">https://github.com/rafaelpadilla/Object-Detection-Metrics</a></figcaption>
</figure>
</div>
<ul>
<li>Each prediction has a confidence score for the classification, and is either a TP or FP (depending on the IoU threshold).</li>
</ul>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/meanAP-2.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>

<section id="map-mean-average-precision-1" class="title-slide slide level1 center">
<h1>mAP: mean average precision</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Let’s now <strong>sort</strong> the predictions with a decreasing confidence score and <strong>incrementally</strong> compute the prediction and recall:</li>
</ul>
<p><span class="math display">
    P = \frac{\text{TP}}{\text{TP} + \text{FP}}
</span> <span class="math display">
    R = \frac{\text{TP}}{\text{TP} + \text{FN}}
</span></p>
<ul>
<li><p>We just accumulate the number of TP and FP over the 24 predictions.</p></li>
<li><p>Note that <code>TP + FN</code> is the number of ground truths and is constant (15), so the recall will increase.</p></li>
<li><p>This equivalent to setting a high threshold for the confidence score and progressively decreasing it.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/meanAP-3.png"></p>
<figcaption>Source: <a href="https://github.com/rafaelpadilla/Object-Detection-Metrics" class="uri">https://github.com/rafaelpadilla/Object-Detection-Metrics</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="map-mean-average-precision-2" class="title-slide slide level1 center">
<h1>mAP: mean average precision</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>If we plot the <strong>precision x recall curve</strong> (PR curve) for the 24 predictions, we obtain:</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/meanAP-4.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
<ul>
<li>The precision globally decreases with the recall, as we use predictions with lower confidence scores, but there are some oscillations.</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/meanAP-3.png"></p>
<figcaption>Source: <a href="https://github.com/rafaelpadilla/Object-Detection-Metrics" class="uri">https://github.com/rafaelpadilla/Object-Detection-Metrics</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="map-mean-average-precision-3" class="title-slide slide level1 center">
<h1>mAP: mean average precision</h1>
<ul>
<li><p>To get rid of these oscillations, we <strong>interpolate</strong> the precision by taking maximal precision value for higher recall (left).</p></li>
<li><p>We can then easily integrate this curve by computing the <strong>area under the curve</strong> (AUC, right), what defines the <strong>average precision</strong> (AP).</p></li>
</ul>
<p><span class="math display">
    \text{AP} = \sum_n (R_{n} - R_{n-1}) \, P_n
</span></p>
<div class="columns">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/meanAP-5.png"></p>
<figcaption>Source: <a href="https://github.com/rafaelpadilla/Object-Detection-Metrics" class="uri">https://github.com/rafaelpadilla/Object-Detection-Metrics</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/meanAP-6.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>

<section id="map-mean-average-precision-4" class="title-slide slide level1 center">
<h1>mAP: mean average precision</h1>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p>A good detector sees its precision decreases not that much when the recall increases, i.e.&nbsp;when it is still correct when it increasingly detects objects.</p></li>
<li><p>The ideal detector has an AP of 1.</p></li>
<li><p>When averaging the AP over the classes, one obtains the <strong>mean average precision (mAP)</strong>:</p></li>
</ul>
<p><span class="math display">
    \text{mAP} = \dfrac{1}{N_\text{classes}} \, \sum_{i=1}^{N_\text{classes}} \, AP_i
</span></p>
<ul>
<li><p>One usually reports the mAP value with the IoU threshold, e.g.&nbsp;<code>mAP@0.5</code>.</p></li>
<li><p>mAP is a better trade-off between precision and recall than the F1 score.</p></li>
<li><p><code>scikit-learn</code> is your friend:</p></li>
</ul>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/mAP-ssd.webp"></p>
<figcaption>Source: Van Etten, A. (2019). Satellite Imagery Multiscale Rapid Detection with Windowed Networks. 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), 735–743. doi:10.1109/WACV.2019.00083</figcaption>
</figure>
</div>
</div></div>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a aria-hidden="true" tabindex="-1"></a>mAP <span class="op">=</span> sklearn.metrics.average_precision_score(t, y, average<span class="op">=</span><span class="st">"micro"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>

<section id="additional-resources-on-object-detection" class="title-slide slide level1 center">
<h1>Additional resources on object detection</h1>
<ul>
<li><p><a href="https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852" class="uri">https://medium.com/comet-app/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852</a></p></li>
<li><p><a href="https://medium.com/@smallfishbigsea/faster-r-cnn-explained-864d4fb7e3f8" class="uri">https://medium.com/@smallfishbigsea/faster-r-cnn-explained-864d4fb7e3f8</a></p></li>
<li><p><a href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e" class="uri">https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e</a></p></li>
<li><p><a href="https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088" class="uri">https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088</a></p></li>
<li><p><a href="https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06" class="uri">https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06</a></p></li>
<li><p><a href="https://towardsdatascience.com/lidar-3d-object-detection-methods-f34cf3227aea" class="uri">https://towardsdatascience.com/lidar-3d-object-detection-methods-f34cf3227aea</a></p></li>
</ul>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false,"theme":"whiteboard"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/vitay\.github\.io\/course-neurocomputing\/");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>