<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-0626ff4d7a71b55c8707dcae1d04a9b6.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.39">

  <meta name="author" content="Julien Vitay">
  <title>Neurocomputing</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-31e5e70a95169d1719a5a4fd7b5514a9.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="img/tuc-new-large.png" data-background-opacity="1" data-background-position="top" data-background-size="30%" class="quarto-title-block center">
  <h1 class="title">Neurocomputing</h1>
  <p class="subtitle">Hopfield networks</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Julien Vitay 
</div>
        <p class="quarto-title-affiliation">
            Professur für Künstliche Intelligenz - Fakultät für Informatik
          </p>
    </div>
</div>

</section>
<section id="associative-memory" class="title-slide slide level1 center">
<h1>1 - Associative memory</h1>

</section>

<section id="procedural-vs.-episodic-memory" class="title-slide slide level1 center">
<h1>Procedural vs.&nbsp;episodic memory</h1>
<ul>
<li><p>In deep learning, our biggest enemy was <strong>overfitting</strong>, i.e.&nbsp;learning by heart the training examples.</p></li>
<li><p>But what if it was actually useful in cognitive tasks?</p></li>
<li><p>Deep networks implement a <strong>procedural memory</strong>: they know <strong>how</strong> to do things.</p></li>
<li><p>A fundamental aspect of cognition is <strong>episodic memory</strong>: remembering <strong>when</strong> specific events happened.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/episodicmemory.gif"></p>
<figcaption>Source: <a href="https://brain-basedlearning.weebly.com/memory.html" class="uri">https://brain-basedlearning.weebly.com/memory.html</a></figcaption>
</figure>
</div>
</section>

<section id="when-can-episodic-memory-be-useful" class="title-slide slide level1 center">
<h1>When can episodic memory be useful?</h1>
<ul>
<li><p>Episodic memory is particularly useful when retrieving memories from <strong>degraded</strong> or <strong>partial</strong> inputs.</p></li>
<li><p>When the reconstruction is similar to the remembered input, we talk about <strong>auto-associative memory</strong>.</p></li>
<li><p>An item can be retrieved by just knowing part of its content: <strong>content-adressable memory</strong>.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/AM-reconstruction.png"></p>
<figcaption>Source: <a href="https://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2015/slides/lec14.hopfield.pdf" class="uri">https://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2015/slides/lec14.hopfield.pdf</a></figcaption>
</figure>
</div>
</section>

<section id="auto-associative-memory" class="title-slide slide level1 center">
<h1>Auto-associative memory</h1>
<p>Hmunas rmebmeer iamprtnot envtes in tiher leivs. You mihgt be albe to rlecal eervy deiatl of yuor frist eaxm at cllgeoe; or of yuor fsirt pbuilc sepceh; or of yuor frsit day in katigrneedrn; or the fisrt tmie you wnet to a new scohol atefr yuor fimlay mveod to a new ctiy. Hmaun moemry wkors wtih asncisoatois. If you haer the vicoe of an old fernid on the pnohe, you may slntesnoauopy rlaecl seortis taht you had not tghuoht of for yares. If you are hrgnuy and see a pcturie of a bnaana, you mihgt vdivliy rclael the ttsae and semll of a bnanaa and teerbhy rieazle taht you are ideend hngury. In tihs lcterue, we peesrnt modles of nrueal ntkweros taht dbriecse the rcaell of puielovsry seortd imtes form mmorey.</p>
<div class="footer">
<p>Text scrambler by <a href="http://www.stevesachs.com/jumbler.cgi" class="uri">http://www.stevesachs.com/jumbler.cgi</a></p>
</div>
</section>

<section id="auto-associative-memory-1" class="title-slide slide level1 center">
<h1>Auto-associative memory</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>The classical approach is the <strong>nearest neighbour</strong> algorithm.</p></li>
<li><p>One compares a new input to each of the training examples using a given metric (distance) and assigns the input to the closest example.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/nearestneighbour.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>Another approach is to have a recurrent neural network <strong>memorize</strong> the training examples and retrieve them given the input.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/associativememory.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<div class="footer">
<p>Source: <a href="http://didawiki.di.unipi.it/lib/exe/fetch.php/bionics-engineering/computational-neuroscience/2-hopfield-hand.pdf" class="uri">http://didawiki.di.unipi.it/lib/exe/fetch.php/bionics-engineering/computational-neuroscience/2-hopfield-hand.pdf</a></p>
</div>
</section>

<section id="hetero-associative-memory" class="title-slide slide level1 center">
<h1>Hetero-associative memory</h1>
<ul>
<li><p>When the reconstruction is different from the input, it is an <strong>hetero-associative memory</strong>.</p></li>
<li><p>Hetero-associative memories often work in both directions (bidirectional associative memory):</p>
<ul>
<li>name <span class="math inline">\leftrightarrow</span> face.</li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/heteroassociative.jpg" class="quarto-figure quarto-figure-center" style="width:70.0%"></p>
</figure>
</div>
<div class="footer">
<p>Source: <a href="https://slideplayer.com/slide/7303074/" class="uri">https://slideplayer.com/slide/7303074/</a></p>
</div>
<!-- # Proust's madeleine

::: {.columns}
::: {.column width=50%}


“No sooner had the warm liquid mixed with the crumbs touched my palate than a shudder ran through me and I stopped, intent upon the extraordinary thing that was happening to me. An exquisite pleasure had invaded my senses, something isolated, detached, with no suggestion of its origin. And at once the vicissitudes of life had become indifferent to me, its disasters innocuous, its brevity illusory – this new sensation having had on me the effect which love has of filling me with a precious essence; or rather this essence was not in me it was me. ... Whence did it come? What did it mean? How could I seize and apprehend it? ... And suddenly the memory revealed itself. The taste was that of the little piece of madeleine which on Sunday mornings at Combray (because on those mornings I did not go out before mass), when I went to say good morning to her in her bedroom, my aunt Léonie used to give me, dipping it first in her own cup of tea or tisane. The sight of the little madeleine had recalled nothing to my mind before I tasted it. And all from my cup of tea.”


:::
::: {.column width=50%}


![](img/madeleine-de-proust.jpg)


> Marcel Proust, In Search of Lost Time

:::
:::

 -->
</section>

<section id="hopfield-networks" class="title-slide slide level1 center">
<h1>2 - Hopfield networks</h1>

</section>

<section id="feedforward-and-recurrent-neural-networks" class="title-slide slide level1 center">
<h1>Feedforward and recurrent neural networks</h1>
<ul>
<li>Feedforward networks only depend on the current input:</li>
</ul>
<p><span class="math display">\mathbf{y}_t = f(W \times \mathbf{x}_t + \mathbf{b})</span></p>
<ul>
<li>Recurrent networks also depend on their previous output:</li>
</ul>
<p><span class="math display">\mathbf{y}_t = f(W \times [\mathbf{x}_t \, ; \, \mathbf{y}_{t-1}] + \mathbf{b})</span></p>
<ul>
<li>Both are strongly dependent on their inputs and do not have their own dynamics.</li>
</ul>
</section>

<section id="hopfield-networks-1" class="title-slide slide level1 center">
<h1>Hopfield networks</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><strong>Hopfield networks</strong> <span class="citation" data-cites="Hopfield1982 Hopfield1983">(<a href="#/references" role="doc-biblioref" onclick="">Hopfield, 1982</a>; <a href="#/references" role="doc-biblioref" onclick="">Hopfield et al., 1983</a>)</span> only depend on a single input (one constant value per neuron) and their previous output using <strong>recurrent weights</strong>:</li>
</ul>
<p><span class="math display">\mathbf{y}_t = f(\mathbf{x} + W \times \mathbf{y}_{t-1} + \mathbf{b})</span></p>
<ul>
<li><p>For a single constant input <span class="math inline">\mathbf{x}</span>, one lets the network <strong>converge</strong> for enough time steps <span class="math inline">T</span> and observe what the final output <span class="math inline">\mathbf{y}_T</span> is.</p></li>
<li><p>Hopfield network have their own <strong>dynamics</strong>: the output evolves over time, but the input is constant.</p></li>
<li><p>One can even omit the input <span class="math inline">\mathbf{x}</span> and merge it with the bias <span class="math inline">\mathbf{b}</span>: the dynamics will only depend on the <strong>initial state</strong> <span class="math inline">\mathbf{y}_0</span>.</p></li>
</ul>
<p><span class="math display">\mathbf{y}_t = f(W \times \mathbf{y}_{t-1} + \mathbf{b})</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-net.png"></p>
<figcaption>Source: <a href="https://en.wikipedia.org/wiki/Hopfield_network">Wikipedia CC BY-SA 3.0</a>, Author: Zawersh</figcaption>
</figure>
</div>
</div></div>
</section>

<section id="hopfield-networks-2" class="title-slide slide level1 center">
<h1>Hopfield networks</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>Binary Hopfield networks use <strong>binary units</strong>.</p></li>
<li><p>The neuron <span class="math inline">i</span> has a net activation <span class="math inline">x_i</span> (<strong>potential</strong>) depending on the other neurons <span class="math inline">j</span> through weights <span class="math inline">w_{ji}</span>:</p></li>
</ul>
<p><span class="math display">x_i = \sum_{j \neq i} w_{ji} \, y_j + b</span></p>
<ul>
<li>The output <span class="math inline">y_i</span> is the sign of the potential:</li>
</ul>
<p><span class="math display">y_i = \text{sign}(x_i)</span></p>
<p><span class="math display">\text{sign}(x) = \begin{cases} +1 \; \text{if} \, x&gt;0 \\ -1 \; \text{otherwise.}\end{cases}</span></p>
<ul>
<li><p>There are <strong>no self-connections</strong>: <span class="math inline">w_{ii} = 0</span>.</p></li>
<li><p>The weights are <strong>symmetrical</strong>: <span class="math inline">w_{ij} = w_{ji}</span>.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-simple.png" style="width:70.0%"></p>
<figcaption>Source: <a href="http://didawiki.di.unipi.it/lib/exe/fetch.php/bionics-engineering/computational-neuroscience/2-hopfield-hand.pdf" class="uri">http://didawiki.di.unipi.it/lib/exe/fetch.php/bionics-engineering/computational-neuroscience/2-hopfield-hand.pdf</a></figcaption>
</figure>
</div>
<ul>
<li>In matrix-vector form:</li>
</ul>
<p><span class="math display">\mathbf{y} = \text{sign}(W \times \mathbf{y} + \mathbf{b})</span></p>
</div></div>
</section>

<section id="hopfield-networks-3" class="title-slide slide level1 center">
<h1>Hopfield networks</h1>
<ul>
<li><p>At each time step, a neuron will <strong>flip</strong> its state if the sign of the potential <span class="math inline">x_i = \sum_{j \neq i} w_{ji} \, y_j + b</span> does not match its current output <span class="math inline">y_i</span>.</p></li>
<li><p>This will in turn modify the potential of all other neurons, who may also flip. The potential of that neuron may change its sign, so the neuron will flip again.</p></li>
<li><p>After a finite number of iterations, the network reaches a <strong>stable state</strong> (proof later).</p></li>
<li><p>Neurons are evaluated one after the other: <strong>asynchronous evaluation</strong>.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-flip.png"></p>
<figcaption>Source: <a href="https://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2015/slides/lec14.hopfield.pdf" class="uri">https://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2015/slides/lec14.hopfield.pdf</a></figcaption>
</figure>
</div>
</section>

<section id="hopfield-networks-4" class="title-slide slide level1 center">
<h1>Hopfield networks</h1>
<ul>
<li><p>Let’s consider a Hopfield network with 5 neurons, <strong>sparse connectivity</strong> and no bias.</p></li>
<li><p>In the initial state, 2 neurons are on (+1), 3 are off (-1).</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-demo1.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></p>
</figure>
</div>
</section>

<section id="hopfield-networks-5" class="title-slide slide level1 center">
<h1>Hopfield networks</h1>
<ul>
<li><p>Let’s evaluate the top-right neuron.</p></li>
<li><p>Its potential is -4 * 1 + 3 * (-1) + 3 * (-1) = -10 <span class="math inline">&lt;</span> 0. Its output stays at -1.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-demo2.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></p>
</figure>
</div>
</section>

<section id="hopfield-networks-6" class="title-slide slide level1 center">
<h1>Hopfield networks</h1>
<ul>
<li><p>Now the bottom-left neuron.</p></li>
<li><p>The same: 3 * 1 + (-1) * (-1) = 4 <span class="math inline">&gt;</span> 0, the output stays at +1.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-demo3.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></p>
</figure>
</div>
</section>

<section id="hopfield-networks-7" class="title-slide slide level1 center">
<h1>Hopfield networks</h1>
<ul>
<li><p>But the bottom-middle neuron has to flip its sign: -1 * 1 + 4 * 1 + 3 * (-1) - 1 * (-1) = 1 <span class="math inline">&gt;</span> 0.</p></li>
<li><p>Its new output is +1.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-demo4.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></p>
</figure>
</div>
</section>

<section id="hopfield-networks-8" class="title-slide slide level1 center">
<h1>Hopfield networks</h1>
<ul>
<li><p>We can continue evaluating the neurons, but nobody will flip its sign.</p></li>
<li><p>This configuration is a <strong>stable pattern</strong> of the network.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-demo5.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></p>
</figure>
</div>
</section>

<section id="hopfield-networks-9" class="title-slide slide level1 center">
<h1>Hopfield networks</h1>
<ul>
<li><p>There is another stable pattern, where the two other neurons are active: <strong>symmetric</strong> or <strong>ghost</strong> pattern.</p></li>
<li><p>All other patterns are unstable and will eventually lead to one of the two <strong>stored patterns</strong>.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-demo6.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></p>
</figure>
</div>
</section>

<section id="hopfield-networks-10" class="title-slide slide level1 center">
<h1>Hopfield networks</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-demo5.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-demo6.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<ul>
<li><p>The weight matrix <span class="math inline">W</span> allows to encode a given number of stable patterns, which are <strong>fixed points</strong> of the network’s dynamics.</p></li>
<li><p>Any initial configuration will converge to one of the stable patterns.</p></li>
</ul>
</section>

<section id="hopfield-networks-11" class="title-slide slide level1 center">
<h1>Hopfield networks</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>Initialize a <strong>symmetrical weight matrix</strong> without self-connections.</p></li>
<li><p>Set an input to the network through the bias <span class="math inline">\mathbf{b}</span>.</p></li>
<li><p><strong>while</strong> not stable:</p>
<ul>
<li><p>Pick a neuron <span class="math inline">i</span> randomly.</p></li>
<li><p>Compute its potential:</p></li>
</ul>
<p><span class="math display">x_i = \sum_{j \neq i} w_{ji} \, y_j + b</span></p>
<ul>
<li>Flip its output if needed:</li>
</ul>
<p><span class="math display">y_i = \text{sign}(x_i)</span></p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-simple.png"></p>
<figcaption>Source: <a href="http://didawiki.di.unipi.it/lib/exe/fetch.php/bionics-engineering/computational-neuroscience/2-hopfield-hand.pdf" class="uri">http://didawiki.di.unipi.it/lib/exe/fetch.php/bionics-engineering/computational-neuroscience/2-hopfield-hand.pdf</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="asynchronous-evaluation" class="title-slide slide level1 center">
<h1>Asynchronous evaluation</h1>
<ul>
<li><p>Why do we need to update neurons one by one, instead of all together as in ANNs (vector-based)?</p></li>
<li><p>Consider the two neurons below:</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-asynchronous1.png" class="quarto-figure quarto-figure-center" style="width:30.0%"></p>
</figure>
</div>
<ul>
<li>If you update them at the same time, they will both flip:</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-asynchronous2.png" class="quarto-figure quarto-figure-center" style="width:30.0%"></p>
</figure>
</div>
<ul>
<li>But at the next update, they will both flip again: the network will oscillate for ever.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-asynchronous1.png" class="quarto-figure quarto-figure-center" style="width:30.0%"></p>
</figure>
</div>
</section>

<section id="asynchronous-evaluation-1" class="title-slide slide level1 center">
<h1>Asynchronous evaluation</h1>
<ul>
<li>By updating neurons one at a time (randomly), you make sure that the network converges to a stable pattern:</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-asynchronous1.png" class="quarto-figure quarto-figure-center" style="width:30.0%"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-asynchronous3.png" class="quarto-figure quarto-figure-center" style="width:30.0%"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-asynchronous3.png" class="quarto-figure quarto-figure-center" style="width:30.0%"></p>
</figure>
</div>
</section>

<section id="energy-of-the-hopfield-network" class="title-slide slide level1 center">
<h1>Energy of the Hopfield network</h1>
<ul>
<li><p>Let’s have a look at the quantity <span class="math inline">y_i \, (\sum_{j \neq i} w_{ji} \, y_j + b)</span> before and after an update:</p>
<ul>
<li><p>If the neuron does not flip, the quantity does not change.</p></li>
<li><p>If the neuron flips (<span class="math inline">y_i</span> goes from +1 to -1, or from -1 to +1), this means that:</p>
<ul>
<li><p><span class="math inline">y_i</span> and <span class="math inline">\sum_{j \neq i} w_{ji} \, y_j + b</span> had different signs before the update, so <span class="math inline">y_i \, (\sum_{j \neq i} w_{ji} \, y_j + b)</span> was negative.</p></li>
<li><p>After the flip, <span class="math inline">y_i</span> and <span class="math inline">\sum_{j \neq i} w_{ji} \, y_j + b</span> have the same sign, so <span class="math inline">y_i \, (\sum_{j \neq i} w_{ji} \, y_j + b)</span> becomes positive.</p></li>
</ul></li>
</ul></li>
<li><p>The <strong>change</strong> in the quantity <span class="math inline">y_i \, (\sum_{j \neq i} w_{ji} \, y_j + b)</span> is always positive or equal to zero:</p></li>
</ul>
<p><span class="math display"> \Delta [y_i \, (\sum_{j \neq i} w_{ji} \, y_j + b)] \geq 0</span></p>
<ul>
<li>No update can decrease this quantity.</li>
</ul>
</section>

<section id="energy-of-the-hopfield-network-1" class="title-slide slide level1 center">
<h1>Energy of the Hopfield network</h1>
<ul>
<li>Let’s now sum this quantity over the complete network and reverse its sign:</li>
</ul>
<p><span class="math display">E(\mathbf{y}) = - \sum_i y_i \, (\sum_{j &gt; i} w_{ji} \, y_j + b)</span></p>
<ul>
<li>We can expand it and simplify it knowing that <span class="math inline">w_{ii}=0</span> and <span class="math inline">w_{ij} = w_{ji}</span>:</li>
</ul>
<p><span class="math display">E(\mathbf{y}) = - \frac{1}{2} \, \sum_{i, j} w_{ij} \, y_i \, y_j - \sum_j y_j \, b_j</span></p>
<ul>
<li><p>The term <span class="math inline">\frac{1}{2}</span> comes from the fact that the weights are symmetric and count twice in the double sum.</p></li>
<li><p>In a matrix-vector form, it becomes:</p></li>
</ul>
<p><span class="math display">E(\mathbf{y}) = -\frac{1}{2} \, \mathbf{y}^T \times W \times \mathbf{y} - \mathbf{b}^T \times \mathbf{y}</span></p>
<ul>
<li><p><span class="math inline">E(\mathbf{y})</span> is called the <strong>energy</strong> of the network or its <strong>Lyapunov function</strong> for a pattern <span class="math inline">(\mathbf{y})</span>.</p></li>
<li><p>We know that updates can only <strong>decrease the energy</strong> of the network, it will never go up.</p></li>
<li><p>Moreover, the energy has a <strong>lower bound</strong>: it cannot get below a certain value as everything is finite.</p></li>
</ul>
</section>

<section id="energy-of-the-hopfield-network-2" class="title-slide slide level1 center">
<h1>Energy of the Hopfield network</h1>
<ul>
<li>The energy of the network can only decrease but has a lower bound.</li>
</ul>
<p><span class="math display">E(\mathbf{y}) = -\frac{1}{2} \, \mathbf{y}^T \times W \times \mathbf{y} - \mathbf{b}^T \times \mathbf{y}</span></p>
<ul>
<li>Stable patterns are <strong>local minima</strong> of the energy function: no update can increase the energy.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-energy.png"></p>
<figcaption>Source: <a href="http://didawiki.di.unipi.it/lib/exe/fetch.php/bionics-engineering/computational-neuroscience/2-hopfield-hand.pdf" class="uri">http://didawiki.di.unipi.it/lib/exe/fetch.php/bionics-engineering/computational-neuroscience/2-hopfield-hand.pdf</a></figcaption>
</figure>
</div>
</section>

<section id="energy-of-the-hopfield-network-3" class="title-slide slide level1 center">
<h1>Energy of the Hopfield network</h1>
<ul>
<li><p>Stable patterns are <strong>point attractors</strong>.</p></li>
<li><p>Other patterns have higher energies and are <strong>attracted</strong> by the closest stable pattern (attraction basin).</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield_energy_landscape.png"></p>
<figcaption>Source: <a href="https://en.wikipedia.org/wiki/Hopfield_network" class="uri">https://en.wikipedia.org/wiki/Hopfield_network</a></figcaption>
</figure>
</div>
</section>

<section id="capacity-of-a-hopfield-network" class="title-slide slide level1 center">
<h1>Capacity of a Hopfield network</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-simple2.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
<ul>
<li>It can be shown that for a network with <span class="math inline">N</span> units, one can store up to <span class="math inline">0.14 N</span> different patterns:</li>
</ul>
<p><span class="math display">C \approx 0.14 \, N</span></p>
<ul>
<li><p>If you have 1000 neurons, you can store 140 patterns.</p></li>
<li><p>As you need 1 million weights for it, it is not very efficient…</p></li>
</ul>
<div class="footer">
<p><span class="citation" data-cites="McEliece1987">McEliece et al. (<a href="#/references" role="doc-biblioref" onclick="">1987</a>)</span> The capacity of the Hopfield associative memory. IEEE Transactions on Information Theory 33:461–482. doi:10.1109/TIT.1987.1057328</p>
</div>
</section>

<section id="storing-patterns" class="title-slide slide level1 center">
<h1>Storing patterns</h1>
<ul>
<li>The weights define the stored patterns through their contribution to the energy:</li>
</ul>
<p><span class="math display">E = -\frac{1}{2} \, \mathbf{y}^T \times W \times \mathbf{y} - \mathbf{b}^T \times \mathbf{y}</span></p>
<ul>
<li><p>How do you choose the weights <span class="math inline">W</span> so that the desired patterns <span class="math inline">(\mathbf{y}^1, \mathbf{y}^2, \dots, \mathbf{y}^P)</span> are local minima of the energy function?</p></li>
<li><p>Let’s omit the bias for a while, as it does not depend on <span class="math inline">W</span>. One can replace the bias with a weight to a neuron whose activity is always +1.</p></li>
<li><p>The pattern <span class="math inline">\mathbf{y}^1 = [y^1_1, y^1_2, \ldots, y^1_N]^T</span> is stable if no neuron flips after the update:</p></li>
</ul>
<p><span class="math display">y^1_i = \text{sign}(\sum_{j\neq i} w_{ij} \, y^1_j) \; \; \forall i</span></p>
<ul>
<li>Which weights respect this stability constraint?</li>
</ul>
</section>

<section id="hebbs-rule-cells-that-fire-together-wire-together" class="title-slide slide level1 center">
<h1>Hebb’s rule: Cells that fire together wire together</h1>
<div class="columns">
<div class="column" style="width:60%;">
<blockquote>
<p>When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.</p>
<p><strong>Donald Hebb</strong>, 1949</p>
</blockquote>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hebb-principle.jpg"></p>
<figcaption>Source: <a href="https://thebrain.mcgill.ca/flash/i/i_07/i_07_cl/i_07_cl_tra/i_07_cl_tra.html" class="uri">https://thebrain.mcgill.ca/flash/i/i_07/i_07_cl/i_07_cl_tra/i_07_cl_tra.html</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="hebbian-learning" class="title-slide slide level1 center">
<h1>Hebbian learning</h1>
<ul>
<li><p><strong>Hebbian learning</strong> between two neurons states that the synaptic efficiency (weight) of their connection should be increased if the activity of the two neurons is correlated.</p></li>
<li><p>The correlation between the activities is simply the product:</p></li>
</ul>
<p><span class="math display">\Delta w_{i,j} = y_i \, y_j</span></p>
<ul>
<li><p>If both activities are high, the weight will increase.</p></li>
<li><p>If one of the activities is low, the weight won’t change.</p></li>
<li><p>It is a very rudimentary model of synaptic plasticity, but verified experimentally.</p></li>
</ul>
</section>

<section id="storing-patterns-1" class="title-slide slide level1 center">
<h1>Storing patterns</h1>
<ul>
<li>The fixed point respects:</li>
</ul>
<p><span class="math display">y^1_i = \text{sign}(\sum_{j\neq i} w_{ij} \, y^1_j) \; \; \forall i</span></p>
<ul>
<li>If we use <span class="math inline">w_{i,j} = y^1_i \, y^1_j</span> as the result of Hebbian learning (weights initialized at 0), we obtain</li>
</ul>
<p><span class="math display">y^1_i = \text{sign}(\sum_{j\neq i} y^1_i \, y^1_j \, y^1_j) = \text{sign}(\sum_{j\neq i} y^1_i) = \text{sign}((N-1) \, y^1_i) = \text{sign}(y^1_i) = y^1_i \; \; \forall i</span></p>
<p>as <span class="math inline">y^1_j \, y^1_j = 1</span> (binary units).</p>
<ul>
<li><p>This means that setting <span class="math inline">w_{i,j} = y^1_i \, y^1_j</span> makes <span class="math inline">\mathbf{y}^1</span> a fixed point of the system!</p></li>
<li><p>Remembering that <span class="math inline">w_{ii}=0</span>, we find that <span class="math inline">W</span> is the correlation matrix of <span class="math inline">\mathbf{y}^1</span> minus the identity:</p></li>
</ul>
<p><span class="math display">W = \mathbf{y}^1 \times (\mathbf{y}^1)^T - I</span></p>
<p>(the diagonal of <span class="math inline">\mathbf{y}^1 \times (\mathbf{y}^1)^T</span> is always 1, as <span class="math inline">y^1_j \, y^1_j = 1</span>).</p>
</section>

<section id="storing-patterns-2" class="title-slide slide level1 center">
<h1>Storing patterns</h1>
<ul>
<li>If we have <span class="math inline">P</span> patterns <span class="math inline">(\mathbf{y}^1, \mathbf{y}^2, \dots, \mathbf{y}^P)</span> to store, the corresponding weight matrix is:</li>
</ul>
<p><span class="math display">W = \frac{1}{P} \, \sum_{k=1}^P \mathbf{y}^k \times (\mathbf{y}^k)^T - I</span></p>
<ul>
<li><p><span class="math inline">\frac{1}{P} \, \sum_{k=1}^P \mathbf{y}^k \times (\mathbf{y}^k)^T</span> is the <strong>correlation matrix</strong> of the patterns.</p></li>
<li><p>This does not sound much like <strong>learning</strong> as before, as we are forming the matrix directly from the data, but it is a biologically realistic implementation of <strong>Hebbian learning</strong>.</p></li>
<li><p>We only need to iterate <strong>once</strong> over the training patterns, not multiple epochs.</p></li>
<li><p>Learning can be online: the weight matrix is modified when a new pattern <span class="math inline">\mathbf{y}^k</span> has to be remembered:</p></li>
</ul>
<p><span class="math display">W = W + \mathbf{y}^k \times (\mathbf{y}^k)^T - I</span></p>
<ul>
<li>There is no catastrophic forgetting until we reach the <strong>capacity</strong> <span class="math inline">C = 0.14 \, N</span> of the network.</li>
</ul>
</section>

<section id="hopfield-networks-12" class="title-slide slide level1 center">
<h1>Hopfield networks</h1>
<ul>
<li>Given <span class="math inline">P</span> patterns <span class="math inline">(\mathbf{y}^1, \mathbf{y}^2, \dots, \mathbf{y}^P)</span> to store, build the weight matrix:</li>
</ul>
<p><span class="math display">W = \frac{1}{P} \, \sum_{k=1}^P \mathbf{y}^k \times (\mathbf{y}^k)^T - I</span></p>
<ul>
<li>The energy of the Hopfield network for a new pattern <span class="math inline">\mathbf{y}</span> is (implicitly):</li>
</ul>
<p><span class="math display">
\begin{aligned}
    E(\mathbf{y}) &amp;=  -\frac{1}{2} \, \mathbf{y}^T \times (\frac{1}{P} \, \sum_{k=1}^P \mathbf{y}^k \times (\mathbf{y}^k)^T - I) \times \mathbf{y} - \mathbf{b}^T \times \mathbf{y} \\
    &amp;= -\frac{1}{2 P} \, \sum_{k=1}^P  ((\mathbf{y}^k)^T \times \mathbf{y})^2   - (\frac{1}{2} \, \mathbf{y}^T + \mathbf{b}^T) \times \mathbf{y} \\
\end{aligned}
</span></p>
<p>i.e.&nbsp;a quadratic function of the dot product between the current pattern <span class="math inline">\mathbf{y}</span> and the stored patterns <span class="math inline">\mathbf{y}^k</span>.</p>
<ul>
<li>The stored patterns are local minima of this energy function, which can be retrieved from any pattern <span class="math inline">\mathbf{y}</span> by iteratively applying the <strong>asynchronous update</strong>:</li>
</ul>
<p><span class="math display">\mathbf{y} = \text{sign}(W \times \mathbf{y} + \mathbf{b})</span></p>
</section>

<section id="spurious-patterns" class="title-slide slide level1 center">
<h1>Spurious patterns</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-spurious.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li><p>The problem when the capacity of the network is full is that the stored patterns will start to overlap.</p></li>
<li><p>The retrieved patterns will be a linear combination of the stored patterns, what is called a <strong>spurious pattern</strong> or <strong>metastable state</strong>.</p></li>
</ul>
<p><span class="math display">\mathbf{y} = \pm \, \text{sign}(\alpha_1 \, \mathbf{y}^1 + \alpha_2 \, \mathbf{y}^2 + \dots + \alpha_P \, \mathbf{y}^P)</span></p>
<ul>
<li>A spurious pattern has never seen by the network, but is remembered like other memories (hallucinations).</li>
</ul>
</div></div>
<ul>
<li><p><strong>Unlearning</strong> methods (Hopfield, Feinstein and Palmer, 1983) use a <strong>sleep / wake cycle</strong>:</p>
<ul>
<li><p>When the network is awake, it remembers patterns.</p></li>
<li><p>When the network sleeps (dreams), it unlearns spurious patterns.</p></li>
</ul></li>
</ul>
<div class="footer">
<p><span class="citation" data-cites="Hopfield1983">Hopfield et al. (<a href="#/references" role="doc-biblioref" onclick="">1983</a>)</span> Unlearning has a stabilizing effect in collective memories. Nature 304:158–159.</p>
</div>
</section>

<section id="applications-of-hopfield-networks" class="title-slide slide level1 center">
<h1>Applications of Hopfield networks</h1>
<ul>
<li><p><strong>Optimization:</strong></p>
<ul>
<li><p>Traveling salesman problem <a href="http://fuzzy.cs.ovgu.de/ci/nn/v07_hopfield_en.pdf" class="uri">http://fuzzy.cs.ovgu.de/ci/nn/v07_hopfield_en.pdf</a></p></li>
<li><p>Timetable scheduling</p></li>
<li><p>Routing in communication networks</p></li>
</ul></li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p><strong>Physics:</strong></p>
<ul>
<li>Spin glasses (magnetism)</li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/spinglasses.jpeg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<ul>
<li><p><strong>Computer Vision:</strong></p>
<ul>
<li>Image reconstruction and restoration</li>
</ul></li>
<li><p><strong>Neuroscience:</strong></p>
<ul>
<li>Models of the hippocampus, episodic memory</li>
</ul></li>
</ul>
</section>

<section id="pattern-completion" class="title-slide slide level1 center">
<h1>Pattern completion</h1>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/HOxSKBxUVpg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<div class="footer">
<p><a href="https://www.youtube.com/watch?v=HOxSKBxUVpg" class="uri">https://www.youtube.com/watch?v=HOxSKBxUVpg</a></p>
</div>
</section>

<section id="pattern-completion-1" class="title-slide slide level1 center">
<h1>Pattern completion</h1>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/fCvQcNzUZf0" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<div class="footer">
<p><a href="https://www.youtube.com/watch?v=fCvQcNzUZf0" class="uri">https://www.youtube.com/watch?v=fCvQcNzUZf0</a></p>
</div>
</section>

<section id="modern-hopfield-networks-dense-associative-memories" class="title-slide slide level1 center">
<h1>3 - Modern Hopfield networks / Dense Associative Memories</h1>

</section>

<section id="problem-with-old-school-hopfield-networks" class="title-slide slide level1 center">
<h1>Problem with old-school Hopfield networks</h1>
<ul>
<li><p>The problems with Hopfield networks are:</p>
<ul>
<li><p>Their limited capacity <span class="math inline">0.14 \, N</span>.</p></li>
<li><p>Ghost patterns (reversed images).</p></li>
<li><p>Spurious patterns (bad separation of patterns).</p></li>
<li><p>Retrieval is not error-free.</p></li>
</ul></li>
<li><p>In this example, the masked Homer is closer to the Bart pattern in the energy function, so it converges to its ghost pattern.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-simpson1.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
<div class="footer">
<p>Source: <a href="https://ml-jku.github.io/hopfield-layers/" class="uri">https://ml-jku.github.io/hopfield-layers/</a></p>
</div>
</section>

<section id="problem-with-old-school-hopfield-networks-1" class="title-slide slide level1 center">
<h1>Problem with old-school Hopfield networks</h1>
<ul>
<li>The problem comes mainly from the fact the energy function is a <strong>quadratic function</strong> of the dot product between a state <span class="math inline">\mathbf{y}</span> and the patterns <span class="math inline">\mathbf{y}^k</span>:</li>
</ul>
<p><span class="math display">
    E(\mathbf{y}) \approx -\frac{1}{2 P} \, \sum_{k=1}^P  ((\mathbf{y}^k)^T \times \mathbf{y})^2
</span></p>
<ul>
<li><p><span class="math inline">-((\mathbf{y}^k)^T \times \mathbf{y})^2</span> has minimum when <span class="math inline">\mathbf{y} = \mathbf{y}^k</span>.</p></li>
<li><p>Quadratic functions are very wide, so it is hard to avoid <strong>overlap</strong> between the patterns.</p></li>
</ul>
<div class="columns">
<div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-spurious.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:70%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield_energy_landscape.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<ul>
<li>If we had a <strong>sharper</strong> energy functions, could not we store more patterns and avoid interference?</li>
</ul>
</section>

<section id="modern-hopfield-networks" class="title-slide slide level1 center">
<h1>Modern Hopfield networks</h1>
<ul>
<li>Yes. We could define the energy function as a polynomial function of order <span class="math inline">a&gt; 2</span> (Krotov and Hopfield, 2016):</li>
</ul>
<p><span class="math display">
    E(\mathbf{y}) = -\frac{1}{P} \, \sum_{k=1}^P  ((\mathbf{y}^k)^T \times \mathbf{y})^a
</span></p>
<p>and get a polynomial capacity <span class="math inline">C \approx \alpha_a \, N^{a-1}</span>.</p>
<ul>
<li>Or even an exponential function <span class="math inline">a = \infty</span> (Demircigil et al., 2017):</li>
</ul>
<p><span class="math display">
    E(\mathbf{y}) = - \frac{1}{P} \, \sum_{k=1}^P  \exp((\mathbf{y}^k)^T \times \mathbf{y})
</span></p>
<p>and get an exponential capacity <span class="math inline">C \approx 2^{\frac{N}{2}}</span>! One could store exponentially more patterns than neurons.</p>
<ul>
<li>The question is then: <strong>which update rule would minimize these energies?</strong></li>
</ul>
<div class="footer">
<p><span class="citation" data-cites="Krotov2016">Krotov and Hopfield (<a href="#/references" role="doc-biblioref" onclick="">2016</a>)</span> and <span class="citation" data-cites="Demircigil2017">Demircigil et al. (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span></p>
</div>
</section>

<section id="modern-hopfield-networks-1" class="title-slide slide level1 center">
<h1>Modern Hopfield networks</h1>
<ul>
<li><span class="citation" data-cites="Krotov2016">Krotov and Hopfield (<a href="#/references" role="doc-biblioref" onclick="">2016</a>)</span> and <span class="citation" data-cites="Demircigil2017">Demircigil et al. (<a href="#/references" role="doc-biblioref" onclick="">2017</a>)</span> show that the binary units <span class="math inline">y_i</span> can still be updated asynchronously by comparing the energy of the model with the unit <strong>on or off</strong>:</li>
</ul>
<p><span class="math display">y_i = \text{sign}(- E(y_i = +1) + E(y_i=-1))</span></p>
<ul>
<li><p>If the energy is lower with the unit on than with the unit off, turn it on! Otherwise turn it off.</p></li>
<li><p>Note that computing the energy necessitates to iterate over all patterns, so in practice you should keep the number of patterns small:</p></li>
</ul>
<p><span class="math display">
    E(\mathbf{y}) = - \frac{1}{P} \, \sum_{k=1}^P  \exp((\mathbf{y}^k)^T \times \mathbf{y})
</span></p>
<ul>
<li>However, you are not bounded by <span class="math inline">0.14 \, N</span> anymore, just by the available computational power and RAM.</li>
</ul>
</section>

<section id="modern-hopfield-networks-2" class="title-slide slide level1 center">
<h1>Modern Hopfield networks</h1>
<ul>
<li><p>The increased capacity of the modern Hopfield network makes sure that you store many patterns without interference (separability of patterns).</p></li>
<li><p>Convergence occurs in only one step (one update per neuron).</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-simpson2.png"></p>
<figcaption>Source: <a href="https://ml-jku.github.io/hopfield-layers/" class="uri">https://ml-jku.github.io/hopfield-layers/</a></figcaption>
</figure>
</div>
</section>

<section id="hopfield-networks-is-all-you-need" class="title-slide slide level1 center">
<h1>4 - Hopfield networks is all you need</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/paper-hopfield.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
<div class="footer">
<p><span class="citation" data-cites="Ramsauer2020">Ramsauer et al. (<a href="#/references" role="doc-biblioref" onclick="">2020</a>)</span> Hopfield Networks is All You Need. arXiv:200802217</p>
</div>
</section>

<section id="hopfield-networks-is-all-you-need-1" class="title-slide slide level1 center">
<h1>Hopfield networks is all you need</h1>
<ul>
<li><p><span class="citation" data-cites="Ramsauer2020">Ramsauer et al. (<a href="#/references" role="doc-biblioref" onclick="">2020</a>)</span> extend the principle to <strong>continuous patterns</strong>, i.e.&nbsp;vectors.</p></li>
<li><p>Let’s put our <span class="math inline">P</span> patterns <span class="math inline">(\mathbf{y}^1, \mathbf{y}^2, \dots, \mathbf{y}^P)</span> in a <span class="math inline">N \times P</span> matrix:</p></li>
</ul>
<p><span class="math display">
    X = \begin{bmatrix} \mathbf{y}^1, \mathbf{y}^2, \dots, \mathbf{y}^P \end{bmatrix}
</span></p>
<ul>
<li>We can define the following energy function for a vector <span class="math inline">\mathbf{y}</span>:</li>
</ul>
<p><span class="math display">
    E(\mathbf{y}) = - \text{lse}(\beta, X^T \, \mathbf{y}) + \frac{1}{2} \, \mathbf{y}^T \mathbf{y} + \beta^{-1} \, \log P + \frac{1}{2} \, M
</span></p>
<p>where:</p>
<p><span class="math display">
    \text{lse}(\beta, \mathbf{z}) = \beta^{-1} \, \log (\sum_{i=1}^P \exp \beta z_i)
</span></p>
<p>is the <strong>log-sum-exp</strong> function and <span class="math inline">M</span> is the maximum norm of the patterns.</p>
<ul>
<li>The first term is similar to the energy of a modern Hopfield network.</li>
</ul>
<div class="footer">
<p><span class="citation" data-cites="Ramsauer2020">Ramsauer et al. (<a href="#/references" role="doc-biblioref" onclick="">2020</a>)</span> Hopfield Networks is All You Need. arXiv:200802217</p>
</div>
</section>

<section id="hopfield-networks-is-all-you-need-2" class="title-slide slide level1 center">
<h1>Hopfield networks is all you need</h1>
<ul>
<li>The update rule that minimizes the energy</li>
</ul>
<p><span class="math display">
    E(\mathbf{y}) = - \text{lse}(\beta, X^T \, \mathbf{y}) + \frac{1}{2} \, \mathbf{y}^T \mathbf{y} + \beta^{-1} \, \log P + \frac{1}{2} \, M
</span></p>
<p>is:</p>
<p><span class="math display">
    \mathbf{y} = \text{softmax}(\beta \, \mathbf{y} \, X^T) \, X^T
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/retrieve_homer.svg" style="width:55.0%"></p>
<figcaption>Source: <a href="https://ml-jku.github.io/hopfield-layers/" class="uri">https://ml-jku.github.io/hopfield-layers/</a></figcaption>
</figure>
</div>
<ul>
<li><p>Why? Just read the 100 pages of mathematical proof.</p></li>
<li><p>Take home message: these are just matrix-vector multiplications and a softmax. We can do that!</p></li>
</ul>
</section>

<section id="hopfield-networks-is-all-you-need-3" class="title-slide slide level1 center">
<h1>Hopfield networks is all you need</h1>
<ul>
<li>Continuous Hopfield networks can retrieve precisely continous vectors with an exponential capacity.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/experiment_with_24_patterns_continuous_beta8.000.png"></p>
<figcaption>Source: <a href="https://ml-jku.github.io/hopfield-layers/" class="uri">https://ml-jku.github.io/hopfield-layers/</a></figcaption>
</figure>
</div>
</section>

<section id="hopfield-networks-is-all-you-need-4" class="title-slide slide level1 center">
<h1>Hopfield networks is all you need</h1>
<div class="columns">
<div class="column" style="width:70%;">
<ul>
<li><p>The sharpness of the attractors is controlled by the <strong>temperature parameter</strong> <span class="math inline">\beta</span>.</p></li>
<li><p>You decide whether you want single patterns or meta-stable states, i.e.&nbsp;<strong>combinations of similar patterns</strong>.</p></li>
<li><p>Why would we want that? Because it is the principle of <strong>self-attention</strong>.</p></li>
<li><p>Which other words in the sentence are related to the current word?</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/transformer-principle.png"></p>
<figcaption>Source: <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html" class="uri">https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/reconstruction_with_different_betas.png"></p>
<figcaption>Source: <a href="https://ml-jku.github.io/hopfield-layers/" class="uri">https://ml-jku.github.io/hopfield-layers/</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="hopfield-networks-is-all-you-need-5" class="title-slide slide level1 center">
<h1>Hopfield networks is all you need</h1>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Using the representation of a word <span class="math inline">\mathbf{y}</span>, as well as the rest of the sentence <span class="math inline">X</span>, we can retrieve a new representation <span class="math inline">\mathbf{y}^\text{new}</span> that is a mixture of all words in the sentence.</li>
</ul>
<p><span class="math display">
    \mathbf{y}^\text{new} = \text{softmax}(\beta \, \mathbf{y} \, X^T) \, X^T
</span></p>
<ul>
<li><p>This makes the representation of a word more context-related.</p></li>
<li><p>The representations <span class="math inline">\mathbf{y}</span> and <span class="math inline">X</span> can be learned using weight matrices, so backpropagation can be used.</p></li>
<li><p>This was the key insight of the <strong>transformer</strong> network that has replaced attentional RNNs in NLP.</p></li>
<li><p><strong>Hopfield layers</strong> can replace the transformer self-attention with a better performance.</p></li>
<li><p>The transformer network was introduced with the title “Attention is all you need”, hence the title of this paper…</p></li>
<li><p>The authors claim that a Hopfield layer can also replace fully-connected layers, LSTM layers, attentional layers, but also SVM, KNN or LVQ…</p></li>
</ul>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/hopfield-transformer.png"></p>
<figcaption>Source: <a href="https://ml-jku.github.io/hopfield-layers/" class="uri">https://ml-jku.github.io/hopfield-layers/</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="additional-readings" class="title-slide slide level1 center">
<h1>Additional readings</h1>
<ul>
<li><p>Hopfield, J. J. (1982) Neural networks and physical systems with emergent collective computational properties. Proc. Nat. Acad. Sci. (USA) 79, 2554-2558.</p></li>
<li><p>A video from Geoffrey Hinton himself:</p></li>
</ul>
<p><a href="https://www.youtube.com/watch?v=Rs1XMS8NqB4" class="uri">https://www.youtube.com/watch?v=Rs1XMS8NqB4</a></p>
<ul>
<li><p>John J. Hopfield (2007), Scholarpedia, 2(5):1977. <a href="http://www.scholarpedia.org/article/Hopfield_network" class="uri">http://www.scholarpedia.org/article/Hopfield_network</a></p></li>
<li><p>Chris Eliasmith (2007), Scholarpedia, 2(10):1380. <a href="http://www.scholarpedia.org/article/Attractor_network" class="uri">http://www.scholarpedia.org/article/Attractor_network</a></p></li>
<li><p>Slides from Davide Bacciu (Pisa) <a href="http://didawiki.di.unipi.it/lib/exe/fetch.php/bionics-engineering/computational-neuroscience/2-hopfield-hand.pdf" class="uri">http://didawiki.di.unipi.it/lib/exe/fetch.php/bionics-engineering/computational-neuroscience/2-hopfield-hand.pdf</a></p></li>
</ul>
</section>

<section id="references" class="title-slide slide level1 smaller scrollable">
<h1>References</h1>


<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-Demircigil2017" class="csl-entry" role="listitem">
Demircigil, M., Heusel, J., Löwe, M., Upgang, S., and Vermet, F. (2017). On a model of associative memory with huge storage capacity. <em>J Stat Phys</em> 168, 288–299. doi:<a href="https://doi.org/10.1007/s10955-017-1806-y">10.1007/s10955-017-1806-y</a>.
</div>
<div id="ref-Hopfield1982" class="csl-entry" role="listitem">
Hopfield, J. J. (1982). Neural networks and physical systems with emergent collective computational abilities. <em>PNAS</em> 79, 2554–2558. doi:<a href="https://doi.org/10.1073/pnas.79.8.2554">10.1073/pnas.79.8.2554</a>.
</div>
<div id="ref-Hopfield1983" class="csl-entry" role="listitem">
Hopfield, J. J., Feinstein, D. I., and Palmer, R. G. (1983). <span>“<span>Unlearning</span>”</span> has a stabilizing effect in collective memories. <em>Nature</em> 304, 158–159. doi:<a href="https://doi.org/10.1038/304158a0">10.1038/304158a0</a>.
</div>
<div id="ref-Krotov2016" class="csl-entry" role="listitem">
Krotov, D., and Hopfield, J. J. (2016). Dense <span>Associative Memory</span> for <span>Pattern Recognition</span>. <a href="http://arxiv.org/abs/1606.01164">http://arxiv.org/abs/1606.01164</a>.
</div>
<div id="ref-McEliece1987" class="csl-entry" role="listitem">
McEliece, R., Posner, E., Rodemich, E., and Venkatesh, S. (1987). The capacity of the <span>Hopfield</span> associative memory. <em>IEEE Transactions on Information Theory</em> 33, 461–482. doi:<a href="https://doi.org/10.1109/TIT.1987.1057328">10.1109/TIT.1987.1057328</a>.
</div>
<div id="ref-Ramsauer2020" class="csl-entry" role="listitem">
Ramsauer, H., Schäfl, B., Lehner, J., Seidl, P., Widrich, M., Adler, T., et al. (2020). Hopfield <span>Networks</span> is <span>All You Need</span>. <a href="http://arxiv.org/abs/2008.02217">http://arxiv.org/abs/2008.02217</a>.
</div>
</div>
</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false,"theme":"whiteboard"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/vitay\.github\.io\/course-neurocomputing\/");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>