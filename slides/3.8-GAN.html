<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-0626ff4d7a71b55c8707dcae1d04a9b6.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.6.39">

  <meta name="author" content="Julien Vitay">
  <title>Neurocomputing</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-31e5e70a95169d1719a5a4fd7b5514a9.css">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="img/tuc-new-large.png" data-background-opacity="1" data-background-position="top" data-background-size="30%" class="quarto-title-block center">
  <h1 class="title">Neurocomputing</h1>
  <p class="subtitle">Generative Adversarial Networks</p>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Julien Vitay 
</div>
        <p class="quarto-title-affiliation">
            Professur für Künstliche Intelligenz - Fakultät für Informatik
          </p>
    </div>
</div>

</section>
<section id="generative-adversarial-network" class="title-slide slide level1 center">
<h1>1 - Generative adversarial network</h1>

</section>

<section id="generative-models" class="title-slide slide level1 center">
<h1>Generative models</h1>
<ul>
<li>An autoencoder learns to first encode inputs in a <strong>latent space</strong> and then use a generative model to model the data distribution.</li>
</ul>
<p><span class="math display">\mathcal{L}_\text{autoencoder}(\theta, \phi) = \mathbb{E}_{\mathbf{x} \in \mathcal{D}, \mathbf{z} \sim q_\phi(\mathbf{z}|\mathbf{x})} [ - \log p_\theta(\mathbf{z})]</span></p>
<ul>
<li>Couldn’t we learn a decoder using random noise as input but still learning the distribution of the data?</li>
</ul>
<p><span class="math display">\mathcal{L}_\text{GAN}(\theta, \phi) = \mathbb{E}_{\mathbf{z} \sim \mathcal{N}(0, 1)} [ - \log p_\theta(\mathbf{z}) ]</span></p>
<ul>
<li>After all, this is how random numbers are generated: a uniform distribution of pseudo-random numbers is transformed into samples of another distribution using a mathematical formula.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/generation-distribution.jpeg" style="width:65.0%"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29" class="uri">https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29</a></figcaption>
</figure>
</div>
</section>

<section id="generative-models-1" class="title-slide slide level1 center">
<h1>Generative models</h1>
<ul>
<li><p>The problem is how to estimate the discrepancy between the true distribution and the generated distribution when we only have samples.</p></li>
<li><p>The Maximum Mean Discrepancy (MMD) approach allows to do that, but does not work very well in highly-dimensional spaces.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan-principle2.png"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29" class="uri">https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29</a></figcaption>
</figure>
</div>
</section>

<section id="generative-adversarial-network-1" class="title-slide slide level1 center">
<h1>Generative adversarial network</h1>
<ul>
<li><p>The <strong>Generative Adversarial Network</strong> (GAN, Goodfellow at al., 2014) is a smart way of providing a loss function to the generative model. It is composed of two parts:</p>
<ul>
<li><p>The <strong>Generator</strong> (or decoder) produces an image based on latent variables sampled from some random distribution (e.g.&nbsp;uniform or normal).</p></li>
<li><p>The <strong>Discriminator</strong> has to recognize real images from generated ones.</p></li>
</ul></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan-concept.png" style="width:70.0%"></p>
<figcaption>Source: <a href="https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f" class="uri">https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f</a></figcaption>
</figure>
</div>
<div class="footer">
<p>Goodfellow IJ, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, Bengio Y. 2014. Generative Adversarial Networks. arXiv:14062661</p>
</div>
</section>

<section id="generative-adversarial-network-2" class="title-slide slide level1 center">
<h1>Generative adversarial network</h1>
<ul>
<li><p>The generator only sees noisy latent representations and outputs a reconstruction.</p></li>
<li><p>The discriminator gets alternatively real or generated inputs and predicts whether it is real or fake.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan-simple2.png" style="width:80.0%"></p>
<figcaption>Source: <a href="https://www.oreilly.com/library/view/java-deep-learning/9781788997454/60579068-af4b-4bbf-83f1-e988fbe3b226.xhtml" class="uri">https://www.oreilly.com/library/view/java-deep-learning/9781788997454/60579068-af4b-4bbf-83f1-e988fbe3b226.xhtml</a></figcaption>
</figure>
</div>
</section>

<section id="the-discriminator-should-be-able-to-recognize-false-bills-from-true-ones" class="title-slide slide level1 center">
<h1>The discriminator should be able to recognize false bills from true ones</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan1.png"></p>
<figcaption>Source: <a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7" class="uri">https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7</a></figcaption>
</figure>
</div>
</section>

<section id="the-generator-should-be-able-to-generate-realistic-bills" class="title-slide slide level1 center">
<h1>The generator should be able to generate realistic bills</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan2.png"></p>
<figcaption>Source: <a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7" class="uri">https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7</a></figcaption>
</figure>
</div>
</section>

<section id="the-generator-is-initially-very-bad" class="title-slide slide level1 center">
<h1>The generator is initially very bad…</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan3.png"></p>
<figcaption>Source: <a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7" class="uri">https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7</a></figcaption>
</figure>
</div>
</section>

<section id="but-the-discriminator-too" class="title-slide slide level1 center">
<h1>… but the discriminator too!</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan4.png"></p>
<figcaption>Source: <a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7" class="uri">https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7</a></figcaption>
</figure>
</div>
</section>

<section id="after-a-while-the-discriminator-gets-better" class="title-slide slide level1 center">
<h1>After a while, the discriminator gets better…</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan5.png"></p>
<figcaption>Source: <a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7" class="uri">https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7</a></figcaption>
</figure>
</div>
</section>

<section id="so-the-generator-also-has-to-improve" class="title-slide slide level1 center">
<h1>So the generator also has to improve</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan6.png"></p>
<figcaption>Source: <a href="https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7" class="uri">https://medium.com/@ageitgey/abusing-generative-adversarial-networks-to-make-8-bit-pixel-art-e45d9b96cee7</a></figcaption>
</figure>
</div>
</section>

<section id="generative-adversarial-network-3" class="title-slide slide level1 center">
<h1>Generative adversarial network</h1>
<ul>
<li><p>The generator and the discriminator are in competition with each other.</p></li>
<li><p>The discriminator uses pure <strong>supervised learning</strong>: we know if the input is real or generated (binary classification) and train the discriminator accordingly.</p></li>
<li><p>The generator tries to fool the discriminator, without ever seeing the data!</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan-principle.png"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29" class="uri">https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29</a></figcaption>
</figure>
</div>
</section>

<section id="loss-of-the-discriminator" class="title-slide slide level1 center">
<h1>Loss of the discriminator</h1>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p>Let’s define <span class="math inline">x \sim P_\text{data}(x)</span> as a real image from the dataset and <span class="math inline">G(z)</span> as an image generated by the generator, where <span class="math inline">z \sim P_z(z)</span> is a random input.</p></li>
<li><p>The output of the discriminator is a single sigmoid neuron:</p>
<ul>
<li><p><span class="math inline">D(x) = 1</span> for real images.</p></li>
<li><p><span class="math inline">D(G(z)) = 0</span> for generated images</p></li>
</ul></li>
<li><p>We want both <span class="math inline">D(x)</span> and <span class="math inline">1-D(G(z))</span> to be close from 1.</p></li>
</ul>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/negative-loglikelihood.png" class="quarto-figure quarto-figure-center" style="width:80.0%"></p>
</figure>
</div>
</div></div>
<ul>
<li>The goal of the discriminator is to <strong>minimize</strong> the negative log-likelihood (binary cross-entropy) of classifying correctly the data:</li>
</ul>
<p><span class="math display">
    \mathcal{L}(D) = \mathbb{E}_{x \sim P_\text{data}(x)} [ - \log D(x)] + \mathbb{E}_{z \sim P_z(z)} [ - \log(1 - D(G(z)))]
</span></p>
<ul>
<li>It is similar to logistic regression: <span class="math inline">x</span> belongs to the positive class, <span class="math inline">G(z)</span> to the negative class.</li>
</ul>
<p><span class="math display">
    \mathcal{L}(\mathbf{w}, b) =  - \sum_{i=1}^{N} [t_i \, \log y_i + (1 - t_i) \, \log( 1- y_i)]
</span></p>
</section>

<section id="loss-of-the-generator" class="title-slide slide level1 center">
<h1>Loss of the generator</h1>
<ul>
<li>The goal of the generator is to <strong>maximize</strong> the negative log-likelihood of the discriminator being correct on the generated images, i.e.&nbsp;fool it:</li>
</ul>
<p><span class="math display">
    \mathcal{J}(G) = \mathbb{E}_{z \sim P_z(z)} [ - \log(1 - D(G(z)))]
</span></p>
<ul>
<li>The generator tries to maximize what the discriminator tries to minimize.</li>
</ul>
<div class="columns">
<div class="column" style="width:65%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan-simple2.png"></p>
<figcaption>Source: <a href="https://www.oreilly.com/library/view/java-deep-learning/9781788997454/60579068-af4b-4bbf-83f1-e988fbe3b226.xhtml" class="uri">https://www.oreilly.com/library/view/java-deep-learning/9781788997454/60579068-af4b-4bbf-83f1-e988fbe3b226.xhtml</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:35%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/negative-loglikelihood.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
</section>

<section id="gan-loss" class="title-slide slide level1 center">
<h1>GAN loss</h1>
<ul>
<li>Putting both objectives together, we obtain the following <strong>minimax</strong> problem:</li>
</ul>
<p><span class="math display">
    \min_G \max_D \, \mathcal{V}(D, G) = \mathbb{E}_{x \sim P_\text{data}(x)} [\log D(x)] + \mathbb{E}_{z \sim P_z(z)} [\log(1 - D(G(z)))]
</span></p>
<ul>
<li><p><span class="math inline">D</span> and <span class="math inline">G</span> compete on the same objective function: one tries to maximize it, the other to minimize it.</p></li>
<li><p>Note that the generator <span class="math inline">G</span> never sees the data <span class="math inline">x</span>: all it gets is a <strong>backpropagated gradient</strong> through the discriminator:</p></li>
</ul>
<p><span class="math display">\nabla_{G(z)} \, \mathcal{V}(D, G) = \nabla_{D(G(z))} \, \mathcal{V}(D, G) \times \nabla_{G(z)} \, D(G(z))</span></p>
<ul>
<li>It informs the generator which <strong>pixels</strong> are the most responsible for an eventual bad decision of the discriminator.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan-simple2.png" style="width:40.0%"></p>
<figcaption>Source: <a href="https://www.oreilly.com/library/view/java-deep-learning/9781788997454/60579068-af4b-4bbf-83f1-e988fbe3b226.xhtml" class="uri">https://www.oreilly.com/library/view/java-deep-learning/9781788997454/60579068-af4b-4bbf-83f1-e988fbe3b226.xhtml</a></figcaption>
</figure>
</div>
</section>

<section id="gan-loss-1" class="title-slide slide level1 center">
<h1>GAN loss</h1>
<ul>
<li>This objective function can be optimized when the generator uses gradient descent and the discriminator gradient ascent: just apply a minus sign on the weight updates!</li>
</ul>
<p><span class="math display">
    \min_G \max_D V(D, G) = \mathbb{E}_{x \sim P_\text{data}(x)} [\log D(x)] + \mathbb{E}_{z \sim P_z(z)} [\log(1 - D(G(z)))]
</span></p>
<ul>
<li><p>Both can therefore use the usual <strong>backpropagation</strong> algorithm to adapt their parameters.</p></li>
<li><p>The discriminator and the generator should reach a <strong>Nash equilibrium</strong>: they try to beat each other, but both become better over time.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan-simple2.png" style="width:60.0%"></p>
<figcaption>Source: <a href="https://www.oreilly.com/library/view/java-deep-learning/9781788997454/60579068-af4b-4bbf-83f1-e988fbe3b226.xhtml" class="uri">https://www.oreilly.com/library/view/java-deep-learning/9781788997454/60579068-af4b-4bbf-83f1-e988fbe3b226.xhtml</a></figcaption>
</figure>
</div>
</section>

<section id="generative-adversarial-network-4" class="title-slide slide level1 center">
<h1>Generative adversarial network</h1>
<ul>
<li>The loss functions reach an equilibrium, it is quite hard to tell when the network has converged.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan-loss.png" style="width:80.0%"></p>
<figcaption>Research project - Vivek Bakul Maru - TU Chemnitz</figcaption>
</figure>
</div>
</section>

<section id="dcgan-deep-convolutional-gan" class="title-slide slide level1 center">
<h1>DCGAN : Deep convolutional GAN</h1>
<ul>
<li>DCGAN is the convolutional version of GAN, using transposed convolutions in the generator and concolutions with stride in the discriminator.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dcgan-flat.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dcgan.png" class="quarto-figure quarto-figure-center" style="width:60.0%"></p>
</figure>
</div>
<div class="footer">
<p>Radford, Metz and Chintala (2015). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. arxiv:1511.06434</p>
</div>
</section>

<section id="generative-adversarial-networks" class="title-slide slide level1 center">
<h1>Generative adversarial networks</h1>
<ul>
<li><p>GAN are quite sensible to train: the discriminator should not become too good too early, otherwise there is no usable gradient for the generator.</p></li>
<li><p>In practice, one updates the generator more often than the discriminator.</p></li>
<li><p>There has been many improvements on GANs to stabilizes training:</p>
<ul>
<li><p>Wasserstein GAN (relying on the Wasserstein distance instead of the log-likelihood).</p></li>
<li><p>f-GAN (relying on any f-divergence).</p></li>
</ul></li>
<li><p>But the generator often <strong>collapses</strong>, i.e.&nbsp;outputs always the same image regarless the input noise.</p></li>
<li><p>Hyperparameter tuning is very difficult.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/gan-improvement.png" style="width:90.0%"></p>
<figcaption>Source: Brundage et al.&nbsp;(2018). The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation. arXiv:180207228</figcaption>
</figure>
</div>
<div class="footer">
<p>Salimans et al.&nbsp;(2016). Improved techniques for training GANs. In Advances in Neural Information Processing Systems.</p>
</div>
</section>

<section id="stylegan2" class="title-slide slide level1 center">
<h1>StyleGAN2</h1>
<ul>
<li>StyleGAN2 from NVIDIA is one of the most realistic GAN variant. Check its generated faces at:</li>
</ul>
<p><a href="https://thispersondoesnotexist.com/" class="uri">https://thispersondoesnotexist.com/</a></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/stylegan.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
<div class="footer">
<p>Karras T, Laine S, Aittala M, Hellsten J, Lehtinen J, Aila T. (2020). Analyzing and Improving the Image Quality of StyleGAN. arXiv:191204958</p>
</div>
</section>

<section id="conditional-gans" class="title-slide slide level1 center">
<h1>2 - Conditional GANs</h1>

</section>

<section id="conditional-gan-cgan" class="title-slide slide level1 center">
<h1>Conditional GAN (cGAN)</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cgan.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>The generator can also get additional <strong>deterministic</strong> information to the latent space, not only the random vector <span class="math inline">z</span>.</p></li>
<li><p>One can for example provide the <strong>label</strong> (class) in the context of supervised learning, allowing to generate many <strong>new</strong> examples of each class: data augmentation.</p></li>
<li><p>One could also provide the output of a pre-trained CNN (ResNet) to condition on images.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cgan-mnist.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div></div>
<div class="footer">
<p>Mirza and Osindero (2014). Conditional Generative Adversarial Nets. arXiv:1411.1784</p>
</div>
</section>

<section id="cgan-text-to-image-synthesis" class="title-slide slide level1 center">
<h1>cGAN: text-to-image synthesis</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dcgan_network.jpg" class="quarto-figure quarto-figure-center" style="width:70.0%"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dcgan-textimage.jpg" class="quarto-figure quarto-figure-center" style="width:75.0%"></p>
</figure>
</div>
<div class="footer">
<p>Source: Reed et al.&nbsp;(2016). Generative Adversarial Text to Image Synthesis. arXiv:1605.05396</p>
</div>
</section>

<section id="pix2pix-image-to-image-translation" class="title-slide slide level1 center">
<h1>pix2pix: image-to-image translation</h1>
<ul>
<li><p>cGAN can be extended to have an autoencoder-like architecture, allowing to generate images from images.</p></li>
<li><p><strong>pix2pix</strong> is trained on pairs of similar images in different domains. The conversion from one domain to another is easy in one direction, but we want to learn the opposite.</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dcgan-imageimage.jpg" class="quarto-figure quarto-figure-center" style="width:80.0%"></p>
</figure>
</div>
<div class="footer">
<p>Isola P, Zhu J-Y, Zhou T, Efros AA. 2018. Image-to-Image Translation with Conditional Adversarial Networks. arXiv:161107004. <a href="https://phillipi.github.io/pix2pix/" class="uri">https://phillipi.github.io/pix2pix/</a></p>
</div>
</section>

<section id="pix2pix-image-to-image-translation-1" class="title-slide slide level1 center">
<h1>pix2pix: image-to-image translation</h1>
<div class="columns">
<div class="column" style="width:20%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/pix2pix-generator-principle.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:80%;">
<ul>
<li><p>The goal of the generator is to convert for example a black-and-white image into a colorized one.</p></li>
<li><p>It is a deep convolutional autoencoder, with convolutions with strides and transposed convolutions (SegNet-like).</p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/pix2pix-generator1.png"></p>
<figcaption>Source: <a href="https://affinelayer.com/pix2pix/" class="uri">https://affinelayer.com/pix2pix/</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="pix2pix-image-to-image-translation-2" class="title-slide slide level1 center">
<h1>pix2pix: image-to-image translation</h1>
<div class="columns">
<div class="column" style="width:20%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/pix2pix-generator-principle.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:80%;">
<ul>
<li>In practice, it has a <strong>U-Net</strong> architecture with skip connections to generate fine details.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/pix2pix-generator2.png" class="quarto-figure quarto-figure-center" style="width:70.0%"></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/pix2pix-generator3.png"></p>
<figcaption>Source: <a href="https://affinelayer.com/pix2pix/" class="uri">https://affinelayer.com/pix2pix/</a></figcaption>
</figure>
</div>
</div></div>
</section>

<section id="pix2pix-image-to-image-translation-3" class="title-slide slide level1 center">
<h1>pix2pix: image-to-image translation</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/pix2pix-discriminator-principle.png" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li><p>The discriminator takes a <strong>pair</strong> of images as input: input/target or input/generated.</p></li>
<li><p>It does not output a single value real/fake, but a 30x30 “image” telling how real or fake is the corresponding <strong>patch</strong> of the unknown image.</p></li>
<li><p>Patches correspond to overlapping 70x70 regions of the 256x256 input image.</p></li>
<li><p>This type of discriminator is called a <strong>PatchGAN</strong>.</p></li>
</ul>
</div></div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/pix2pix-discriminator.png"></p>
<figcaption>Source: <a href="https://affinelayer.com/pix2pix/" class="uri">https://affinelayer.com/pix2pix/</a></figcaption>
</figure>
</div>
</section>

<section id="pix2pix-image-to-image-translation-4" class="title-slide slide level1 center">
<h1>pix2pix: image-to-image translation</h1>
<ul>
<li>The discriminator is trained like in a regular GAN by alternating input/target or input/generated pairs.</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/pix2pix-discriminator-training.png" style="width:80.0%"></p>
<figcaption>Source: <a href="https://affinelayer.com/pix2pix/" class="uri">https://affinelayer.com/pix2pix/</a></figcaption>
</figure>
</div>
</section>

<section id="pix2pix-image-to-image-translation-5" class="title-slide slide level1 center">
<h1>pix2pix: image-to-image translation</h1>
<ul>
<li>The generator is trained by maximizing the GAN loss (using gradients backpropagated through the discriminator) but also by minimizing the L1 distance between the generated image and the target (supervised learning).</li>
</ul>
<p><span class="math display">
    \min_G \max_D V(D, G) = V_\text{GAN}(D, G) + \lambda \, \mathbb{E}_\mathcal{D} [|T - G|]
</span></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/pix2pix-generator-training.png" style="width:70.0%"></p>
<figcaption>Source: <a href="https://affinelayer.com/pix2pix/" class="uri">https://affinelayer.com/pix2pix/</a></figcaption>
</figure>
</div>
</section>

<section id="cyclegan-neural-style-transfer" class="title-slide slide level1 center">
<h1>CycleGAN : Neural Style Transfer</h1>
<div class="columns">
<div class="column" style="width:55%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/img_translation.jpeg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:45%;">
<ul>
<li><p>The drawback of pix2pix is that you need <strong>paired</strong> examples of each domain, which is sometimes difficult to obtain.</p></li>
<li><p>In <strong>style transfer</strong>, we are interested in converting images using unpaired datasets, for example realistic photographies and paintings.</p></li>
<li><p><strong>CycleGAN</strong> is a GAN architecture for neural style transfer.</p></li>
</ul>
</div></div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/doge_starrynight.jpg" style="width:80.0%"></p>
<figcaption>Source: <a href="https://hardikbansal.github.io/CycleGANBlog/" class="uri">https://hardikbansal.github.io/CycleGANBlog/</a></figcaption>
</figure>
</div>
<div class="footer">
<p>Zhu J-Y, Park T, Isola P, Efros AA. 2020. Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. arXiv:170310593</p>
</div>
</section>

<section id="cyclegan-neural-style-transfer-1" class="title-slide slide level1 center">
<h1>CycleGAN : Neural Style Transfer</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cycle-gan-zebra-horse-images.jpg" class="quarto-figure quarto-figure-center"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Let’s suppose that we want to transform <strong>domain A</strong> (horses) into <strong>domain B</strong> (zebras) or the other way around.</p></li>
<li><p>The problem is that the two datasets are not paired, so we cannot provide targets to pix2pix (supervised learning).</p></li>
<li><p>If we just select any zebra target for a horse input, pix2pix would learn to generate zebras that do not correspond to the input horse (the shape may be lost).</p></li>
<li><p>How about we train a second GAN to generate the target?</p></li>
</ul>
</div></div>
<div class="footer">
<p>Zhu J-Y, Park T, Isola P, Efros AA. 2020. Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. arXiv:170310593</p>
</div>
</section>

<section id="cyclegan-neural-style-transfer-2" class="title-slide slide level1 center">
<h1>CycleGAN : Neural Style Transfer</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cyclegan-AB.jpeg" style="width:80.0%"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff" class="uri">https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff</a></figcaption>
</figure>
</div>
</section>

<section id="cyclegan-neural-style-transfer-3" class="title-slide slide level1 center">
<h1>CycleGAN : Neural Style Transfer</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cyclegan-BA.jpeg" style="width:80.0%"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff" class="uri">https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff</a></figcaption>
</figure>
</div>
</section>

<section id="cyclegan-neural-style-transfer-4" class="title-slide slide level1 center">
<h1>CycleGAN : Neural Style Transfer</h1>
<div class="columns">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cyclegan.jpeg"></p>
<figcaption>Source: <a href="https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff" class="uri">https://towardsdatascience.com/gender-swap-and-cyclegan-in-tensorflow-2-0-359fe74ab7ff</a></figcaption>
</figure>
</div>
</div><div class="column" style="width:55%;">
<p><strong>Cycle A2B2A</strong></p>
<ul>
<li><p>The A2B generator generates a sample of B from an image of A.</p></li>
<li><p>The B discriminator allows to train A2B using real images of B.</p></li>
<li><p>The B2A generator generates a sample of A from the output of A2B, which can be used to minimize the L1-reconstruction loss (shape-preserving).</p></li>
</ul>
<p><strong>Cycle B2A2B</strong></p>
<ul>
<li><p>In the B2A2B cycle, the domains are reversed, what allows to train the A discriminator.</p></li>
<li><p>This cycle is repeated throughout training, allowing to train both GANS concurrently.</p></li>
</ul>
</div></div>
</section>

<section id="cyclegan-neural-style-transfer-5" class="title-slide slide level1 center">
<h1>CycleGAN : Neural Style Transfer</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cycleGAN2.jpg"></p>
<figcaption>Source: <a href="https://github.com/junyanz/CycleGAN" class="uri">https://github.com/junyanz/CycleGAN</a></figcaption>
</figure>
</div>
</section>

<section id="cyclegan-neural-style-transfer-6" class="title-slide slide level1 center">
<h1>CycleGAN : Neural Style Transfer</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cycleGAN3.jpg"></p>
<figcaption>Source: <a href="https://github.com/junyanz/CycleGAN" class="uri">https://github.com/junyanz/CycleGAN</a></figcaption>
</figure>
</div>
</section>

<section id="cyclegan-neural-style-transfer-7" class="title-slide slide level1 center">
<h1>CycleGAN : Neural Style Transfer</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/cycleGAN4.jpg" style="width:80.0%"></p>
<figcaption>Source: <a href="https://github.com/junyanz/CycleGAN" class="uri">https://github.com/junyanz/CycleGAN</a></figcaption>
</figure>
</div>
</section>

<section id="neural-doodle" class="title-slide slide level1 center">
<h1>Neural Doodle</h1>
<p></p><div id="youtube-frame" style="position: relative; padding-bottom: 56.25%; /* 16:9 */ height: 0;"><iframe width="100%" height="" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;" src="https://www.youtube.com/embed/fu2fzx4w3mI" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><p></p>
<div class="footer">
<p><a href="https://github.com/alexjc/neural-doodle" class="uri">https://github.com/alexjc/neural-doodle</a></p>
</div>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"8\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false,"theme":"whiteboard"},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp("https:\/\/vitay\.github\.io\/course-neurocomputing\/");
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>