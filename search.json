[
  {
    "objectID": "webpage/Installation.html",
    "href": "webpage/Installation.html",
    "title": "Python installation",
    "section": "",
    "text": "In the B202 (FRIZ), conda is already installed. You first need to initialize it by typing this command in a terminal:\nconda init tcsh\nClose the terminal, open a new one, and type:\nconda create --name deeprl python=3.9\nconda activate deeprl\nconda install -c conda-forge numpy matplotlib jupyterlab\nThis can take a while, be patient.\nBefore every session, or when you open a new terminal, you will need to type:\nconda activate deeprl\nHere are the main Python dependencies necessary for the exercises:\nIf you are using Linux, you can probably install all the dependencies (except gym) from your package manager. For the others, use either Anaconda or Colab."
  },
  {
    "objectID": "webpage/Installation.html#anaconda",
    "href": "webpage/Installation.html#anaconda",
    "title": "Python installation",
    "section": "Anaconda",
    "text": "Anaconda\n\nInstalling Anaconda\nPython should be already installed if you use Linux, a very old version if you use MacOS, and probably nothing under Windows. Moreover, Python 2.7 became obsolete in December 2019 but is still the default on some distributions.\nFor these reasons, we strongly recommend installing Python 3 using the Anaconda distribution, or even better the community-driven fork Miniforge:\nhttps://github.com/conda-forge/miniforge\nAnaconda offers all the major Python packages in one place, with a focus on data science and machine learning. To install it, simply download the installer / script for your OS and follow the instructions. Beware, the installation takes quite a lot of space on the disk (around 1 GB), so choose the installation path wisely.\n\n\nInstalling packages\nTo install packages (for example numpy), you just have to type in a terminal:\nconda install numpy\nRefer to the docs (https://docs.anaconda.com/anaconda/) to know more.\nIf you prefer your local Python installation, or if a package is not available or outdated with Anaconda, the pip utility allows to also install virtually any Python package:\npip install numpy\n\n\nVirtual environments\nIt is a good idea to isolate the required packages from the rest of your Python installation, otherwise conflicts between package versions may arise.\nVirtual environments allow to create an isolated Python distribution for a project. The Python ecosystem offers many tools for that:\n\nvenv, the default Python 3 module.\nvirtualenv\npyenv\npipenv\n\nAs we advise to use Anaconda, we focus here on conda environments, but the logic is always the same.\nTo create a conda environment with the name deeprl using Python 3.9, type in a terminal:\nconda create --name deeprl python=3.9\nYou should see that it installs a bunch of basic packages along python:\n(base) ~/ conda create --name deeprl python=3.9\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /Users/vitay/Applications/miniforge3/envs/deeprl\n\n  added / updated specs:\n    - python=3.9\n\n\nThe following NEW packages will be INSTALLED:\n\n  bzip2              conda-forge/osx-arm64::bzip2-1.0.8-h3422bc3_4 None\n  ca-certificates    conda-forge/osx-arm64::ca-certificates-2022.9.24-h4653dfc_0 None\n  libffi             conda-forge/osx-arm64::libffi-3.4.2-h3422bc3_5 None\n  libsqlite          conda-forge/osx-arm64::libsqlite-3.39.4-h76d750c_0 None\n  libzlib            conda-forge/osx-arm64::libzlib-1.2.12-h03a7124_4 None\n  ncurses            conda-forge/osx-arm64::ncurses-6.3-h07bb92c_1 None\n  openssl            conda-forge/osx-arm64::openssl-3.0.5-h03a7124_2 None\n  pip                conda-forge/noarch::pip-22.2.2-pyhd8ed1ab_0 None\n  python             conda-forge/osx-arm64::python-3.9.13-h96fcbfb_0_cpython None\n  readline           conda-forge/osx-arm64::readline-8.1.2-h46ed386_0 None\n  setuptools         conda-forge/noarch::setuptools-65.4.1-pyhd8ed1ab_0 None\n  sqlite             conda-forge/osx-arm64::sqlite-3.39.4-h2229b38_0 None\n  tk                 conda-forge/osx-arm64::tk-8.6.12-he1e0b03_0 None\n  tzdata             conda-forge/noarch::tzdata-2022d-h191b570_0 None\n  wheel              conda-forge/noarch::wheel-0.37.1-pyhd8ed1ab_0 None\n  xz                 conda-forge/osx-arm64::xz-5.2.6-h57fd34a_0 None\n\n\nProceed ([y]/n)?\n\nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n#\n# To activate this environment, use\n#\n#     $ conda activate deeprl\n#\n# To deactivate an active environment, use\n#\n#     $ conda deactivate\n\nRetrieving notices: ...working... done\nAs indicated at the end of the message, you need to activate the environment to use its packages:\nconda activate deeprl\nWhen you are done, you can deactivate it, or simply close the terminal.\n\n\n\n\n\n\nYou need to activate the environment every time you start an exercise or open a new terminal!\n\n\n\nYou can then install all the required packages to their latest versions, alternating between conda and pip:\nconda install numpy matplotlib jupyterlab\npip install tensorflow\npip install gym[all]\n\nIf you installed the regular Anaconda and not miniforge, we strongly advise to force using the conda forge channel:\nconda install -c conda-forge numpy matplotlib jupyterlab\n\nAlternatively, you can use one of the following files and install everything in one shot:\n\nconda-linux.yml for Linux and (possibly) Windows.\nconda-macos.yml for MacOS arm64 (M1). Untested on Intel-based macs.\n\nconda env create -f conda-linux.yml\nconda env create -f conda-macos.yml\n\n\n\n\n\n\nIf you have a CUDA-capable NVIDIA graphical card, follow these instructions to install tensorflow:\nhttps://www.tensorflow.org/install/pip\n\n\n\n\n\nUsing notebooks\nWhen the installation is complete, you just need to download the Jupyter notebook (.ipynb), activate your environment, and type:\njupyter lab name_of_the_notebook.ipynb\nto open a browser tab with the notebook."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Neurocomputing",
    "section": "",
    "text": "You will find below the links to the slides for each lecture (html and pdf).\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n1.1 - Introduction\nIntroduction to the main concepts of reinforcement learning and showcasing of the current applications.\nhtml, pdf\n\n\n1.2 - (optional) Basics in math\nMathematical background necessary to follow this course.\nhtml, pdf\n\n\n1.3 - Neurons\nQuick journey from biological neurons to artificial neurons.\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n2.1 - Optimization\nOverview of gradient descent and regularization.\nhtml, pdf\n\n\n2.2 - Linear regression\nLinear regression, multiple linear regression, logistic regression, polynomial regression and how to evaluate them.\nhtml, pdf\n\n\n2.3 - Linear classification\nHard linear classification, Maximum Likelihood Estimation, Soft linear classication, multi-class softmax classification.\nhtml, pdf\n\n\n2.4 - Learning theory\nVapnik-Chervonenkis dimension, Cover’s theorem, feature spaces and the kernel methods..\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n3.1 - Feedforward neural networks\nBasic neural network aka Multi-layer perceptrons (MLP), and the almighty backpropagation algorithm.\nhtml, pdf\n\n\n3.2 - Modern neural networks\nAdvanced methods for training neural networks: optimizers, activation functions, normalization, etc.\nhtml, pdf\n\n\n3.3 - Convolutional neural networks\nCNNs like AlexNet and its followers (VGG, ResNet, Inception) started the deep learning hype and revolutionized computer vision..\nhtml, pdf\n\n\n3.4 - Object detection\nObject detection networks (R-CNN, YOLO, SSD) are able to locate objects in an image.\nhtml, pdf\n\n\n3.5 - Segmentation network\nSegmentation networks (U-Net) can tell which pixels belong to an object.\nhtml, pdf\n\n\n3.6 - Autoencoders\nAutoencoders and variational autoencoders (VAE) can be used to extract latent representations from raw data.\nhtml, pdf\n\n\n3.7 - Restricted Boltzmann machines\nRBMs are generative stochastic neural networks that can learn the distribution of their inputs.\nhtml, pdf\n\n\n3.8 - Generative Adversarial Networks\nGANs are generative networks able to generate images from pure noise.\nhtml, pdf\n\n\n3.9 - Recurrent neural networks\nRNNs, especially LSTMs, were long the weapon of choice to process temporal sequences (text, video, etc)..\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n4.1 - Transformers\nThe Transformer architecture of (Vaswani, 2017) used self-attention to replace RNNs and start the second wave of AI hype.\nhtml, pdf\n\n\n4.2 - Contrastive learning\nContrastive learning is a form of self-supervised allowing to learn context-relevant representations from raw data..\nhtml, pdf\n\n\n4.3 - Vision Transformer\nVision transformers use the Transformer architecture to be the new state of the art in computer vision.\nhtml, pdf\n\n\n4.4 - Diffusion models\nDiffusion models are a novel probabilistic architecture allowing to learn to generate images (Midjourney, Dall-E, etc) through incremental denoising.\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n5.1 - Limits of deep learning\nThis lecture (provocatively) explains why deep learning-based approaches will never be able to achieve Artificial General Intelligence and why more brain-inspired approaches (neuro-AI) are the next step for AI..\nhtml, pdf\n\n\n5.2 - Hopfield networks\nHopfield network allow to implement associative memory, a fundamental aspect of cognition..\nhtml, pdf\n\n\n5.3 - Reservoir Computing\nReservoir Computing (RC) is a paradigm allowing to train recurrent neural networks on time series with much less compuations than with deep learning approaches.\nhtml, pdf\n\n\n5.4 - Spiking networks\nSpiking networks, in addition to being closer to brain functioning, allow to perform the same computations as deep netowkrs without requiring as much communication, allowing energy-efficient implementations on neuro-morphic hardware.\nhtml, pdf\n\n\n5.5 - Beyond deep learning\nTo conclude, we will see some of the requirement of genetral intelligence that need to be added to our models.\nhtml, pdf"
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Neurocomputing",
    "section": "",
    "text": "You will find below the links to the slides for each lecture (html and pdf).\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n1.1 - Introduction\nIntroduction to the main concepts of reinforcement learning and showcasing of the current applications.\nhtml, pdf\n\n\n1.2 - (optional) Basics in math\nMathematical background necessary to follow this course.\nhtml, pdf\n\n\n1.3 - Neurons\nQuick journey from biological neurons to artificial neurons.\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n2.1 - Optimization\nOverview of gradient descent and regularization.\nhtml, pdf\n\n\n2.2 - Linear regression\nLinear regression, multiple linear regression, logistic regression, polynomial regression and how to evaluate them.\nhtml, pdf\n\n\n2.3 - Linear classification\nHard linear classification, Maximum Likelihood Estimation, Soft linear classication, multi-class softmax classification.\nhtml, pdf\n\n\n2.4 - Learning theory\nVapnik-Chervonenkis dimension, Cover’s theorem, feature spaces and the kernel methods..\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n3.1 - Feedforward neural networks\nBasic neural network aka Multi-layer perceptrons (MLP), and the almighty backpropagation algorithm.\nhtml, pdf\n\n\n3.2 - Modern neural networks\nAdvanced methods for training neural networks: optimizers, activation functions, normalization, etc.\nhtml, pdf\n\n\n3.3 - Convolutional neural networks\nCNNs like AlexNet and its followers (VGG, ResNet, Inception) started the deep learning hype and revolutionized computer vision..\nhtml, pdf\n\n\n3.4 - Object detection\nObject detection networks (R-CNN, YOLO, SSD) are able to locate objects in an image.\nhtml, pdf\n\n\n3.5 - Segmentation network\nSegmentation networks (U-Net) can tell which pixels belong to an object.\nhtml, pdf\n\n\n3.6 - Autoencoders\nAutoencoders and variational autoencoders (VAE) can be used to extract latent representations from raw data.\nhtml, pdf\n\n\n3.7 - Restricted Boltzmann machines\nRBMs are generative stochastic neural networks that can learn the distribution of their inputs.\nhtml, pdf\n\n\n3.8 - Generative Adversarial Networks\nGANs are generative networks able to generate images from pure noise.\nhtml, pdf\n\n\n3.9 - Recurrent neural networks\nRNNs, especially LSTMs, were long the weapon of choice to process temporal sequences (text, video, etc)..\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n4.1 - Transformers\nThe Transformer architecture of (Vaswani, 2017) used self-attention to replace RNNs and start the second wave of AI hype.\nhtml, pdf\n\n\n4.2 - Contrastive learning\nContrastive learning is a form of self-supervised allowing to learn context-relevant representations from raw data..\nhtml, pdf\n\n\n4.3 - Vision Transformer\nVision transformers use the Transformer architecture to be the new state of the art in computer vision.\nhtml, pdf\n\n\n4.4 - Diffusion models\nDiffusion models are a novel probabilistic architecture allowing to learn to generate images (Midjourney, Dall-E, etc) through incremental denoising.\nhtml, pdf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlides\n\n\n\n\n5.1 - Limits of deep learning\nThis lecture (provocatively) explains why deep learning-based approaches will never be able to achieve Artificial General Intelligence and why more brain-inspired approaches (neuro-AI) are the next step for AI..\nhtml, pdf\n\n\n5.2 - Hopfield networks\nHopfield network allow to implement associative memory, a fundamental aspect of cognition..\nhtml, pdf\n\n\n5.3 - Reservoir Computing\nReservoir Computing (RC) is a paradigm allowing to train recurrent neural networks on time series with much less compuations than with deep learning approaches.\nhtml, pdf\n\n\n5.4 - Spiking networks\nSpiking networks, in addition to being closer to brain functioning, allow to perform the same computations as deep netowkrs without requiring as much communication, allowing energy-efficient implementations on neuro-morphic hardware.\nhtml, pdf\n\n\n5.5 - Beyond deep learning\nTo conclude, we will see some of the requirement of genetral intelligence that need to be added to our models.\nhtml, pdf"
  },
  {
    "objectID": "index.html#exercises",
    "href": "index.html#exercises",
    "title": "Neurocomputing",
    "section": "Exercises",
    "text": "Exercises\nYou will find below links to download the notebooks for the exercises (which you have to fill) and their solution (which you can look at after you have finished the exercise). It is recommended not to look at the solution while doing the exercise unless you are lost. Alternatively, you can run the notebooks directly on Colab (https://colab.research.google.com/) if you have a Google account.\nFor instructions on how to install a Python distribution on your computer, check this page.\n\n\n\n\n\n\nNotebook\nSolution\n\n\n\n\n1 - Introduction to Python\nIntroduction to the Python programming language. Optional for students already knowing Python.\nipynb, colab\nipynb, colab\n\n\n2 - Numpy and Matplotlib\nPresentation of the numpy library for numerical computations and matplotlib for visualization. Also optional for students already familiar.\nipynb, colab\nipynb, colab\n\n\n3 - Linear regression\nImplementation of the basic linear regression algorithm in Python and scikit-learn.\nipynb, colab\nipynb, colab\n\n\n4 - Multiple Linear regression\nMLR on the California Housing dataset using scikit-learn.\nipynb, colab\nipynb, colab\n\n\n5 - Cross-validation\nDifferent approaches to cross-validation using scikit-learn.\nipynb, colab\nipynb, colab\n\n\n6 - Linear classification\nHard and soft linear classification.\nipynb, colab\nipynb, colab\n\n\n7 - Softmax classifier\nSoftmax classifier for multi-class classification.\nipynb, colab\nipynb, colab\n\n\n8 - Multi-layer perceptron\nBasic implementation in Python+Numpy of the multi-layer perceptron and the backpropagation algorithm.\nipynb, colab\nipynb, colab\n\n\n9 - MNIST classification using keras\nKeras tutorial applied to classifying the MNIST dataset with a MLP.\nipynb, colab\nipynb, colab\n\n\n10 - Convolutional neural networks\nImplementation of a CNN in keras for MNIST.\nipynb, colab\nipynb, colab\n\n\n11 - Transfer learning\nLeveraging data augmentation and/or pre-trained CNNs (Xception) for learning a small cats vs. dogs dataset.\nipynb, colab\nipynb, colab\n\n\n12 - Variational autoencoders\nImplementing a VAE in keras.\nipynb, colab\nipynb, colab\n\n\n13 - Recurrent neural networks\nSentiment analysis and time series prediction using LSTM layers.\nipynb, colab\nipynb, colab"
  },
  {
    "objectID": "index.html#recommended-readings",
    "href": "index.html#recommended-readings",
    "title": "Neurocomputing",
    "section": "Recommended readings",
    "text": "Recommended readings\n\nKevin Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022. https://probml.github.io/pml-book/book1.html\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.\nFrançois Chollet. Deep Learning with Python. Manning publications, 2017. https://www.manning.com/books/deep-learning-with-python.\nSimon S. Haykin. Neural Networks and Learning Machines, 3rd Edition. Pearson, 2009. http://dai.fmph.uniba.sk/courses/NN/haykin.neural-networks.3ed.2009.pdf."
  }
]