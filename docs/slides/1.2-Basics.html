<!DOCTYPE html>
<html lang="en"><head>
<script src="1.2-Basics_files/libs/clipboard/clipboard.min.js"></script>
<script src="1.2-Basics_files/libs/quarto-html/tabby.min.js"></script>
<script src="1.2-Basics_files/libs/quarto-html/popper.min.js"></script>
<script src="1.2-Basics_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="1.2-Basics_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="1.2-Basics_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="1.2-Basics_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.1.175">

  <meta name="author" content="Julien Vitay">
  <title>Neurocomputing</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="1.2-Basics_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="1.2-Basics_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="1.2-Basics_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="1.2-Basics_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="1.2-Basics_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="1.2-Basics_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="1.2-Basics_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="1.2-Basics_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="1.2-Basics_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="img/tuc.svg" data-background-opacity="1" data-background-position="top" data-background-size="30%" class="center">
  <h1 class="title">Neurocomputing</h1>
  <p class="subtitle">Basics in mathematics</p>
  <p class="author">Julien Vitay</p>
  <p class="institute">Professur für Künstliche Intelligenz - Fakultät für Informatik</p>
  <p class="date"><a href="https://tu-chemnitz.de/informatik/KI/edu/neurocomputing" class="uri">https://tu-chemnitz.de/informatik/KI/edu/neurocomputing</a></p>
</section>

<section id="outline" class="title-slide slide level1 center">
<h1>Outline</h1>
<ol type="1">
<li><p>Linear algebra</p></li>
<li><p>Calculus</p></li>
<li><p>Probability theory</p></li>
<li><p>Statistics</p></li>
<li><p>Information theory</p></li>
</ol>
</section>

<section id="linear-algebra" class="title-slide slide level1 center">
<h1>1 - Linear algebra</h1>

</section>

<section id="mathematical-objects" class="title-slide slide level1 center">
<h1>Mathematical objects</h1>
<ul>
<li><p><strong>Scalars</strong> <span class="math inline">\(x\)</span> are 0-dimensional values. They can either take real values (<span class="math inline">\(x \in \Re\)</span>, e.g.&nbsp;<span class="math inline">\(x = 1.4573\)</span>, floats in CS) or natural values (<span class="math inline">\(x \in \mathbb{N}\)</span>, e.g.&nbsp;<span class="math inline">\(x = 3\)</span>, integers in CS).</p></li>
<li><p><strong>Vectors</strong> <span class="math inline">\(\mathbf{x}\)</span> are 1-dimensional arrays of length <span class="math inline">\(d\)</span>.</p></li>
<li><p>The bold notation <span class="math inline">\(\mathbf{x}\)</span> will be used in this course, but you may also be accustomed to the arrow notation <span class="math inline">\(\overrightarrow{x}\)</span> used on the blackboard. When using real numbers, the <strong>vector space</strong> with <span class="math inline">\(d\)</span> dimensions is noted <span class="math inline">\(\Re^d\)</span>, so we can note <span class="math inline">\(\mathbf{x} \in \Re^d\)</span>.</p></li>
<li><p>Vectors are typically represented vertically to outline their <span class="math inline">\(d\)</span> elements <span class="math inline">\(x_1, x_2, \ldots, x_d\)</span>:</p></li>
</ul>
<p><span class="math display">\[\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix}\]</span></p>
</section>

<section id="mathematical-objects-1" class="title-slide slide level1 center">
<h1>Mathematical objects</h1>
<ul>
<li><p><strong>Matrices</strong> <span class="math inline">\(A\)</span> are 2-dimensional arrays of size (or shape) <span class="math inline">\(m \times n\)</span> (<span class="math inline">\(m\)</span> rows, <span class="math inline">\(n\)</span> columns, <span class="math inline">\(A \in \Re^{m \times n}\)</span>).</p></li>
<li><p>They are represented by a capital letter to distinguish them from scalars (classically also in bold <span class="math inline">\(\mathbf{A}\)</span> but not here). The element <span class="math inline">\(a_{ij}\)</span> of a matrix <span class="math inline">\(A\)</span> is the element on the <span class="math inline">\(i\)</span>-th row and <span class="math inline">\(j\)</span>-th column.</p></li>
</ul>
<p><span class="math display">\[A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix}\]</span></p>
<ul>
<li><strong>Tensors</strong> <span class="math inline">\(\mathcal{A}\)</span> are arrays with more than two dimensions. We will not really do math on these objects, but they are useful internally (hence the name of the <code>tensorflow</code> library).</li>
</ul>
</section>

<section id="vectors" class="title-slide slide level1 center">
<h1>Vectors</h1>
<ul>
<li><p>A vector can be thought of as the <strong>coordinates of a point</strong> in an Euclidean space (such the 2D space), relative to the origin.</p></li>
<li><p>A vector space relies on two fundamental operations, which are that:</p></li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Vectors can be added:</li>
</ul>
<p><span class="math display">\[\mathbf{x} + \mathbf{y} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix} + \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_d \end{bmatrix} = \begin{bmatrix} x_1 + y_1 \\ x_2 + y_2 \\ \vdots \\ x_d + y_d \end{bmatrix}\]</span></p>
<ul>
<li>Vectors can be multiplied by a scalar:</li>
</ul>
<p><span class="math display">\[a \, \mathbf{x} = a \, \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix} = \begin{bmatrix} a \, x_1 \\ a \, x_2 \\ \vdots \\ a \, x_d \end{bmatrix}\]</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vectorspace.png"></p>
<p></p><figcaption>Source: <a href="https://mathinsight.org/image/vector_2d_add" class="uri">https://mathinsight.org/image/vector_2d_add</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>

<section id="properties-of-vector-spaces" class="title-slide slide level1 center">
<h1>Properties of vector spaces</h1>
<ul>
<li><p>These two operations generate a lot of nice properties (see <a href="https://en.wikipedia.org/wiki/Vector_space" class="uri">https://en.wikipedia.org/wiki/Vector_space</a> for a full list), including:</p>
<ul>
<li>associativity:</li>
</ul>
<p><span class="math display">\[\mathbf{x} + (\mathbf{y} + \mathbf{z}) = (\mathbf{x} + \mathbf{y}) + \mathbf{z}\]</span></p>
<ul>
<li>commutativity:</li>
</ul>
<p><span class="math display">\[\mathbf{x} + \mathbf{y} = \mathbf{y} + \mathbf{x}\]</span></p>
<ul>
<li>the existence of a zero vector</li>
</ul>
<p><span class="math display">\[\mathbf{x} + \mathbf{0} = \mathbf{x}\]</span></p>
<ul>
<li>inversion:</li>
</ul>
<p><span class="math display">\[\mathbf{x} + (-\mathbf{x}) = \mathbf{0}\]</span></p>
<ul>
<li>distributivity:</li>
</ul>
<p><span class="math display">\[a \, (\mathbf{x} + \mathbf{y}) = a \, \mathbf{x} + a \, \mathbf{y}\]</span></p></li>
</ul>
</section>

<section id="norm-of-a-vector" class="title-slide slide level1 center">
<h1>Norm of a vector</h1>
<div class="columns">
<div class="column" style="width:15%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/vector_norms.png"></p>
</figure>
</div>
</div><div class="column" style="width:85%;">
<ul>
<li>Vectors have a <strong>norm</strong> (or length) <span class="math inline">\(||\mathbf{x}||\)</span>. The most intuitive one (if you know the Pythagoras theorem) is the <strong>Euclidean norm</strong> or <span class="math inline">\(L^2\)</span>-norm, which sums the square of each element:</li>
</ul>
<p><span class="math display">\[||\mathbf{x}||_2 = \sqrt{x_1^2 + x_2^2 + \ldots + x_d^2}\]</span></p>
<ul>
<li>Other norms exist, distinguished by the subscript. The <strong><span class="math inline">\(L^1\)</span>-norm</strong> (also called the Manhattan norm) sums the absolute value of each element:</li>
</ul>
<p><span class="math display">\[||\mathbf{x}||_1 = |x_1| + |x_2| + \ldots + |x_d|\]</span></p>
<ul>
<li>The <strong>p-norm</strong> generalizes the Euclidean norm to other powers <span class="math inline">\(p\)</span>:</li>
</ul>
<p><span class="math display">\[||\mathbf{x}||_p = (|x_1|^p + |x_2|^p + \ldots + |x_d|^p)^{\frac{1}{p}}\]</span></p>
<ul>
<li>The <strong>infinity norm</strong> (or maximum norm) <span class="math inline">\(L^\infty\)</span> returns the maximum element of the vector:</li>
</ul>
<p><span class="math display">\[||\mathbf{x}||_\infty = \max(|x_1|, |x_2|, \ldots, |x_d|)\]</span></p>
</div>
</div>
<div class="footer">
<p>Source: <a href="https://en.wikipedia.org/wiki/Norm_(mathematics)" class="uri">https://en.wikipedia.org/wiki/Norm_(mathematics)</a></p>
</div>
</section>

<section id="dot-product" class="title-slide slide level1 center">
<h1>Dot product</h1>
<ul>
<li>One important operation for vectors is the <strong>dot product</strong> (also called scalar product or inner product) between two vectors:</li>
</ul>
<p><span class="math display">\[\langle \mathbf{x} \cdot \mathbf{y} \rangle = \langle \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix} \cdot \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_d \end{bmatrix} \rangle = x_1 \, y_1 + x_2 \, y_2 + \ldots + x_d \, y_d\]</span></p>
<ul>
<li><p>The dot product basically sums one by one the product of the elements of each vector. The angular brackets are sometimes omitted (<span class="math inline">\(\mathbf{x} \cdot \mathbf{y}\)</span>) but we will use them in this course for clarity.</p></li>
<li><p>One can notice immediately that the dot product is <strong>symmetric</strong>:</p></li>
</ul>
<p><span class="math display">\[\langle \mathbf{x} \cdot \mathbf{y} \rangle = \langle \mathbf{y} \cdot \mathbf{x} \rangle\]</span></p>
<p>and <strong>linear</strong>:</p>
<p><span class="math display">\[\langle (a \, \mathbf{x} + b\, \mathbf{y}) \cdot \mathbf{z} \rangle = a\, \langle \mathbf{x} \cdot \mathbf{z} \rangle + b \, \langle \mathbf{y} \cdot \mathbf{z} \rangle\]</span></p>
</section>

<section id="dot-product-1" class="title-slide slide level1 center">
<h1>Dot product</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>The dot product is an indirect measurement of the <strong>angle</strong> <span class="math inline">\(\theta\)</span> between two vectors:</li>
</ul>
<p><span class="math display">\[\langle \mathbf{x} \cdot \mathbf{y} \rangle = ||\mathbf{x}||_2 \, ||\mathbf{y}||_2 \, \cos(\theta)\]</span></p>
<ul>
<li><p>If you normalize the two vectors by dividing them by their norm (which is a scalar), we indeed have the cosine of the angle between them</p></li>
<li><p>The higher the normalized dot product, the more the two vectors point towards the same direction (<strong>cosine distance</strong> between two vectors).</p></li>
</ul>
<p><span class="math display">\[\langle \displaystyle\frac{\mathbf{x}}{||\mathbf{x}||_2} \cdot \frac{\mathbf{y}}{||\mathbf{y}||_2} \rangle =  \cos(\theta)\]</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dot_product_projection_unit_vector.png"></p>
<p></p><figcaption>Source: <a href="https://mathinsight.org/image/dot_product_projection_unit_vector" class="uri">https://mathinsight.org/image/dot_product_projection_unit_vector</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>

<section id="matrices" class="title-slide slide level1 center">
<h1>Matrices</h1>
<ul>
<li>Matrices are derived from vectors, so most of the previous properties will be true. Let’s consider this 4x3 matrix:</li>
</ul>
<p><span class="math display">\[A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33} \\
a_{41} &amp; a_{42} &amp; a_{43} \\
\end{bmatrix}\]</span></p>
<ul>
<li>Each column of the matrix is a vector with 4 elements:</li>
</ul>
<p><span class="math display">\[\mathbf{a}_1 = \begin{bmatrix}
a_{11} \\
a_{21} \\
a_{31} \\
a_{41} \\
\end{bmatrix} \qquad
\mathbf{a}_2 = \begin{bmatrix}
a_{12} \\
a_{22} \\
a_{32} \\
a_{42} \\
\end{bmatrix} \qquad
\mathbf{a}_3 = \begin{bmatrix}
a_{13} \\
a_{23} \\
a_{33} \\
a_{43} \\
\end{bmatrix} \qquad
\]</span></p>
<ul>
<li>A <span class="math inline">\(m \times n\)</span> matrix is therefore a collection of <span class="math inline">\(n\)</span> vectors of size <span class="math inline">\(m\)</span> put side by side column-wise:</li>
</ul>
<p><span class="math display">\[A = \begin{bmatrix}
\mathbf{a}_1 &amp; \mathbf{a}_2 &amp; \mathbf{a}_3\\
\end{bmatrix}\]</span></p>
</section>

<section id="properties-of-matrix-spaces" class="title-slide slide level1 center">
<h1>Properties of matrix spaces</h1>
<ul>
<li>All properties of the vector spaces (associativity, commutativity, distributivity) also apply to matrices, as additions and multiplications with a scalar are defined.</li>
</ul>
<p><span class="math display">\[\alpha \, A + \beta \, B = \begin{bmatrix}
\alpha\, a_{11} + \beta \, b_{11} &amp; \alpha\, a_{12} + \beta \, b_{12} &amp; \alpha\, a_{13} + \beta \, b_{13} \\
\alpha\, a_{21} + \beta \, b_{21} &amp; \alpha\, a_{22} + \beta \, b_{22} &amp; \alpha\, a_{23} + \beta \, b_{23} \\
\alpha\, a_{31} + \beta \, b_{31} &amp; \alpha\, a_{32} + \beta \, b_{32} &amp; \alpha\, a_{33} + \beta \, b_{33} \\
\alpha\, a_{41} + \beta \, b_{41} &amp; \alpha\, a_{42} + \beta \, b_{42} &amp; \alpha\, a_{43} + \beta \, b_{43} \\
\end{bmatrix}\]</span></p>
<p><strong>Note:</strong> Beware, you can only add matrices of the same dimensions <span class="math inline">\(m\times n\)</span>. You cannot add a <span class="math inline">\(2\times 3\)</span> matrix to a <span class="math inline">\(5 \times 4\)</span> one.</p>
</section>

<section id="transposition" class="title-slide slide level1 center">
<h1>Transposition</h1>
<ul>
<li>The <strong>transpose</strong> <span class="math inline">\(A^T\)</span> of a <span class="math inline">\(m \times n\)</span> matrix <span class="math inline">\(A\)</span> is a <span class="math inline">\(n \times m\)</span> matrix, where the row and column indices are swapped:</li>
</ul>
<p><span class="math display">\[A = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix}, \qquad
A^T = \begin{bmatrix}
a_{11} &amp; a_{21} &amp; \cdots &amp; a_{m1} \\
a_{12} &amp; a_{22} &amp; \cdots &amp; a_{m2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{1n} &amp; a_{2n} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix}
\]</span></p>
<ul>
<li>This is also true for vectors, which become horizontal after transposition:</li>
</ul>
<p><span class="math display">\[\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix}, \qquad
\mathbf{x}^T = \begin{bmatrix} x_1 &amp; x_2 &amp; \ldots &amp; x_d \end{bmatrix}
\]</span></p>
</section>

<section id="matrix-multiplication" class="title-slide slide level1 center">
<h1>Matrix multiplication</h1>
<ul>
<li>If <span class="math inline">\(A\)</span> is a <span class="math inline">\(m\times n\)</span> matrix and <span class="math inline">\(B\)</span> a <span class="math inline">\(n \times p\)</span> matrix:</li>
</ul>
<p><span class="math display">\[
A=\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix},\quad
B=\begin{bmatrix}
b_{11} &amp; b_{12} &amp; \cdots &amp; b_{1p} \\
b_{21} &amp; b_{22} &amp; \cdots &amp; b_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
b_{n1} &amp; b_{n2} &amp; \cdots &amp; b_{np} \\
\end{bmatrix}
\]</span></p>
<p>we can multiply them to obtain a <span class="math inline">\(m \times p\)</span> matrix:</p>
<p><span class="math display">\[
C = A \times B =\begin{bmatrix}
c_{11} &amp; c_{12} &amp; \cdots &amp; c_{1p} \\
c_{21} &amp; c_{22} &amp; \cdots &amp; c_{2p} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
c_{m1} &amp; c_{m2} &amp; \cdots &amp; c_{mp} \\
\end{bmatrix}
\]</span></p>
<p>where each element <span class="math inline">\(c_{ij}\)</span> is the dot product of the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(A\)</span> and <span class="math inline">\(j\)</span>th column of <span class="math inline">\(B\)</span>:</p>
<p><span class="math display">\[c_{ij} = \langle A_{i, :} \cdot B_{:, j} \rangle = a_{i1}b_{1j} + a_{i2}b_{2j} +\cdots + a_{in}b_{nj}= \sum_{k=1}^n a_{ik}b_{kj}\]</span></p>
<p><strong>Note:</strong> <span class="math inline">\(n\)</span>, the number of columns of <span class="math inline">\(A\)</span> and rows of <span class="math inline">\(B\)</span>, must be the same!</p>
</section>

<section id="matrix-multiplication-1" class="title-slide slide level1 center">
<h1>Matrix multiplication</h1>
<ul>
<li>The element <span class="math inline">\(c_{ij}\)</span> of <span class="math inline">\(C = A \times B\)</span> is the dot product between the <span class="math inline">\(i\)</span>th row of <span class="math inline">\(A\)</span> and the <span class="math inline">\(j\)</span>th column of <span class="math inline">\(B\)</span>.</li>
</ul>

<img data-src="img/matrixmultiplication.jpg" style="width:50.0%" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Book%3A_Mathematical_Methods_in_Chemistry_(Levitus)/15%3A_Matrices/15.03%3A_Matrix_Multiplication" class="uri">https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Book%3A_Mathematical_Methods_in_Chemistry_(Levitus)/15%3A_Matrices/15.03%3A_Matrix_Multiplication</a> CC BY-NC-SA; Marcia Levitus</p></section>

<section id="matrix-vector-multiplication" class="title-slide slide level1 center">
<h1>Matrix-vector multiplication</h1>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Thinking of vectors as <span class="math inline">\(n \times 1\)</span> matrices, we can multiply a matrix <span class="math inline">\(m \times n\)</span> with a vector:</li>
</ul>
<p><span class="math display">\[
\mathbf{y} = A \times \mathbf{x} = \begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn} \\
\end{bmatrix} \times \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_m \end{bmatrix}
\]</span></p>
<ul>
<li><p>The result <span class="math inline">\(\mathbf{y}\)</span> is a vector of size <span class="math inline">\(m\)</span>.</p></li>
<li><p>In that sense, a matrix <span class="math inline">\(A\)</span> can transform a vector of size <span class="math inline">\(n\)</span> into a vector of size <span class="math inline">\(m\)</span>:</p>
<ul>
<li><span class="math inline">\(A\)</span> represents a <strong>projection</strong> from <span class="math inline">\(\Re^n\)</span> to <span class="math inline">\(\Re^m\)</span>.</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/projection.png"></p>
<p></p><figcaption>Source: <a href="https://en.wikipedia.org/wiki/Homogeneous_coordinate" class="uri">https://en.wikipedia.org/wiki/Homogeneous_coordinate</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>

<section id="dot-product-2" class="title-slide slide level1 center">
<h1>Dot product</h1>
<ul>
<li>Note that the <strong>dot product</strong> between two vectors of size <span class="math inline">\(n\)</span> is the matrix multiplication between the transpose of the first vector and the second one:</li>
</ul>
<p><span class="math display">\[\mathbf{x}^T \times \mathbf{y} = \begin{bmatrix} x_1 &amp; x_2 &amp; \ldots &amp; x_n \end{bmatrix} \times \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix} = x_1 \, y_1 + x_2 \, y_2 + \ldots + x_n \, y_n = \langle \mathbf{x} \cdot \mathbf{y} \rangle\]</span></p>
</section>

<section id="matrix-inversion" class="title-slide slide level1 center">
<h1>Matrix inversion</h1>
<ul>
<li><p>Square matrices of size <span class="math inline">\(n \times n\)</span> can be inverted.</p></li>
<li><p>The <strong>inverse</strong> <span class="math inline">\(A^{-1}\)</span> of a matrix <span class="math inline">\(A\)</span> is defined by:</p></li>
</ul>
<p><span class="math display">\[A \times A^{-1} = A^{-1} \times A = I\]</span></p>
<p>where <span class="math inline">\(I\)</span> is the identity matrix (a matrix with ones on the diagonal and 0 otherwise).</p>
<ul>
<li>Matrix inversion allows to solve linear systems of equations. Given the problem:</li>
</ul>
<p><span class="math display">\[
\begin{cases}
    a_{11} \, x_1 + a_{12} \, x_2 + \ldots + a_{1n} \, x_n = b_1 \\
    a_{21} \, x_1 + a_{22} \, x_2 + \ldots + a_{2n} \, x_n = b_2 \\
    \ldots \\
    a_{n1} \, x_1 + a_{n2} \, x_2 + \ldots + a_{nn} \, x_n = b_n \\
\end{cases}
\]</span></p>
<p>which is equivalent to:</p>
<p><span class="math display">\[A \times \mathbf{x} = \mathbf{b}\]</span></p>
<ul>
<li>We can multiply both sides to the left with <span class="math inline">\(A^{-1}\)</span> (if it exists) and obtain:</li>
</ul>
<p><span class="math display">\[\mathbf{x} = A^{-1} \times \mathbf{b}\]</span></p>
</section>

<section id="calculus" class="title-slide slide level1 center">
<h1>2 - Calculus</h1>

</section>

<section id="univariate-functions" class="title-slide slide level1 center">
<h1>Univariate functions</h1>
<ul>
<li>A <strong>univariate function</strong> <span class="math inline">\(f\)</span> associates to any real number <span class="math inline">\(x \in \Re\)</span> (or a subset of <span class="math inline">\(\Re\)</span> called the support of the function) another (unique) real number <span class="math inline">\(f(x)\)</span>:</li>
</ul>
<p><span class="math display">\[
\begin{align}
f\colon \quad \Re &amp;\to \Re\\
x &amp;\mapsto f(x),\end{align}
\]</span></p>

<img data-src="img/function.png" class="r-stretch quarto-figure-center"></section>

<section id="multivariate-functions" class="title-slide slide level1 center">
<h1>Multivariate functions</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>A <strong>multivariate function</strong> <span class="math inline">\(f\)</span> associates to any vector <span class="math inline">\(\mathbf{x} \in \Re^n\)</span> (or a subset) a real number <span class="math inline">\(f(\mathbf{x})\)</span>:</li>
</ul>
<p><span class="math display">\[
\begin{align}
f\colon \quad \Re^n &amp;\to \Re\\
\mathbf{x} &amp;\mapsto f(\mathbf{x}),\end{align}
\]</span></p>
<ul>
<li><p>The variables of the function are the elements of the vector.</p></li>
<li><p>For low-dimensional vector spaces, it is possible to represent each element explicitly, for example:</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
f\colon \quad\Re^3 &amp;\to \Re\\
x, y, z &amp;\mapsto f(x, y, z),\end{align}
\]</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/multivariatefunction.png"></p>
<p></p><figcaption>Source: <a href="https://en.wikipedia.org/wiki/Function_of_several_real_variables" class="uri">https://en.wikipedia.org/wiki/Function_of_several_real_variables</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
</section>

<section id="vector-fields" class="title-slide slide level1 center">
<h1>Vector fields</h1>
<ul>
<li><strong>Vector fields</strong> associate to any vector <span class="math inline">\(\mathbf{x} \in \Re^n\)</span> (or a subset) another vector (possibly of different size):</li>
</ul>
<p><span class="math display">\[
\begin{align}
\overrightarrow{f}\colon \quad \Re^n &amp;\to \Re^m\\
\mathbf{x} &amp;\mapsto \overrightarrow{f}(\mathbf{x}),\end{align}
\]</span></p>

<img data-src="img/vectorfield.png" style="width:35.0%" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://en.wikipedia.org/wiki/Vector_field" class="uri">https://en.wikipedia.org/wiki/Vector_field</a></p><p><strong>Note:</strong> The matrix-vector multiplication <span class="math inline">\(\mathbf{y} = A \times \mathbf{x}\)</span> is a linear vector field, mapping any vector <span class="math inline">\(\mathbf{x}\)</span> into another vector <span class="math inline">\(\mathbf{y}\)</span>.</p>
</section>

<section id="differentiation" class="title-slide slide level1 center">
<h1>Differentiation</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>Differential calculus deals with the <strong>derivative</strong> of a function, a process called differentiation.</p></li>
<li><p>The derivative <span class="math inline">\(f'(x)\)</span> or <span class="math inline">\(\displaystyle\frac{d f(x)}{dx}\)</span> of a univariate function <span class="math inline">\(f(x)\)</span> is defined as the local <em>slope</em> of the tangent to the function for a given value of <span class="math inline">\(x\)</span>:</p></li>
</ul>
<p><span class="math display">\[f'(x) = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}\]</span></p>
<ul>
<li>The line passing through the points <span class="math inline">\((x, f(x))\)</span> and <span class="math inline">\((x + h, f(x + h))\)</span> becomes tangent to the function when <span class="math inline">\(h\)</span> becomes very small.</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/derivative-approx.png"></p>
</figure>
</div>
</div>
</div>
</section>

<section id="differentiation-1" class="title-slide slide level1 center">
<h1>Differentiation</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>The sign of the derivative tells you how the function behaves locally:</p>
<ul>
<li><p>If the derivative is positive, increasing a little bit <span class="math inline">\(x\)</span> increases the function <span class="math inline">\(f(x)\)</span>, so the function is <strong>locally increasing</strong>.</p></li>
<li><p>If the derivative is negative, increasing a little bit <span class="math inline">\(x\)</span> decreases the function <span class="math inline">\(f(x)\)</span>, so the function is <strong>locally decreasing</strong>.</p></li>
</ul></li>
<li><p>It basically allows you to measure the local influence of <span class="math inline">\(x\)</span> on <span class="math inline">\(f(x)\)</span>: if I change a little bit the value <span class="math inline">\(x\)</span>, what happens to <span class="math inline">\(f(x)\)</span>? This will be very useful in machine learning.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/derivative-approx.png"></p>
</figure>
</div>
</div>
</div>
</section>

<section id="extrema" class="title-slide slide level1 center">
<h1>Extrema</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li><p>A special case is when the derivative is equal to 0 in <span class="math inline">\(x\)</span>. <span class="math inline">\(x\)</span> is then called an <strong>extremum</strong> (or optimum) of the function, i.e.&nbsp;it can be a maximum or minimum.</p></li>
<li><p>You can tell whether an extremum is a maximum or a minimum by looking at its second-order derivative:</p>
<ul>
<li><p>If <span class="math inline">\(f''(x) &gt; 0\)</span>, the extremum is a <strong>minimum</strong>.</p></li>
<li><p>If <span class="math inline">\(f''(x) &lt; 0\)</span>, the extremum is a <strong>maximum</strong>.</p></li>
<li><p>If <span class="math inline">\(f''(x) = 0\)</span>, the extremum is a <strong>saddle point</strong>.</p></li>
</ul></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/optimization-example.png"></p>
</figure>
</div>
</div>
</div>
</section>

<section id="gradients" class="title-slide slide level1 center">
<h1>Gradients</h1>
<ul>
<li>The derivative of a <strong>multivariate function</strong> <span class="math inline">\(f(\mathbf{x})\)</span> is a vector of partial derivatives called the <strong>gradient of the function</strong> <span class="math inline">\(\nabla_\mathbf{x} \, f(\mathbf{x})\)</span>:</li>
</ul>
<p><span class="math display">\[
    \nabla_\mathbf{x} \, f(\mathbf{x}) = \begin{bmatrix}
        \displaystyle\frac{\partial f(\mathbf{x})}{\partial x_1} \\
        \displaystyle\frac{\partial f(\mathbf{x})}{\partial x_2} \\
        \ldots \\
        \displaystyle\frac{\partial f(\mathbf{x})}{\partial x_n} \\
    \end{bmatrix}
\]</span></p>
<ul>
<li>The subscript to the <span class="math inline">\(\nabla\)</span> operator denotes <em>with respect to</em> (w.r.t) which variable the differentiation is done.</li>
</ul>
</section>

<section id="partial-derivatives" class="title-slide slide level1 center">
<h1>Partial derivatives</h1>
<ul>
<li>A <strong>partial derivative</strong> w.r.t. to particular variable (or element of the vector) is simply achieved by differentiating the function while considering all other variables to be <strong>constant</strong>. For example the function:</li>
</ul>
<p><span class="math display">\[f(x, y) = x^2 + 3 \, x \, y + 4 \, x \, y^2 - 1\]</span></p>
<p>can be partially differentiated w.r.t. <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> as:</p>
<p><span class="math display">\[\begin{cases}
\displaystyle\frac{\partial f(x, y)}{\partial x} = 2 \, x + 3\, y + 4 \, y^2 \\
\\
\displaystyle\frac{\partial f(x, y)}{\partial y} = 3 \, x + 8\, x \, y
\end{cases}\]</span></p>
</section>

<section id="jacobian" class="title-slide slide level1 center">
<h1>Jacobian</h1>
<ul>
<li>The gradient can be generalized to <strong>vector fields</strong>, where the <strong>Jacobian</strong> or <strong>Jacobi matrix</strong> is a matrix containing all partial derivatives.</li>
</ul>
<p><span class="math display">\[
J = \begin{bmatrix}
    \dfrac{\partial \mathbf{f}}{\partial x_1} &amp; \cdots &amp; \dfrac{\partial \mathbf{f}}{\partial x_n} \end{bmatrix}
= \begin{bmatrix}
    \dfrac{\partial f_1}{\partial x_1} &amp; \cdots &amp; \dfrac{\partial f_1}{\partial x_n}\\
    \vdots &amp; \ddots &amp; \vdots\\
    \dfrac{\partial f_m}{\partial x_1} &amp; \cdots &amp; \dfrac{\partial f_m}{\partial x_n} \end{bmatrix}
\]</span></p>
</section>

<section id="analytical-properties" class="title-slide slide level1 center">
<h1>Analytical properties</h1>
<ul>
<li>Differentiation is linear, which means that if we define the function:</li>
</ul>
<p><span class="math display">\[h(x) = a \, f(x) + b \, g(x)\]</span></p>
<p>its derivative is:</p>
<p><span class="math display">\[h'(x) = a \, f'(x) + b \, g'(x)\]</span></p>
<ul>
<li>A product of functions can also be differentiated analytically (product rule):</li>
</ul>
<p><span class="math display">\[(f(x) \times g(x))' = f'(x) \times g(x) + f(x) \times g'(x)\]</span></p>
<p><strong>Example:</strong></p>
<p><span class="math display">\[f(x) = x^2 \, e^x\]</span></p>
<p><span class="math display">\[f'(x) = 2 \, x \, e^x + x^2 \cdot e^x\]</span></p>
</section>

<section id="chain-rule" class="title-slide slide level1 center">
<h1>Chain rule</h1>
<ul>
<li>A very important concept for neural networks is the <strong>chain rule</strong>, which tells how to differentiate <strong>function compositions</strong> (functions of a function) of the form:</li>
</ul>
<p><span class="math display">\[(f \circ g) (x) = f(g(x))\]</span></p>
<ul>
<li>The derivative of <span class="math inline">\(f \circ g\)</span> is:</li>
</ul>
<p><span class="math display">\[(f \circ g)' (x) = (f' \circ g) (x) \times g'(x)\]</span></p>
<ul>
<li>The chain rule may be more understandable using Leibniz’s notation:</li>
</ul>
<p><span class="math display">\[\frac{d (f \circ g) (x)}{dx} = \frac{d f (g (x))}{d g(x)} \times \frac{d g (x)}{dx}\]</span></p>
<ul>
<li>By posing <span class="math inline">\(y = g(x)\)</span> as an intermediary variable, it becomes:</li>
</ul>
<p><span class="math display">\[\frac{d f(y)}{dx} = \frac{d f(y)}{dy} \times \frac{dy}{dx}\]</span></p>
</section>

<section id="chain-rule-1" class="title-slide slide level1 center">
<h1>Chain rule</h1>
<ul>
<li>The function :</li>
</ul>
<p><span class="math display">\[h(x) = \frac{1}{2 \, x + 1}\]</span></p>
<p>is the function composition of <span class="math inline">\(g(x) = 2 \, x + 1\)</span> and <span class="math inline">\(f(x) = \displaystyle\frac{1}{x}\)</span>, whose derivatives are known:</p>
<p><span class="math display">\[g'(x) = 2\]</span> <span class="math display">\[f'(x) = -\displaystyle\frac{1}{x^2}\]</span></p>
<ul>
<li>Its derivative according to the <strong>chain rule</strong> is:</li>
</ul>
<p><span class="math display">\[h'(x) = f'(g(x)) \times g'(x) = -\displaystyle\frac{1}{(2 \, x + 1)^2} \times 2\]</span></p>
</section>

<section id="chain-rule-2" class="title-slide slide level1 center">
<h1>Chain rule</h1>
<ul>
<li>The chain rule also applies to partial derivatives:</li>
</ul>
<p><span class="math display">\[
    \displaystyle\frac{\partial f \circ g (x, y)}{\partial x} = \frac{\partial f \circ g (x, y)}{\partial g (x, y)} \times \frac{\partial g (x, y)}{\partial x}
\]</span></p>
<p>and gradients:</p>
<p><span class="math display">\[
    \nabla_\mathbf{x} \, f \circ g (\mathbf{x}) = \nabla_{g(\mathbf{x})} \, f \circ g (\mathbf{x}) \times \nabla_\mathbf{x} \, g (\mathbf{x})
\]</span></p>
</section>

<section id="integrals" class="title-slide slide level1 center">
<h1>Integrals</h1>
<ul>
<li>The opposite operation of differentation is <strong>integration</strong>. Given a function <span class="math inline">\(f(x)\)</span>, we search a function <span class="math inline">\(F(x)\)</span> whose <em>derivative</em> is <span class="math inline">\(f(x)\)</span>:</li>
</ul>
<p><span class="math display">\[F'(x) = f(x)\]</span></p>
<ul>
<li>The <strong>integral</strong> of <span class="math inline">\(f\)</span> is noted:</li>
</ul>
<p><span class="math display">\[F(x) = \int f(x) \, dx\]</span></p>
<p><span class="math inline">\(dx\)</span> being an infinitesimal interval (similar to <span class="math inline">\(h\)</span> in the definition of the derivative).</p>
<ul>
<li>There are tons of formal definitions of integrals (Riemann, Lebesgue, Darboux…) and we will not get into details here as we will not use integrals a lot.</li>
</ul>
</section>

<section id="integrals-1" class="title-slide slide level1 center">
<h1>Integrals</h1>
<ul>
<li><p>The most important to understand for now is maybe that the integral of a function is the <strong>area under the curve</strong>.</p></li>
<li><p>The area under the curve of a function <span class="math inline">\(f\)</span> on the interval <span class="math inline">\([a, b]\)</span> is:</p></li>
</ul>
<p><span class="math display">\[\mathcal{S} = \int_a^b f(x) \, dx\]</span></p>

<img data-src="img/riemann-sum1.svg" style="width:50.0%" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://www.math24.net/riemann-sums-definite-integral/" class="uri">https://www.math24.net/riemann-sums-definite-integral/</a></p></section>

<section id="integrals-2" class="title-slide slide level1 center">
<h1>Integrals</h1>
<ul>
<li><p>One way to approximate this surface is to split the interval <span class="math inline">\([a, b]\)</span> into <span class="math inline">\(n\)</span> intervals of width <span class="math inline">\(dx\)</span> with the points <span class="math inline">\(x_1, x_2, \ldots, x_n\)</span>.</p></li>
<li><p>This defines <span class="math inline">\(n\)</span> rectangles of width <span class="math inline">\(dx\)</span> and height <span class="math inline">\(f(x_i)\)</span>, so their surface is <span class="math inline">\(f(x_i) \, dx\)</span>.</p></li>
<li><p>The area under the curve can then be approximated by the sum of the surfaces of all these rectangles.</p></li>
</ul>

<img data-src="img/riemann-sum.svg" style="width:50.0%" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://www.math24.net/riemann-sums-definite-integral/" class="uri">https://www.math24.net/riemann-sums-definite-integral/</a></p></section>

<section id="integrals-3" class="title-slide slide level1 center">
<h1>Integrals</h1>
<ul>
<li>When <span class="math inline">\(n \to \infty\)</span>, or equivalently <span class="math inline">\(dx \to 0\)</span>, the sum of these rectangular areas (called the Riemann sum) becomes exactly the area under the curve. This is the definition of the definite integral:</li>
</ul>
<p><span class="math display">\[\int_a^b f(x) \, dx = \lim_{dx \to 0} \sum_{i=1}^n f(x_i) \, dx\]</span></p>
<ul>
<li>Very roughly speaking, the integral can be considered as the equivalent of a sum for continuous functions.</li>
</ul>

<img data-src="img/riemann-sum.svg" style="width:50.0%" class="r-stretch quarto-figure-center"><p class="caption">Source: <a href="https://www.math24.net/riemann-sums-definite-integral/" class="uri">https://www.math24.net/riemann-sums-definite-integral/</a></p></section>

<section id="probability-theory" class="title-slide slide level1 center">
<h1>3 - Probability theory</h1>

</section>

<section id="discrete-probability-distributions" class="title-slide slide level1 center">
<h1>Discrete probability distributions</h1>
<div class="columns">
<div class="column" style="width:30%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/dice.svg" style="width:70.0%"></p>
<p></p><figcaption>Credit: <a href="https://commons.wikimedia.org/wiki/File:2-Dice-Icon.svg" class="uri">https://commons.wikimedia.org/wiki/File:2-Dice-Icon.svg</a></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:70%;">
<ul>
<li><p>Let’s note <span class="math inline">\(X\)</span> a <strong>discrete random variable</strong> with <span class="math inline">\(n\)</span> realizations (or outcomes) <span class="math inline">\(x_1, \ldots, x_n\)</span>.</p></li>
<li><p>The <strong>probability</strong> that <span class="math inline">\(X\)</span> takes the value <span class="math inline">\(x_i\)</span> is defined by the <em>relative frequency of occurrence</em>, i.e.&nbsp;the proportion of samples having the value <span class="math inline">\(x_i\)</span>, when the total number <span class="math inline">\(N\)</span> of samples tends to infinity:</p></li>
</ul>
<p><span class="math display">\[
    P(X = x_i) = \frac{\text{Number of favorable cases}}{\text{Total number of samples}}
\]</span></p>
</div>
</div>
<ul>
<li><p>The set of probabilities <span class="math inline">\(\{P(X = x_i)\}_{i=1}^n\)</span> define the <strong>probability distribution</strong> for the random variable (or probability mass function, pmf).</p></li>
<li><p>By definition, we have <span class="math inline">\(0 \leq P(X = x_i) \leq 1\)</span> and the probabilities <strong>have</strong> to respect:</p></li>
</ul>
<p><span class="math display">\[
    \sum_{i=1}^n P(X = x_i) = 1
\]</span></p>
</section>

<section id="mathematical-expectation-and-variance" class="title-slide slide level1 center">
<h1>Mathematical expectation and variance</h1>
<ul>
<li>An important metric for a random variable is its <strong>mathematical expectation</strong> or expected value, i.e.&nbsp;its “mean” realization weighted by the probabilities:</li>
</ul>
<p><span class="math display">\[
    \mathbb{E}[X] = \sum_{i=1}^n P(X = x_i) \, x_i
\]</span></p>
<ul>
<li>The expectation does not even need to be a valid realization:</li>
</ul>
<p><span class="math display">\[
    \mathbb{E}[\text{Coin}] = \frac{1}{2} \, 0 + \frac{1}{2} \, 1 = 0.5
\]</span></p>
<p><span class="math display">\[
    \mathbb{E}[\text{Dice}] = \frac{1}{6} \, (1 + 2 + 3 + 4 + 5 + 6) = 3.5
\]</span></p>
<ul>
<li>We can also compute the mathematical expectation of <strong>functions of</strong> a random variable:</li>
</ul>
<p><span class="math display">\[
    \mathbb{E}[f(X)] = \sum_{i=1}^n P(X = x_i) \, f(x_i)
\]</span></p>
</section>

<section id="mathematical-expectation-and-variance-1" class="title-slide slide level1 center">
<h1>Mathematical expectation and variance</h1>
<ul>
<li>The <strong>variance</strong> of a random variable is the squared deviation around the mean:</li>
</ul>
<p><span class="math display">\[
    \text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \sum_{i=1}^n P(X = x_i) \, (x_i - \mathbb{E}[X])^2
\]</span></p>
<ul>
<li>Variance of a coin:</li>
</ul>
<p><span class="math display">\[
    \text{Var}(\text{Coin}) = \frac{1}{2} \, (0 - 0.5)^2 + \frac{1}{2} \, (1 - 0.5)^2 = 0.25
\]</span></p>
<ul>
<li>Variance of a dice:</li>
</ul>
<p><span class="math display">\[
    \text{Var}(\text{Dice}) = \frac{1}{6} \, ((1-3.5)^2 + (2-3.5)^2 + (3-3.5)^2 + (4-3.5)^2 + (5-3.5)^2 + (6-3.5)^2) = \frac{105}{36}
\]</span></p>
</section>

<section id="continuous-probability-distributions" class="title-slide slide level1 center">
<h1>Continuous probability distributions</h1>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/normaldistribution.png"></p>
<p></p><figcaption>Source: <a href="https://en.wikipedia.org/wiki/Normal_distribution" class="uri">https://en.wikipedia.org/wiki/Normal_distribution</a></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li><p><strong>Continuous random variables</strong> can take an infinity of continuous values, e.g.&nbsp;<span class="math inline">\(\Re\)</span> or some subset.</p></li>
<li><p>The closed set of values they can take is called the <strong>support</strong> <span class="math inline">\(\mathcal{D}_X\)</span> of the probability distribution.</p></li>
<li><p>The probability distribution is described by a <strong>probability density function</strong> (pdf) <span class="math inline">\(f(x)\)</span>.</p></li>
<li><p>The pdf of a distribution must be positive (<span class="math inline">\(f(x) \geq 0 \, \forall x \in \mathcal{D}_X\)</span>) and its integral must be equal to 1:</p></li>
</ul>
<p><span class="math display">\[
    \int_{x \in \mathcal{D}_X} f(x) \, dx = 1
\]</span></p>
</div>
</div>
<ul>
<li>The pdf does not give the probability of taking a particular value <span class="math inline">\(x\)</span> (it is 0), but allows to get the probability that a value lies in a specific interval:</li>
</ul>
<p><span class="math display">\[
    P(a \leq X \leq b) = \int_{a}^b f(x) \, dx
\]</span></p>
<ul>
<li>One can however think of the pdf as the <strong>likelihood</strong> that a value <span class="math inline">\(x\)</span> comes from that distribution.</li>
</ul>
</section>

<section id="expectation-and-variance-of-continuous-distributions" class="title-slide slide level1 center">
<h1>Expectation and variance of continuous distributions</h1>
<ul>
<li>The mathematical expectation is now defined by an integral instead of a sum:</li>
</ul>
<p><span class="math display">\[
    \mathbb{E}[X] = \int_{x \in \mathcal{D}_X} f(x) \, x \, dx
\]</span></p>
<p>the variance:</p>
<p><span class="math display">\[
    \text{Var}(X) = \int_{x \in \mathcal{D}_X} f(x) \, (x - \mathbb{E}[X])^2 \, dx
\]</span></p>
<p>or a function of the random variable:</p>
<p><span class="math display">\[
    \mathbb{E}[g(X)] = \int_{x \in \mathcal{D}_X} f(x) \, g(x) \, dx
\]</span></p>
<ul>
<li>Note that the expectation operator is <strong>linear</strong>:</li>
</ul>
<p><span class="math display">\[
    \mathbb{E}[a \, X + b \, Y] = a \, \mathbb{E}[X] + b \, \mathbb{E}[Y]
\]</span></p>
</section>

<section id="some-parameterized-probability-distributions" class="title-slide slide level1 center">
<h1>Some parameterized probability distributions</h1>
<ul>
<li><p>Probability distributions can in principle have any form: <span class="math inline">\(f(x)\)</span> is unknown.</p></li>
<li><p>However, specific parameterized distributions can be very useful: their pmf/pdf is fully determined by a couple of parameters.</p></li>
<li><p>The <strong>Bernouilli</strong> distribution is a binary (discrete, 0 or 1) distribution with a parameter <span class="math inline">\(p\)</span> specifying the probability to obtain the outcome 1:</p></li>
</ul>
<p><span class="math display">\[
    P(X = 1) = p \; \text{and} \; P(X=0) = 1 - p
\]</span> <span class="math display">\[P(X=x) = p^x \, (1-p)^{1-x}\]</span> <span class="math display">\[\mathbb{E}[X] = p\]</span></p>
<ul>
<li>The <strong>Multinouilli</strong> or <strong>categorical</strong> distribution is a discrete distribution with <span class="math inline">\(k\)</span> realizations. Each realization <span class="math inline">\(x_i\)</span> is associated with a parameter <span class="math inline">\(p_i &gt;0\)</span> representing its probability. We have <span class="math inline">\(\sum_i p_i = 1\)</span>.</li>
</ul>
<p><span class="math display">\[P(X = x_i) = p_i\]</span></p>
<ul>
<li>Knowing <span class="math inline">\(p\)</span> or the <span class="math inline">\(p_i\)</span> tells us everything about the discrete distributions.</li>
</ul>
</section>

<section id="the-uniform-distribution" class="title-slide slide level1 center">
<h1>The uniform distribution</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/uniformdistribution.png"></p>
<p></p><figcaption>Credit: <a href="https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)" class="uri">https://en.wikipedia.org/wiki/Uniform_distribution_(continuous)</a></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>The <strong>uniform distribution</strong> has an equal and constant probability of returning values between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, never outside this range.</p></li>
<li><p>It is parameterized by two parameters:</p>
<ul>
<li><p>the start of the range <span class="math inline">\(a\)</span>.</p></li>
<li><p>the end of the range <span class="math inline">\(b\)</span>.</p></li>
</ul></li>
<li><p>Its support is <span class="math inline">\([a, b]\)</span>.</p></li>
</ul>
</div>
</div>
<ul>
<li>The pdf of the uniform distribution <span class="math inline">\(\mathcal{U}(a, b)\)</span> is defined on <span class="math inline">\([a, b]\)</span> as:</li>
</ul>
<p><span class="math display">\[
    f(x; a, b) = \frac{1}{b - a}
\]</span></p>
<ul>
<li>Knowing <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> completely defines the distribution.</li>
</ul>
</section>

<section id="the-normal-or-gaussian-distribution" class="title-slide slide level1 center">
<h1>The normal or Gaussian distribution</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/normaldistribution.png"></p>
<p></p><figcaption>Credit: <a href="https://en.wikipedia.org/wiki/Normal_distribution" class="uri">https://en.wikipedia.org/wiki/Normal_distribution</a></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>For continuous distributions, the <strong>normal distribution</strong> is the most frequently encountered one.</p></li>
<li><p>It is parameterized by two parameters:</p>
<ul>
<li><p>the mean <span class="math inline">\(\mu\)</span>.</p></li>
<li><p>the variance <span class="math inline">\(\sigma^2\)</span> (or standard deviation <span class="math inline">\(\sigma\)</span>).</p></li>
</ul></li>
<li><p>Its support is <span class="math inline">\(\Re\)</span>.</p></li>
</ul>
</div>
</div>
<ul>
<li>The pdf of the normal distribution <span class="math inline">\(\mathcal{N}(\mu, \sigma)\)</span> is defined on <span class="math inline">\(\Re\)</span> as:</li>
</ul>
<p><span class="math display">\[
    f(x; \mu, \sigma) = \frac{1}{\sqrt{2\,\pi\,\sigma^2}} \, e^{-\displaystyle\frac{(x - \mu)^2}{2\,\sigma^2}}
\]</span></p>
<ul>
<li>Knowing <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> completely defines the distribution.</li>
</ul>
</section>

<section id="the-exponential-distribution" class="title-slide slide level1 center">
<h1>The exponential distribution</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/exponentialdistribution.png"></p>
<p></p><figcaption>Credit: <a href="https://en.wikipedia.org/wiki/Exponential_distribution" class="uri">https://en.wikipedia.org/wiki/Exponential_distribution</a></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>The <strong>exponential distribution</strong> is the probability distribution of the time between events in a Poisson point process, i.e., a process in which events occur continuously and independently at a constant average rate.</p></li>
<li><p>It is parameterized by one parameter:</p>
<ul>
<li>the rate <span class="math inline">\(\lambda\)</span>.</li>
</ul></li>
<li><p>Its support is <span class="math inline">\(\Re^+\)</span> (<span class="math inline">\(x &gt; 0\)</span>).</p></li>
</ul>
</div>
</div>
<ul>
<li>The pdf of the exponential distribution is defined on <span class="math inline">\(\Re^+\)</span> as:</li>
</ul>
<p><span class="math display">\[
    f(x; \lambda) = \lambda \, e^{-\lambda \, x}
\]</span></p>
<ul>
<li>Knowing <span class="math inline">\(\lambda\)</span> completely defines the distribution.</li>
</ul>
</section>

<section id="joint-probabilities" class="title-slide slide level1 center">
<h1>Joint probabilities</h1>
<ul>
<li><p>Let’s now suppose that we have two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> with different probability distributions <span class="math inline">\(P(X)\)</span> and <span class="math inline">\(P(Y)\)</span>.</p></li>
<li><p>The <strong>joint probability</strong> <span class="math inline">\(P(X, Y)\)</span> denotes the probability of observing the realizations <span class="math inline">\(x\)</span> <strong>and</strong> <span class="math inline">\(y\)</span> at the same time:</p></li>
</ul>
<p><span class="math display">\[P(X=x, Y=y)\]</span></p>
<ul>
<li>If the random variables are <strong>independent</strong>, we have:</li>
</ul>
<p><span class="math display">\[P(X=x, Y=y) = P(X=x) \, P(Y=y)\]</span></p>
<ul>
<li>If you know the joint probability, you can compute the <strong>marginal probability distribution</strong> of each variable:</li>
</ul>
<p><span class="math display">\[P(X=x) = \sum_y P(X=x, Y=y)\]</span></p>
<ul>
<li>The same is true for continuous probability distributions:</li>
</ul>
<p><span class="math display">\[
    f(x) = \int f(x, y) \, dy
\]</span></p>
</section>

<section id="conditional-probabilities" class="title-slide slide level1 center">
<h1>Conditional probabilities</h1>
<ul>
<li><p>Some useful information between two random variables is the <strong>conditional probability</strong>.</p></li>
<li><p><span class="math inline">\(P(X=x | Y=y)\)</span> is the conditional probability that <span class="math inline">\(X=x\)</span>, <strong>given</strong> that <span class="math inline">\(Y=y\)</span> is observed.</p></li>
<li><p><span class="math inline">\(Y=y\)</span> is not random anymore: it is a <strong>fact</strong> (at least theoretically).</p></li>
<li><p>You wonder what happens to the probability distribution of <span class="math inline">\(X\)</span> now that you know the value of <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Conditional probabilities are linked to the joint probability by:</p></li>
</ul>
<p><span class="math display">\[
    P(X=x | Y=y) = \frac{P(X=x, Y=y)}{P(Y=y)}
\]</span></p>
<ul>
<li><p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>independent</strong>, we have <span class="math inline">\(P(X=x | Y=y) = P(X=x)\)</span> (knowing <span class="math inline">\(Y\)</span> does not change anything to the probability distribution of <span class="math inline">\(X\)</span>).</p></li>
<li><p>We can use the same notation for the complete probability distributions:</p></li>
</ul>
<p><span class="math display">\[
    P(X | Y) = \frac{P(X, Y)}{P(Y)}
\]</span></p>
</section>

<section id="joint-and-conditional-probabilities-using-a-venn-diagram" class="title-slide slide level1 center">
<h1>Joint and conditional probabilities: using a Venn diagram</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/conditionalprobability.png"></p>
<p></p><figcaption>Credit: <a href="https://www.elevise.co.uk/g-e-m-h-5-u.html" class="uri">https://www.elevise.co.uk/g-e-m-h-5-u.html</a></figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>You ask 50 people whether they like cats or dogs:</p>
<ul>
<li>18 like both cats and dogs.</li>
<li>21 like only dogs.</li>
<li>5 like only cats.</li>
<li>6 like none of them.</li>
</ul></li>
<li><p>We consider loving cats and dogs as random variables (and that our sample size is big enough to use probabilities…)</p></li>
<li><p>We have <span class="math inline">\(P(\text{dog}) = \frac{18+21}{50}\)</span> and <span class="math inline">\(P(\text{cat}) = \frac{18+5}{50}\)</span>.</p></li>
<li><p>Among the 23 who love cats, which proportion also loves dogs?</p></li>
</ul>
</div>
</div>
<ul>
<li><p>The joint probability of loving both cats and dogs is <span class="math inline">\(P(\text{cat}, \text{dog}) = \frac{18}{50}\)</span>.</p></li>
<li><p>The conditional probability of loving dogs given one loves cats is:</p></li>
</ul>
<p><span class="math display">\[P(\text{dog} | \text{cat}) = \frac{P(\text{cat}, \text{dog})}{P(\text{cat})} = \frac{\frac{18}{50}}{\frac{23}{50}} = \frac{18}{23}\]</span></p>
</section>

<section id="bayes-rule" class="title-slide slide level1 center">
<h1>Bayes’ rule</h1>
<ul>
<li>Noticing that the definition of conditional probabilities is symmetric:</li>
</ul>
<p><span class="math display">\[
    P(X, Y) = P(X | Y) \, P(Y) = P(Y | X) \, P(X)
\]</span></p>
<p>we can obtain the <strong>Bayes’ rule</strong>:</p>
<p><span class="math display">\[
    P(Y | X) = \frac{P(X|Y) \, P(Y)}{P(X)}
\]</span></p>
<ul>
<li><p>It is very useful when you already know <span class="math inline">\(P(X|Y)\)</span> and want to obtain <span class="math inline">\(P(Y|X)\)</span> (<strong>Bayesian inference</strong>).</p>
<ul>
<li><p><span class="math inline">\(P(Y | X)\)</span> is called the <strong>posterior probability</strong>.</p></li>
<li><p><span class="math inline">\(P(X | Y)\)</span> is called the <strong>likelihood</strong>.</p></li>
<li><p><span class="math inline">\(P(Y)\)</span> is called the <strong>prior probability</strong> (belief).</p></li>
<li><p><span class="math inline">\(P(X)\)</span> is called the <strong>model evidence</strong> or <strong>marginal likelihood</strong>.</p></li>
</ul></li>
</ul>
</section>

<section id="bayes-rule-example" class="title-slide slide level1 center">
<h1>Bayes’ rule : example</h1>
<ul>
<li>Let’s consider a disease <span class="math inline">\(D\)</span> (binary random variable) and a medical test <span class="math inline">\(T\)</span> (also binary). The disease affects 10% of the general population:</li>
</ul>
<p><span class="math display">\[P(D=1)= 0.1 \qquad \qquad P(D=0)=0.9\]</span></p>
<ul>
<li>When a patient has the disease, the test is positive 80% of the time:</li>
</ul>
<p><span class="math display">\[P(T=1 | D=1) = 0.8 \qquad \qquad P(T=0 | D=1) = 0.2\]</span></p>
<ul>
<li>When a patient does not have the disease, the test is still positive 10% of the time:</li>
</ul>
<p><span class="math display">\[P(T=1 | D=0) = 0.1 \qquad \qquad P(T=0 | D=0) = 0.9\]</span></p>
<ul>
<li>Given that the test is positive, what is the probability that the patient is ill?</li>
</ul>
</section>

<section id="bayes-rule-example-1" class="title-slide slide level1 center">
<h1>Bayes’ rule : example</h1>
<p><span class="math display">\[
\begin{aligned}
    P(D=1|T=1) &amp;= \frac{P(T=1 | D=1) \, P(D=1)}{P(T=1)} \\
               &amp;\\
               &amp;= \frac{P(T=1 | D=1) \, P(D=1)}{P(T=1 | D=1) \, P(D=1) + P(T=1 | D=0) \, P(D=0)} \\
               &amp;\\
               &amp;= \frac{0.8 \times 0.1}{0.8 \times 0.1 + 0.1 \times 0.9} \\
               &amp;\\
               &amp; = 0.47 \\
\end{aligned}
\]</span></p>
</section>

<section id="statistics" class="title-slide slide level1 center">
<h1>4 - Statistics</h1>

</section>

<section id="random-sampling-monte-carlo-sampling" class="title-slide slide level1 center">
<h1>Random sampling / Monte Carlo sampling</h1>
<ul>
<li><p>In ML, we will deal with random variables whose exact probability distribution is unknown, but we are interested in their expectation or variance anyway.</p></li>
<li><p><strong>Random sampling</strong> or <strong>Monte Carlo sampling</strong> (MC) consists of taking <span class="math inline">\(N\)</span> samples <span class="math inline">\(x_i\)</span> out of the distribution <span class="math inline">\(X\)</span> (discrete or continuous) and computing the <strong>sample average</strong>: <span class="math display">\[
  \mathbb{E}[X] = \mathbb{E}_{x \sim X} [x] \approx \frac{1}{N} \, \sum_{i=1}^N x_i
\]</span></p></li>
</ul>

<img data-src="img/normaldistribution.svg" style="width:50.0%" class="r-stretch quarto-figure-center"><ul>
<li>More samples will be obtained where <span class="math inline">\(f(x)\)</span> is high (<span class="math inline">\(x\)</span> is probable), so the average of the sampled data will be close to the expected value of the distribution.</li>
</ul>
</section>

<section id="random-sampling-monte-carlo-sampling-1" class="title-slide slide level1 center">
<h1>Random sampling / Monte Carlo sampling</h1>
<p><strong>Law of big numbers</strong></p>
<blockquote>
<p>As the number of identically distributed, randomly generated variables increases, their sample mean (average) approaches their theoretical mean.</p>
</blockquote>
<p>MC estimates are only correct when:</p>
<ul>
<li><p>the samples are <strong>i.i.d</strong> (independent and identically distributed):</p>
<ul>
<li><p>independent: the samples must be unrelated with each other.</p></li>
<li><p>identically distributed: the samples must come from the same distribution <span class="math inline">\(X\)</span>.</p></li>
</ul></li>
<li><p>the number of samples is large enough. Usually <span class="math inline">\(N &gt; 30\)</span> for simple distributions.</p></li>
</ul>
</section>

<section id="random-sampling-monte-carlo-sampling-2" class="title-slide slide level1 center">
<h1>Random sampling / Monte Carlo sampling</h1>
<ul>
<li>One can estimate any function of the random variable with random sampling:</li>
</ul>
<p><span class="math display">\[
    \mathbb{E}[f(X)] = \mathbb{E}_{x \sim X} [f(x)] \approx \frac{1}{N} \, \sum_{i=1}^N f(x_i)
\]</span></p>
<ul>
<li>Example of Monte Carlo sampling to estimate <span class="math inline">\(\pi/4\)</span>:</li>
</ul>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/montecarlo-1.jpeg"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/montecarlo-2.jpeg"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/montecarlo-3.jpeg"></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 25.0%;justify-content: center;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/montecarlo-4.jpeg"></p>
</figure>
</div>
</div>
</div>
</div>
<div class="footer">
<p>Credit <a href="https://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694" class="uri">https://towardsdatascience.com/an-overview-of-monte-carlo-methods-675384eb1694</a></p>
</div>
</section>

<section id="central-limit-theorem" class="title-slide slide level1 center">
<h1>Central limit theorem</h1>
<ul>
<li><p>Suppose we have an unknown distribution <span class="math inline">\(X\)</span> with expected value <span class="math inline">\(\mu = \mathbb{E}[X]\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.</p></li>
<li><p>We can take randomly <span class="math inline">\(N\)</span> samples from <span class="math inline">\(X\)</span> to compute the sample average:</p></li>
</ul>
<p><span class="math display">\[
    S_N = \frac{1}{N} \, \sum_{i=1}^N x_i
\]</span></p>
<ul>
<li>The <strong>Central Limit Theorem</strong> (CLT) states that:</li>
</ul>
<blockquote>
<p>The distribution of sample averages is normally distributed with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\frac{\sigma^2}{N}\)</span>.</p>
</blockquote>
<p><span class="math display">\[S_N \sim \mathcal{N}(\mu, \frac{\sigma}{\sqrt{N}})\]</span></p>
</section>

<section id="central-limit-theorem-1" class="title-slide slide level1 center">
<h1>Central limit theorem</h1>
<ul>
<li><p>If we perform the sampling multiple times, even with few samples, the average of the sampling averages will be very close to the expected value.</p></li>
<li><p>The more samples we get, the smaller the variance of the estimates.</p></li>
<li><p>Although the distribution <span class="math inline">\(X\)</span> can be anything, the sampling averages are normally distributed.</p></li>
</ul>

<img data-src="img/IllustrationCentralTheorem.png" style="width:70.0%" class="r-stretch quarto-figure-center"><p class="caption">Credit: <a href="https://en.wikipedia.org/wiki/Central_limit_theorem" class="uri">https://en.wikipedia.org/wiki/Central_limit_theorem</a></p></section>

<section id="estimators" class="title-slide slide level1 center">
<h1>Estimators</h1>
<ul>
<li>CLT shows that the sampling average is an <strong>unbiased estimator</strong> of the expected value of a distribution:</li>
</ul>
<p><span class="math display">\[\mathbb{E}(S_N) = \mathbb{E}(X)\]</span></p>
<ul>
<li><p>An estimator is a random variable used to measure parameters of a distribution (e.g.&nbsp;its expectation). The problem is that estimators can generally be <strong>biased</strong>.</p></li>
<li><p>Take the example of a thermometer <span class="math inline">\(M\)</span> measuring the temperature <span class="math inline">\(T\)</span>. <span class="math inline">\(T\)</span> is a random variable (normally distributed with <span class="math inline">\(\mu=20\)</span> and <span class="math inline">\(\sigma=10\)</span>) and the measurements <span class="math inline">\(M\)</span> relate to the temperature with the relation:</p></li>
</ul>
<p><span class="math display">\[
    M = 0.95 \, T + 0.65
\]</span></p>

<img data-src="img/estimators-temperature.png" style="width:70.0%" class="r-stretch quarto-figure-center"></section>

<section id="estimators-1" class="title-slide slide level1 center">
<h1>Estimators</h1>
<ul>
<li><p>The thermometer is not perfect, but do random measurements allow us to estimate the expected value of the temperature?</p></li>
<li><p>We could repeatedly take 100 random samples of the thermometer and see how the distribution of sample averages look like:</p></li>
</ul>

<img data-src="img/estimators-temperature2.png" style="width:40.0%" class="r-stretch quarto-figure-center"><ul>
<li>But, as the expectation is linear, we actually have:</li>
</ul>
<p><span class="math display">\[
    \mathbb{E}[M] = \mathbb{E}[0.95 \, T + 0.65] = 0.95 \, \mathbb{E}[T] + 0.65 = 19.65 \neq \mathbb{E}[T]
\]</span></p>
<ul>
<li>The thermometer is a <strong>biased estimator</strong> of the temperature.</li>
</ul>
</section>

<section id="estimators-2" class="title-slide slide level1 center">
<h1>Estimators</h1>
<ul>
<li><p>Let’s note <span class="math inline">\(\theta\)</span> a parameter of a probability distribution <span class="math inline">\(X\)</span> that we want to estimate (it does not have to be its mean).</p></li>
<li><p>An <strong>estimator</strong> <span class="math inline">\(\hat{\theta}\)</span> is a random variable mapping the sample space of <span class="math inline">\(X\)</span> to a set of sample estimates.</p></li>
<li><p>The <strong>bias</strong> of an estimator is the mean error made by the estimator:</p></li>
</ul>
<p><span class="math display">\[
    \mathcal{B}(\hat{\theta}) = \mathbb{E}[\hat{\theta} - \theta] = \mathbb{E}[\hat{\theta}] - \theta
\]</span></p>
<ul>
<li>The <strong>variance</strong> of an estimator is the deviation of the samples around the expected value:</li>
</ul>
<p><span class="math display">\[
    \text{Var}(\hat{\theta}) = \mathbb{E}[(\hat{\theta} - \mathbb{E}[\hat{\theta}] )^2]
\]</span></p>
<ul>
<li><p>Ideally, we would like estimators with:</p>
<ul>
<li><p><strong>low bias</strong>: the estimations are correct on average (= equal to the true parameter).</p></li>
<li><p><strong>low variance</strong>: we do not need many estimates to get a correct estimate (CLT: <span class="math inline">\(\frac{\sigma}{\sqrt{N}}\)</span>)</p></li>
</ul></li>
</ul>
</section>

<section id="estimators-bias-and-variance" class="title-slide slide level1 center">
<h1>Estimators: bias and variance</h1>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/biasvariance3.png"></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<ul>
<li><p>Unfortunately, the perfect estimator does not exist.</p></li>
<li><p>Estimators will have a bias and a variance:</p>
<ul>
<li><p><strong>Bias</strong>: the estimated values will be wrong, and the policy not optimal.</p></li>
<li><p><strong>Variance</strong>: we will need a lot of samples (trial and error) to have correct estimates.</p></li>
</ul></li>
<li><p>One usually talks of a <strong>bias/variance</strong> trade-off: if you have a small bias, you will have a high variance, or vice versa.</p></li>
<li><p>In machine learning, bias corresponds to underfitting, variance to overfitting.</p></li>
</ul>
</div>
</div>
</section>

<section id="information-theory" class="title-slide slide level1 center">
<h1>5 - Information theory</h1>

</section>

<section id="information" class="title-slide slide level1 center">
<h1>Information</h1>
<ul>
<li><p><strong>Information theory</strong> (Claude Shannon) asks how much information is contained in a probability distribution.</p></li>
<li><p>Information is related to <strong>surprise</strong> or <strong>uncertainty</strong>: are the outcomes of a random variable surprising?</p>
<ul>
<li><p>Almost certain outcomes (<span class="math inline">\(P \sim 1\)</span>) are not surprising because they happen all the time.</p></li>
<li><p>Almost impossible outcomes (<span class="math inline">\(P \sim 0\)</span>) are very surprising because they are very rare.</p></li>
</ul></li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/selfinformation.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>A useful measurement of how surprising is an outcome <span class="math inline">\(x\)</span> is the <strong>self-information</strong>:</li>
</ul>
<p><span class="math display">\[
    I (x) = - \log P(X = x)
\]</span></p>
<ul>
<li><p>Depending on which log is used, self-information has different units:</p>
<ul>
<li><p><span class="math inline">\(\log_2\)</span>: bits or shannons.</p></li>
<li><p><span class="math inline">\(\log_e = \ln\)</span>: nats.</p></li>
</ul></li>
<li><p>But it is just a rescaling, the base never matters.</p></li>
</ul>
</div>
</div>
</section>

<section id="entropy" class="title-slide slide level1 center">
<h1>Entropy</h1>
<ul>
<li>The <strong>entropy</strong> (or Shannon entropy) of a probability distribution is the expected value of the self-information of its outcomes:</li>
</ul>
<p><span class="math display">\[
    H(X) = \mathbb{E}_{x \sim X} [I(x)] = \mathbb{E}_{x \sim X} [- \log P(X = x)]
\]</span></p>
<ul>
<li>It measures the <strong>uncertainty</strong>, <strong>randomness</strong> or <strong>information content</strong> of the random variable.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>In the discrete case:</li>
</ul>
<p><span class="math display">\[
    H(X) = - \sum_x P(x) \, \log P(x)
\]</span></p>
<ul>
<li>In the continuous case:</li>
</ul>
<p><span class="math display">\[
    H(X) = - \int_x f(x) \, \log f(x) \, dx
\]</span></p>
<ul>
<li><p>The entropy of a Bernouilli variable is maximal when both outcomes are <strong>equiprobable</strong>.</p></li>
<li><p>If a variable is <strong>deterministic</strong>, its entropy is minimal and equal to zero.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/entropy-binomial.png"></p>
</figure>
</div>
</div>
</div>
</section>

<section id="joint-and-conditional-entropies" class="title-slide slide level1 center">
<h1>Joint and conditional entropies</h1>
<ul>
<li>The <strong>joint entropy</strong> of two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined by:</li>
</ul>
<p><span class="math display">\[
    H(X, Y) = \mathbb{E}_{x \sim X, y \sim Y} [- \log P(X=x, Y=y)]
\]</span></p>
<ul>
<li>The <strong>conditional entropy</strong> of two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined by:</li>
</ul>
<p><span class="math display">\[
    H(X | Y) = \mathbb{E}_{x \sim X, y \sim Y} [- \log P(X=x | Y=y)]  = \mathbb{E}_{x \sim X, y \sim Y} [- \log \frac{P(X=x , Y=y)}{P(Y=y)}]
\]</span></p>
<ul>
<li>If the variables are <strong>independent</strong>, we have:</li>
</ul>
<p><span class="math display">\[
    H(X, Y) = H(X) + H(Y) \qquad \text{or} \qquad H(X | Y) = H(X)
\]</span></p>
<ul>
<li>Both are related by:</li>
</ul>
<p><span class="math display">\[
    H(X | Y) = H(X, Y) - H(Y)
\]</span></p>
<ul>
<li>The equivalent of Bayes’ rule is:</li>
</ul>
<p><span class="math display">\[
    H(Y |X) = H(X |Y) + H(Y) - H(X)
\]</span></p>
</section>

<section id="mutual-information" class="title-slide slide level1 center">
<h1>Mutual Information</h1>
<ul>
<li>The most important information measurement between two variables is the <strong>mutual information</strong> MI (or information gain):</li>
</ul>
<p><span class="math display">\[
    I(X, Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)
\]</span></p>
<ul>
<li><p>It measures how much information the variable <span class="math inline">\(X\)</span> holds on <span class="math inline">\(Y\)</span>:</p>
<ul>
<li>If the two variables are <strong>independent</strong>, the MI is 0 : <span class="math inline">\(X\)</span> is as random, whether you know <span class="math inline">\(Y\)</span> or not.</li>
</ul>
<p><span class="math display">\[
      I (X, Y) = 0
  \]</span></p>
<ul>
<li>If the two variables are <strong>dependent</strong>, knowing <span class="math inline">\(Y\)</span> gives you information on <span class="math inline">\(X\)</span>, which becomes less random, i.e.&nbsp;less uncertain / surprising.</li>
</ul>
<p><span class="math display">\[
      I (X, Y) &gt; 0
  \]</span></p></li>
<li><p>If you can fully predict <span class="math inline">\(X\)</span> when you know <span class="math inline">\(Y\)</span>, it becomes deterministic (<span class="math inline">\(H(X|Y)=0\)</span>) so the mutual information is maximal (<span class="math inline">\(I(X, Y) = H(X)\)</span>).</p></li>
</ul>
</section>

<section id="cross-entropy" class="title-slide slide level1 center">
<h1>Cross-entropy</h1>
<ul>
<li>The <strong>cross-entropy</strong> between two distributions <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is defined as:</li>
</ul>
<p><span class="math display">\[
    H(X, Y) = \mathbb{E}_{x \sim X}[- \log P(Y=x)]
\]</span></p>
<ul>
<li><p>Beware that the notation <span class="math inline">\(H(X, Y)\)</span> is the same as the joint entropy, but it is a different concept!</p></li>
<li><p>The cross-entropy measures the <strong>negative log-likelihood</strong> that a sample <span class="math inline">\(x\)</span> taken from the distribution <span class="math inline">\(X\)</span> could also come from the distribution <span class="math inline">\(Y\)</span>.</p></li>
<li><p>More exactly, it measures how many bits of information one would need to distinguish the two distributions <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p></li>
</ul>

<img data-src="img/crossentropy.svg" style="width:80.0%" class="r-stretch quarto-figure-center"></section>

<section id="cross-entropy-1" class="title-slide slide level1 center">
<h1>Cross-entropy</h1>

<img data-src="img/crossentropy.svg" style="width:80.0%" class="r-stretch quarto-figure-center"><p><span class="math display">\[
    H(X, Y) = \mathbb{E}_{x \sim X}[- \log P(Y=x)]
\]</span></p>
<ul>
<li><p>If the two distributions are the same <em>almost anywhere</em>, one cannot distinguish samples from the two distributions:</p>
<ul>
<li>The cross-entropy is the same as the entropy of <span class="math inline">\(X\)</span>.</li>
</ul></li>
<li><p>If the two distributions are completely different, one can tell whether a sample <span class="math inline">\(Z\)</span> comes from <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span>:</p>
<ul>
<li>The cross-entropy is higher than the entropy of <span class="math inline">\(X\)</span>.</li>
</ul></li>
</ul>
</section>

<section id="kullback-leibler-divergence" class="title-slide slide level1 center">
<h1>Kullback-Leibler divergence</h1>
<ul>
<li>In practice, the <strong>Kullback-Leibler divergence</strong> <span class="math inline">\(\text{KL}(X ||Y)\)</span> is a better measurement of the similarity (statistical distance) between two probability distributions:</li>
</ul>
<p><span class="math display">\[
    \text{KL}(X ||Y) = \mathbb{E}_{x \sim X}[- \log \frac{P(Y=x)}{P(X=x)}]
\]</span></p>
<ul>
<li>It is linked to the cross-entropy by:</li>
</ul>
<p><span class="math display">\[
    \text{KL}(X ||Y) = H(X, Y) - H(X)
\]</span></p>
<ul>
<li><p>If the two distributions are the same <em>almost anywhere</em>:</p>
<ul>
<li>The KL divergence is zero.</li>
</ul></li>
<li><p>If the two distributions are different:</p>
<ul>
<li>The KL divergence is positive.</li>
</ul></li>
<li><p>Minimizing the KL between two distributions is the same as making the two distributions “equal”.</p></li>
<li><p>Again, the KL is not a metric, as it is not symmetric.</p></li>
</ul>
<div class="footer footer-default">

</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="1.2-Basics_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="1.2-Basics_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="1.2-Basics_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="1.2-Basics_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="1.2-Basics_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="1.2-Basics_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="1.2-Basics_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="1.2-Basics_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="1.2-Basics_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="1.2-Basics_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="1.2-Basics_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        setTimeout(function() {
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const cites = ref.parentNode.getAttribute('data-cites').split(' ');
        tippyHover(ref, function() {
          var popup = window.document.createElement('div');
          cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    });
    </script>
    

</body></html>