<!DOCTYPE html>
<html lang="en"><head>
<script src="2.2-LinearRegression_files/libs/clipboard/clipboard.min.js"></script>
<script src="2.2-LinearRegression_files/libs/quarto-html/tabby.min.js"></script>
<script src="2.2-LinearRegression_files/libs/quarto-html/popper.min.js"></script>
<script src="2.2-LinearRegression_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="2.2-LinearRegression_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="2.2-LinearRegression_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="2.2-LinearRegression_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.1.251">

  <meta name="author" content="Julien Vitay">
  <title>Neurocomputing</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="2.2-LinearRegression_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="2.2-LinearRegression_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="2.2-LinearRegression_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">
  <link href="2.2-LinearRegression_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="2.2-LinearRegression_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="2.2-LinearRegression_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="2.2-LinearRegression_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="2.2-LinearRegression_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="2.2-LinearRegression_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="img/tuc-new-large.png" data-background-opacity="1" data-background-position="top" data-background-size="30%" class="center">
  <h1 class="title">Neurocomputing</h1>
  <p class="subtitle">Linear regression</p>
  <p class="author">Julien Vitay</p>
  <p class="institute">Professur für Künstliche Intelligenz - Fakultät für Informatik</p>
  <p class="date"><a href="https://tu-chemnitz.de/informatik/KI/edu/neurocomputing" class="uri">https://tu-chemnitz.de/informatik/KI/edu/neurocomputing</a></p>
</section>

<section id="linear-regression" class="title-slide slide level1 center">
<h1>1 - Linear regression</h1>

</section>

<section id="linear-regression-1" class="title-slide slide level1 center">
<h1>Linear regression</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/regression-animation2.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>We have a training set of N examples <span class="math inline">\mathcal{D} = (x_i, t_i)_{i=1..N}</span>.</p></li>
<li><p>In <strong>linear regression</strong>, we want to learn a linear model (hypothesis) <span class="math inline">y</span> that is linearly dependent on the input <span class="math inline">x</span>:</p></li>
</ul>
<p><span class="math display">
    y = f_{w, b}(x) = w \, x + b
</span></p>
<ul>
<li><p>The <strong>free parameters</strong> of the model are</p>
<ul>
<li><p>the slope <span class="math inline">w</span>,</p></li>
<li><p>the intercept <span class="math inline">b</span>.</p></li>
</ul></li>
<li><p>The data <span class="math inline">\mathcal{D} = (x_i, t_i)_{i=1..N}</span> is given (fixed).</p></li>
</ul>
</div>
</div>
</section>

<section id="linear-regression-2" class="title-slide slide level1 center">
<h1>Linear regression</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/artificialneuron.svg"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>Mathematical model:</li>
</ul>
<p><span class="math display">
    y = f_{w, b}(x) = w \, x + b
</span></p>
<ul>
<li><p>This corresponds to a single artificial neuron <span class="math inline">y</span> with:</p>
<ul>
<li><p>one input <span class="math inline">x</span>,</p></li>
<li><p>one weight <span class="math inline">w</span>,</p></li>
<li><p>one bias <span class="math inline">b</span>,</p></li>
<li><p>a <strong>linear</strong> activation function.</p></li>
</ul></li>
<li><p>We will see that this generalizes to multiple inputs and outputs.</p></li>
</ul>
</div>
</div>
</section>

<section id="linear-regression-3" class="title-slide slide level1 center">
<h1>Linear regression</h1>
<ul>
<li><p>The goal of the linear regression (or least mean squares - LMS) is to minimize the <strong>mean square error</strong> (mse) between the targets and the predictions.</p></li>
<li><p>It is defined as the mathematical expectation of the quadratic error over the training data:</p></li>
</ul>
<p><span class="math display">
    \mathcal{L}(w, b) =  \mathbb{E}_{x_i, t_i \in \mathcal{D}} [ (t_i - y_i )^2 ]
</span></p>

<img data-src="img/regression-animation-mse-dual.png" style="width:100.0%" class="r-stretch quarto-figure-center"></section>

<section id="linear-regression-4" class="title-slide slide level1 center">
<h1>Linear regression</h1>
<ul>
<li>As the training set is finite and the samples i.i.d (independent and identically distributed), we can simply replace the expectation by a sampling average:</li>
</ul>
<p><span class="math display">
    \mathcal{L}(w, b) = \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )^2
</span></p>

<img data-src="img/regression-animation-mse-dual.png" style="width:100.0%" class="r-stretch quarto-figure-center"></section>

<section id="linear-regression-5" class="title-slide slide level1 center">
<h1>Linear regression</h1>
<ul>
<li><p>The minimum of the mse is achieved when the <strong>prediction</strong> <span class="math inline">y_i = f_{w, b}(x_i)</span> is equal to the <strong>ground truth</strong> <span class="math inline">t_i</span> for all training examples.</p></li>
<li><p>In other words, we want to minimize the <strong>residual error</strong> of the model on the data.</p></li>
<li><p>It is not always possible to obtain the global minimum (0) but the closer, the better.</p></li>
</ul>

<img data-src="img/regression-animation-mse-dual.png" style="width:100.0%" class="r-stretch quarto-figure-center"></section>

<section id="gradient-descent-for-linear-regression" class="title-slide slide level1 center">
<h1>Gradient descent for linear regression</h1>
<ul>
<li>We search for <span class="math inline">w</span> and <span class="math inline">b</span> which minimize the mean square error:</li>
</ul>
<p><span class="math display">
    \mathcal{L}(w, b) = \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )^2
</span></p>
<ul>
<li>We will apply <em>gradient descent</em> to iteratively modify estimates of <span class="math inline">w</span> and <span class="math inline">b</span>:</li>
</ul>
<p><span class="math display">
    \Delta w = - \eta \, \frac{\partial \mathcal{L}(w, b)}{\partial w}
</span></p>
<p><span class="math display">
    \Delta b = - \eta \, \frac{\partial \mathcal{L}(w, b)}{\partial b}
</span></p>
</section>

<section id="gradient-descent-for-linear-regression-1" class="title-slide slide level1 center">
<h1>Gradient descent for linear regression</h1>
<ul>
<li>Let’s search for the partial derivative (gradient) of the quadratic error with respect to <span class="math inline">w</span>:</li>
</ul>
<p><span class="math display">
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = \frac{\partial}{\partial w} [\frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )^2]
</span></p>
<ul>
<li>Partial derivatives are linear, so the derivative of a sum is the sum of the derivatives:</li>
</ul>
<p><span class="math display">
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = \frac{1}{N} \, \sum_{i=1}^{N} \frac{\partial}{\partial w} (t_i - y_i )^2
</span></p>
<ul>
<li>This means we can compute a gradient for each training example instead of for the whole training set (see later the distinction batch/online):</li>
</ul>
<p><span class="math display">
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = \frac{1}{N} \, \sum_{i=1}^{N} \frac{\partial}{\partial w} \mathcal{l}_i(w, b)
    \qquad \text{with} \qquad \mathcal{l}_i(w, b) = (t_i - y_i )^2
</span></p>
</section>

<section id="gradient-descent-for-linear-regression-2" class="title-slide slide level1 center">
<h1>Gradient descent for linear regression</h1>
<ul>
<li><p>The individual loss <span class="math inline">\mathcal{l}_i(w, b) = (t_i - y_i )^2</span> is the composition of two functions:</p>
<ul>
<li><p>a square error function <span class="math inline">g_i(y_i) = (t_i - y_i)^2</span>.</p></li>
<li><p>the prediction <span class="math inline">y_i = f_{w, b}(x_i) = w \, x_i + b</span>.</p></li>
</ul></li>
<li><p>The <strong>chain rule</strong> tells us how to derive such composite functions:</p></li>
</ul>
<p><span class="math display">
    \frac{ d f(g(x))}{dx} = \frac{ d f(g(x))}{d g(x)} \times \frac{ d g(x)}{dx} = \frac{ d f(y)}{dy} \times \frac{ d g(x)}{dx}
</span></p>
<ul>
<li><p>The first derivative considers <span class="math inline">g(x)</span> to be a single variable.</p></li>
<li><p>Applied to our problem, this gives:</p></li>
</ul>
<p><span class="math display">
     \frac{\partial}{\partial w} \mathcal{l}_i(w, b) =  \frac{\partial g_i(y_i)}{\partial y_i} \times  \frac{\partial y_i}{\partial w}
</span></p>
</section>

<section id="gradient-descent-for-linear-regression-3" class="title-slide slide level1 center">
<h1>Gradient descent for linear regression</h1>
<ul>
<li>The square error function <span class="math inline">g_i(y) = (t_i - y)^2</span> is easy to differentiate w.r.t <span class="math inline">y</span>:</li>
</ul>
<p><span class="math display">
    \frac{\partial g_i(y_i)}{\partial y_i} = - 2 \, (t_i - y_i)
</span></p>
<ul>
<li>The prediction <span class="math inline">y_i = w \, x_i + b</span> also w.r.t <span class="math inline">w</span> and <span class="math inline">b</span>:</li>
</ul>
<p><span class="math display">
   \frac{\partial  y_i}{\partial w} = x_i
</span></p>
<p><span class="math display">
   \frac{\partial  y_i}{\partial b} = 1
</span></p>
<ul>
<li>The partial derivative of the individual loss is:</li>
</ul>
<p><span class="math display">
    \frac{\partial \mathcal{l}_i(w, b)}{\partial w} = - 2 \, (t_i - y_i) \, x_i
</span></p>
<p><span class="math display">
    \frac{\partial \mathcal{l}_i(w, b)}{\partial b} = - 2 \, (t_i - y_i)
</span></p>
</section>

<section id="gradient-descent-for-linear-regression-4" class="title-slide slide level1 center">
<h1>Gradient descent for linear regression</h1>
<ul>
<li>This gives us:</li>
</ul>
<p><span class="math display">
    \frac{\partial \mathcal{L}(w, b)}{\partial w} = - \frac{2}{N} \sum_{i=1}^{N} (t_i - y_i) \, x_i
</span></p>
<p><span class="math display">
    \frac{\partial \mathcal{L}(w, b)}{\partial b} = - \frac{2}{N} \sum_{i=1}^{N} (t_i - y_i)
</span></p>
<ul>
<li>Gradient descent is then defined by the learning rules (absorbing the 2 in <span class="math inline">\eta</span>):</li>
</ul>
<p><span class="math display">
    \Delta w = \eta \, \frac{1}{N} \sum_{i=1}^{N} (t_i - y_i) \, x_i
</span></p>
<p><span class="math display">
    \Delta b = \eta \, \frac{1}{N} \sum_{i=1}^{N} (t_i - y_i)
</span></p>
</section>

<section id="least-mean-squares-lms---ordinary-least-squares-ols" class="title-slide slide level1 center">
<h1>Least Mean Squares (LMS) - Ordinary Least Squares (OLS)</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>LMS is a <strong>batch</strong> algorithm: the parameter changes are computed over the whole dataset.</li>
</ul>
<p><span class="math display">
\begin{cases}
    \Delta w = \eta \, \dfrac{1}{N} \displaystyle\sum_{i=1}^{N} (t_i - y_i) \, x_i \\
    \\
    \Delta b = \eta \, \dfrac{1}{N} \displaystyle\sum_{i=1}^{N} (t_i - y_i) \\
\end{cases}
</span></p>
<ul>
<li><p>The parameter changes have to be applied multiple times (<strong>epochs</strong>) in order for the parameters to converge.</p></li>
<li><p>One can stop when the parameters do not change much, or after a fixed number of epochs.</p></li>
</ul>
</div><div class="column" style="width:50%;">
<div class="callout callout-tip callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Least Mean Squares algorithm</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p><span class="math inline">w=0 \quad;\quad b=0</span></p></li>
<li><p><strong>for</strong> M epochs:</p>
<ul>
<li><p><span class="math inline">dw=0 \quad;\quad db=0</span></p></li>
<li><p><strong>for</strong> each sample <span class="math inline">(x_i, t_i)</span>:</p>
<ul>
<li><p><span class="math inline">y_i = w \, x_i + b</span></p></li>
<li><p><span class="math inline">dw = dw + (t_i - y_i) \, x_i</span></p></li>
<li><p><span class="math inline">db = db + (t_i - y_i)</span></p></li>
</ul></li>
<li><p><span class="math inline">\Delta w = \eta \, \frac{1}{N} dw</span></p></li>
<li><p><span class="math inline">\Delta b = \eta \, \frac{1}{N} db</span></p></li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</section>

<section id="least-mean-squares-in-action" class="title-slide slide level1 center">
<h1>Least mean squares in action</h1>

<img data-src="img/regression-animation.gif" class="r-stretch quarto-figure-center"></section>

<section id="least-mean-squares" class="title-slide slide level1 center">
<h1>Least mean squares</h1>
<ul>
<li>During learning, the <strong>mean square error</strong> (mse) decreases with the number of epochs but does not reach zero because of the noise in the data.</li>
</ul>

<img data-src="img/regression-animation-loss.png" style="width:70.0%" class="r-stretch quarto-figure-center"></section>

<section id="delta-learning-rule-online-version-of-lms" class="title-slide slide level1 center">
<h1>Delta learning rule: Online version of LMS</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>LMS is very slow, because it changes the weights only after the whole training set has been evaluated.</li>
</ul>
<p><span class="math display">
\begin{cases}
    \Delta w = \eta \, \dfrac{1}{N} \displaystyle\sum_{i=1}^{N} (t_i - y_i) \, x_i \\
    \\
    \Delta b = \eta \, \dfrac{1}{N} \displaystyle\sum_{i=1}^{N} (t_i - y_i) \\
\end{cases}
</span></p>
</div><div class="column" style="width:50%;">
<div class="callout callout-tip callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Online version of LMS : delta learning rule</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p><span class="math inline">w=0 \quad;\quad b=0</span></p></li>
<li><p><strong>for</strong> M epochs:</p>
<ul>
<li><p><strong>for</strong> each sample <span class="math inline">(x_i, t_i)</span>:</p>
<ul>
<li><p><span class="math inline">y_i = w \, x_i + b</span></p></li>
<li><p><span class="math inline">\Delta w = \eta \, (t_i - y_i ) \, x_i</span></p></li>
<li><p><span class="math inline">\Delta b = \eta \, (t_i - y_i)</span></p></li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<ul>
<li>It is also possible to update the weights immediately after each example using the <strong>delta learning rule</strong>:</li>
</ul>
<p><span class="math display">
\begin{cases}
\Delta w = \eta \, (t_i - y_i) \, x_i \\
\\
\Delta b = \eta \, (t_i - y_i)\\
\end{cases}
</span></p>
<ul>
<li>The batch version is more stable, but the online version is faster: the weights have already learned something when arriving at the end of the first epoch.</li>
</ul>
</section>

<section id="delta-learning-rule-in-action-same-learning-rate" class="title-slide slide level1 center">
<h1>Delta learning rule in action (same learning rate!)</h1>

<img data-src="img/regression-animation-online.gif" class="r-stretch quarto-figure-center"></section>

<section id="delta-learning-rule" class="title-slide slide level1 center">
<h1>Delta learning rule</h1>

<img data-src="img/regression-animation-online-loss.png" class="r-stretch quarto-figure-center"></section>

<section id="multiple-linear-regression" class="title-slide slide level1 center">
<h1>2 - Multiple linear regression</h1>

</section>

<section id="multiple-linear-regression-1" class="title-slide slide level1 center">
<h1>Multiple linear regression</h1>
<ul>
<li>The key idea of linear regression (one input <span class="math inline">x</span>, one output <span class="math inline">y</span>) can be generalized to multiple inputs and outputs.</li>
</ul>
<div class="columns">
<div class="column" style="width:45%;">
<ul>
<li><p><strong>Multiple Linear Regression</strong> (MLR) predicts several output variables based on several explanatory variables or <strong>features</strong>: <span class="math display">
\begin{cases}
y_1 = w_1 \, x_1 + w_2 \, x_2 + b_1\\
\\
y_2 = w_3 \, x_1 + w_4 \, x_2 + b_2\\
\end{cases}
</span></p></li>
<li><p>All we have is some samples: we want to know the best model for the data.</p></li>
</ul>
</div><div class="column" style="width:55%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/multiple-linear-regression.jpg"></p>
</figure>
</div>
</div>
</div>
</section>

<section id="mlr-example-fuel-consumption-and-co2-emissions" class="title-slide slide level1 center">
<h1>MLR example: fuel consumption and CO2 emissions</h1>
<ul>
<li><p>Let’s suppose you have 13971 measurements in some Excel file, linking engine size, number of cylinders, fuel consumption and CO2 emissions of various cars.</p></li>
<li><p>You want to predict fuel consumption and CO2 emissions when you know the engine size and the number of cylinders.</p></li>
</ul>
<table>
<thead>
<tr class="header">
<th>Engine size</th>
<th>Cylinders</th>
<th>Fuel consumption</th>
<th>CO2 emissions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2</td>
<td>4</td>
<td>8.5</td>
<td>196</td>
</tr>
<tr class="even">
<td>2.4</td>
<td>4</td>
<td>9.6</td>
<td>221</td>
</tr>
<tr class="odd">
<td>1.5</td>
<td>4</td>
<td>5.9</td>
<td>136</td>
</tr>
<tr class="even">
<td>3.5</td>
<td>6</td>
<td>11</td>
<td>255</td>
</tr>
<tr class="odd">
<td>3.5</td>
<td>6</td>
<td>11</td>
<td>244</td>
</tr>
<tr class="even">
<td>3.5</td>
<td>6</td>
<td>10</td>
<td>230</td>
</tr>
<tr class="odd">
<td>3.5</td>
<td>6</td>
<td>10</td>
<td>232</td>
</tr>
<tr class="even">
<td>3.7</td>
<td>6</td>
<td>11</td>
<td>255</td>
</tr>
<tr class="odd">
<td>3.7</td>
<td>6</td>
<td>12</td>
<td>267</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
</tbody>
</table>
</section>

<section id="mlr-example-fuel-consumption-and-co2-emissions-1" class="title-slide slide level1 center">
<h1>MLR example: fuel consumption and CO2 emissions</h1>

<img data-src="img/MLR-example-data.png" style="width:65.0%" class="r-stretch quarto-figure-center"></section>

<section id="mlr-example-fuel-consumption-and-co2-emissions-2" class="title-slide slide level1 center">
<h1>MLR example: fuel consumption and CO2 emissions</h1>

<img data-src="img/MLR-example-data-3d.png" style="width:100.0%" class="r-stretch quarto-figure-center"></section>

<section id="mlr-example-fuel-consumption-and-co2-emissions-3" class="title-slide slide level1 center">
<h1>MLR example: fuel consumption and CO2 emissions</h1>
<ul>
<li>Noting the variables <span class="math inline">x_1</span>, <span class="math inline">x_2</span>, <span class="math inline">y_1</span>, <span class="math inline">y_2</span>, we can define our MLR problem:</li>
</ul>
<p><span class="math display">
\begin{cases}
y_1 = w_1 \, x_1 + w_2 \, x_2 + b_1\\
\\
y_2 = w_3 \, x_1 + w_4 \, x_2 + b_2\\
\end{cases}
</span></p>
<p>and use the least mean squares method to obtain the value of the parameters.</p>

<img data-src="img/MLR-example-fit-3d.png" style="width:70.0%" class="r-stretch quarto-figure-center"><ul>
<li>Note: using the Python library <code>scikit-learn</code> (<a href="https://scikit-learn.org" class="uri">https://scikit-learn.org</a>), this is done in two lines of code:</li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb1-2"><a href="#cb1-2"></a>reg <span class="op">=</span> LinearRegression().fit(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>

<section id="multiple-linear-regression-2" class="title-slide slide level1 center">
<h1>Multiple linear regression</h1>
<ul>
<li>The system of equations:</li>
</ul>
<p><span class="math display">
\begin{cases}
y_1 = w_1 \, x_1 + w_2 \, x_2 + b_1\\
\\
y_2 = w_3 \, x_1 + w_4 \, x_2 + b_2\\
\end{cases}
</span></p>
<p>can be put in a matrix-vector form:</p>
<p><span class="math display">
    \begin{bmatrix} y_1 \\ y_2 \\\end{bmatrix} = \begin{bmatrix} w_1 &amp; w_2 \\ w_3 &amp; w_4 \\\end{bmatrix} \times \begin{bmatrix} x_1 \\ x_2 \\\end{bmatrix} + \begin{bmatrix} b_1 \\ b_2 \\\end{bmatrix}
</span></p>
<ul>
<li>We simply create the corresponding vectors and matrices:</li>
</ul>
<p><span class="math display">
    \mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\\end{bmatrix} \qquad \mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\\end{bmatrix} \qquad \mathbf{t} = \begin{bmatrix} t_1 \\ t_2 \\\end{bmatrix} \qquad \mathbf{b} = \begin{bmatrix} b_1 \\ b_2 \\\end{bmatrix} \qquad W = \begin{bmatrix} w_1 &amp; w_2 \\ w_3 &amp; w_4 \\\end{bmatrix}
</span></p>
<ul>
<li><p><span class="math inline">\mathbf{x}</span> is the input vector, <span class="math inline">\mathbf{y}</span> is the output vector, <span class="math inline">\mathbf{t}</span> is the target vector.</p></li>
<li><p><span class="math inline">W</span> is called the <strong>weight matrix</strong> and <span class="math inline">\mathbf{b}</span> the <strong>bias vector</strong>.</p></li>
</ul>
<p><span class="math display">
    \mathbf{y} = f_{W, \mathbf{b}}(\mathbf{x}) = W \times \mathbf{x} + \mathbf{b}
</span></p>
</section>

<section id="multiple-linear-regression-3" class="title-slide slide level1 center">
<h1>Multiple linear regression</h1>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/matrixvector.png" style="width:60.0%"></p>
</figure>
</div>
<div class="columns">
<div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/linearperceptron.svg"></p>
</figure>
</div>
</div><div class="column" style="width:60%;">
<ul>
<li>The model is now defined by:</li>
</ul>
<p><span class="math display">
    \mathbf{y} = f_{W, \mathbf{b}}(\mathbf{x}) = W \times \mathbf{x} + \mathbf{b}
</span></p>
<ul>
<li><p>The problem is exactly the same as before, except that we use vectors and matrices instead of scalars: <span class="math inline">\mathbf{x}</span> and <span class="math inline">\mathbf{y}</span> can have any number of dimensions, the same procedure will apply.</p></li>
<li><p>This corresponds to a <strong>linear neural network</strong> (or linear perceptron), with one <strong>output neuron</strong> per predicted value <span class="math inline">y_i</span> using the linear activation function.</p></li>
</ul>
</div>
</div>
</section>

<section id="multiple-linear-regression-4" class="title-slide slide level1 center">
<h1>Multiple linear regression</h1>
<ul>
<li>The mean square error still needs to be a scalar in order to be minimized. We can define it as the squared norm of the error <strong>vector</strong>:</li>
</ul>
<p><span class="math display">
    \min_{W, \mathbf{b}} \, \mathcal{L}(W, \mathbf{b}) = \mathbb{E}_\mathcal{D} [ ||\mathbf{t} - \mathbf{y}||^2 ] = \mathbb{E}_\mathcal{D} [ ((t_1 - y_1)^2 + (t_2 - y_2)^2) ]
</span></p>
<ul>
<li>In order to apply gradient descent, one needs to calculate partial derivatives w.r.t the weight matrix <span class="math inline">W</span> and the bias vector <span class="math inline">\mathbf{b}</span>, i.e.&nbsp;<strong>gradients</strong>:</li>
</ul>
<p><span class="math display">
    \begin{cases}
    \Delta W = - \eta \, \nabla_W \, \mathcal{L}(W, \mathbf{b}) \\
    \\
    \Delta \mathbf{b} = - \eta \, \nabla_\mathbf{b} \, \mathcal{L}(W, \mathbf{b}) \\
    \end{cases}
</span></p>
<ul>
<li>Some more advanced linear algebra becomes important to know how to compute these gradients:</li>
</ul>
<p><a href="https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf" class="uri">https://web.stanford.edu/class/cs224n/readings/gradient-notes.pdf</a></p>
</section>

<section id="multiple-linear-regression-5" class="title-slide slide level1 center">
<h1>Multiple linear regression</h1>
<ul>
<li>We search the minimum of the mse loss function:</li>
</ul>
<p><span class="math display">
    \min_{W, \mathbf{b}} \, \mathcal{L}(W, \mathbf{b}) = \mathbb{E}_\mathcal{D} [ ||\mathbf{t} - \mathbf{y}||^2 ] \approx \frac{1}{N} \, \sum_{i=1}^N ||\mathbf{t}_i - \mathbf{y}_i||^2 = \frac{1}{N} \, \sum_{i=1}^N \mathcal{l}_i(W, \mathbf{b})
</span></p>
<ul>
<li>The individual loss function <span class="math inline">\mathcal{l}_i(W, \mathbf{b})</span> is the squared <span class="math inline">\mathcal{L}^2</span>-norm of the error vector, what can be expressed as a dot product or a vector multiplication:</li>
</ul>
<p><span class="math display">
    \mathcal{l}_i(W, \mathbf{b}) = ||\mathbf{t}_i - \mathbf{y}_i||^2 = \langle \mathbf{t}_i - \mathbf{y}_i \cdot \mathbf{t}_i - \mathbf{y}_i \rangle = (\mathbf{t}_i - \mathbf{y}_i)^T \times (\mathbf{t}_i - \mathbf{y}_i)
</span></p>
<ul>
<li>Remember:</li>
</ul>
<p><span class="math display">\mathbf{x}^T \times \mathbf{x} = \begin{bmatrix} x_1 &amp; x_2 &amp; \ldots &amp; x_n \end{bmatrix} \times \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = x_1 \, x_1 + x_2 \, x_2 + \ldots + x_n \, x_n = \langle \mathbf{x} \cdot \mathbf{x} \rangle = ||\mathbf{x}||^2_2</span></p>
</section>

<section id="multiple-linear-regression-6" class="title-slide slide level1 center">
<h1>Multiple linear regression</h1>
<ul>
<li>The chain rule tells us in principle that:</li>
</ul>
<p><span class="math display">\nabla_{W} \, \mathcal{l}_i(W, \mathbf{b}) = \nabla_{\mathbf{y}_i} \, \mathcal{l}_i(W, \mathbf{b}) \times \nabla_{W} \, \mathbf{y}_i</span></p>
<ul>
<li>The gradient w.r.t the output vector <span class="math inline">\mathbf{y}_i</span> is quite easy to obtain, as it a quadratic function of <span class="math inline">\mathbf{t}_i - \mathbf{y}_i</span>:</li>
</ul>
<p><span class="math display">\nabla_{\mathbf{y}_i} \, \mathcal{l}_i(W, \mathbf{b}) = \nabla_{\mathbf{y}_i} \, (\mathbf{t}_i - \mathbf{y}_i)^T \times (\mathbf{t}_i - \mathbf{y}_i)</span></p>
<ul>
<li>The proof relies on product differentiation <span class="math inline">(f\times g)' = f' \, g + f \, g'</span>:</li>
</ul>
<p><span class="math display">\begin{aligned}
    \nabla_{\mathbf{y}_i} \, (\mathbf{t}_i - \mathbf{y}_i)^T \times (\mathbf{t}_i - \mathbf{y}_i) &amp; = ( \nabla_{\mathbf{y}_i} \, (\mathbf{t}_i - \mathbf{y}_i) ) \times (\mathbf{t}_i - \mathbf{y}_i) + (\mathbf{t}_i - \mathbf{y}_i) \times \nabla_{\mathbf{y}_i} \, (\mathbf{t}_i - \mathbf{y}_i)  \\
    &amp;\\
    &amp;= - (\mathbf{t}_i - \mathbf{y}_i) - (\mathbf{t}_i - \mathbf{y}_i) \\
    &amp;\\
    &amp;= - 2 \, (\mathbf{t}_i - \mathbf{y}_i) \\
\end{aligned}
</span></p>
<p><strong>Note:</strong> We use the properties <span class="math inline">\nabla_{\mathbf{x}}\, \mathbf{x}^T \times \mathbf{z} = \mathbf{z}</span> and <span class="math inline">\nabla_{\mathbf{z}} \, \mathbf{x}^T \times \mathbf{z} = \mathbf{x}</span> to get rid of the transpose.</p>
</section>

<section id="multiple-linear-regression-7" class="title-slide slide level1 center">
<h1>Multiple linear regression</h1>

<img data-src="img/matrixvector.png" style="width:60.0%" class="r-stretch quarto-figure-center"><ul>
<li><p>The “problem” is when computing <span class="math inline">\nabla_{W} \, \mathbf{y}_i = \nabla_{W} \, (W \times \mathbf{x}_i + \mathbf{b})</span>:</p>
<ul>
<li><p><span class="math inline">\mathbf{y}_i</span> is a vector and <span class="math inline">W</span> a matrix.</p></li>
<li><p><span class="math inline">\nabla_{W} \, \mathbf{y}_i</span> is then a Jacobian (matrix), not a gradient (vector).</p></li>
</ul></li>
<li><p>Intuitively, differentiating <span class="math inline">W \times \mathbf{x}_i + \mathbf{b}</span> w.r.t <span class="math inline">W</span> should return <span class="math inline">\mathbf{x}_i</span>, but it is a vector, not a matrix…</p></li>
<li><p>The gradient (or Jacobian) of <span class="math inline">\mathcal{l}_i(W, \mathbf{b})</span> w.r.t <span class="math inline">W</span> should be a matrix of the same size as <span class="math inline">W</span> so that we can apply gradient descent:</p></li>
</ul>
<p><span class="math display">\Delta W = - \eta \, \nabla_W \, \mathcal{L}(W, \mathbf{b})</span></p>
</section>

<section id="multiple-linear-regression-8" class="title-slide slide level1 center">
<h1>Multiple linear regression</h1>
<ul>
<li>We already know that:</li>
</ul>
<p><span class="math display">\nabla_{W} \, \mathcal{l}_i(W, \mathbf{b}) = - 2\, (\mathbf{t}_i - \mathbf{y}_i) \times \nabla_{W} \, \mathbf{y}_i</span></p>
<ul>
<li><p>If <span class="math inline">\mathbf{x}_i</span> has <span class="math inline">n</span> elements and <span class="math inline">\mathbf{y}_i</span> <span class="math inline">m</span> elements, <span class="math inline">W</span> is a <span class="math inline">m \times n</span> matrix.</p></li>
<li><p>Remember the outer product between two vectors:</p></li>
</ul>
<p><span class="math display">
\mathbf{u} \times \mathbf{v}^\textsf{T} =
  \begin{bmatrix}u_1 \\ u_2 \\ u_3 \\ u_4\end{bmatrix}
    \begin{bmatrix}v_1 &amp; v_2 &amp; v_3\end{bmatrix} =
  \begin{bmatrix}
    u_1v_1 &amp; u_1v_2 &amp; u_1v_3 \\
    u_2v_1 &amp; u_2v_2 &amp; u_2v_3 \\
    u_3v_1 &amp; u_3v_2 &amp; u_3v_3 \\
    u_4v_1 &amp; u_4v_2 &amp; u_4v_3
  \end{bmatrix}.
</span></p>
<ul>
<li>It is easy to see that the outer product between <span class="math inline">(\mathbf{t}_i - \mathbf{y}_i)</span> and <span class="math inline">\mathbf{x}_i</span> gives a <span class="math inline">m \times n</span> matrix:</li>
</ul>
<p><span class="math display">
    \nabla_W \, \mathcal{l}_i(W, \mathbf{b}) = - 2 \, (\mathbf{t}_i - \mathbf{y}_i) \times \mathbf{x}_i^T\\
</span></p>
</section>

<section id="example" class="title-slide slide level1 center">
<h1>Example</h1>
<ul>
<li>Let’s prove it element per element:</li>
</ul>
<p><span class="math display">
    \mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\\end{bmatrix} = W \times \mathbf{x} + \mathbf{b} = \begin{bmatrix} w_1 &amp; w_2 \\ w_3 &amp; w_4 \\\end{bmatrix} \times \begin{bmatrix} x_1 \\ x_2 \\\end{bmatrix} + \begin{bmatrix} b_1 \\ b_2 \\\end{bmatrix}
</span></p>
<p><span class="math display">
\mathcal{l}(W, \mathbf{b}) = (\mathbf{t} - \mathbf{y})^T \times (\mathbf{t} - \mathbf{y}) = \begin{bmatrix} t_1 - y_1 &amp; t_2 - y_2 \\\end{bmatrix} \times \begin{bmatrix} t_1 - y_1 \\ t_2 - y_2 \\\end{bmatrix} = (t_1 - y_1)^2 + (t_2 - y_2)^2
</span></p>
<ul>
<li>The Jacobian w.r.t <span class="math inline">W</span> can be explicitly formed using partial derivatives:</li>
</ul>
<p><span class="math display">
\nabla_W \, \mathcal{l}(W, \mathbf{b}) =
    \begin{bmatrix}
        \dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial w_1} &amp; \dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial w_2} \\
        \\
        \dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial w_3} &amp; \dfrac{\partial \mathcal{l}(W, \mathbf{b})}{\partial w_4} \\
    \end{bmatrix}
    = \begin{bmatrix}
        -2 \, (t_1 - y_1) \, x_1 &amp; -2 \, (t_1 - y_1) \, x_2 \\
        \\
        -2 \, (t_2 - y_2) \, x_1 &amp; -2 \, (t_2 - y_2) \, x_2 \\
    \end{bmatrix}
</span></p>
<ul>
<li>We can rearrange this matrix as an outer product:</li>
</ul>
<p><span class="math display">
\nabla_W \, \mathcal{l}(W, \mathbf{b}) = -2 \, \begin{bmatrix}
t_1 - y_1  \\  t_2 - y_2 \\
\end{bmatrix} \times \begin{bmatrix}
x_1 &amp; x_2 \\
\end{bmatrix}
= - 2 \, (\mathbf{t} - \mathbf{y}) \times \mathbf{x}^T
</span></p>
</section>

<section id="multiple-linear-regression-9" class="title-slide slide level1 center">
<h1>Multiple linear regression</h1>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Batch version (<strong>least mean squares</strong>):</li>
</ul>
<p><span class="math display">\begin{cases}
    \Delta W = \eta \, \dfrac{1}{N} \displaystyle\sum_{i=1}^N \, (\mathbf{t}_i - \mathbf{y}_i ) \times \mathbf{x}_i^T \\
    \\
    \Delta \mathbf{b} = \eta \, \dfrac{1}{N} \displaystyle\sum_{i=1}^N \, (\mathbf{t}_i - \mathbf{y}_i) \\
\end{cases}</span></p>
<ul>
<li>Online version (<strong>delta learning rule</strong>):</li>
</ul>
<p><span class="math display">\begin{cases}
    \Delta W = \eta \, (\mathbf{t}_i - \mathbf{y}_i ) \times \mathbf{x}_i^T \\
    \\
    \Delta \mathbf{b} = \eta \, (\mathbf{t}_i - \mathbf{y}_i) \\
\end{cases}</span></p>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/linearperceptron.svg" style="width:50.0%"></p>
</figure>
</div>
<ul>
<li>This is completely equivalent to having one learning rule per parameter:</li>
</ul>
<p><span class="math display">
\begin{cases}
    \Delta w_1 = \eta \, (t_1 - y_1) \, x_1 \\
    \Delta w_2 = \eta \, (t_1 - y_1) \, x_2 \\
    \Delta w_3 = \eta \, (t_2 - y_2) \, x_1 \\
    \Delta w_4 = \eta \, (t_2 - y_2) \, x_2 \\
\end{cases}
\qquad
\begin{cases}
    \Delta b_1 = \eta \, (t_1 - y_1) \\
    \Delta b_2 = \eta \, (t_2 - y_2) \\
\end{cases}
</span></p>
</div>
</div>
<ul>
<li>The delta learning rule is always of the form: <span class="math inline">\Delta w</span> = eta <span class="math inline">\times</span> error <span class="math inline">\times</span> input. Biases have an input of 1.</li>
</ul>
</section>

<section id="logistic-regression" class="title-slide slide level1 center">
<h1>3 - Logistic regression</h1>

</section>

<section id="logistic-regression-1" class="title-slide slide level1 center">
<h1>Logistic regression</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/sigmoid.png" style="width:70.0%"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li><p>Let’s suppose we want to perform a regression, but where the outputs <span class="math inline">t_i</span> are bounded between 0 and 1.</p></li>
<li><p>We could use a logistic (or sigmoid) function instead of a linear function in order to transform the input into an output:</p></li>
</ul>
<p><span class="math display">
    y = \sigma(w \, x + b )  = \frac{1}{1+\exp(-w \, x - b )}
</span></p>
</div>
</div>
<ul>
<li>The logistic function</li>
</ul>
<p><span class="math display">
    \sigma(x)=\frac{1}{1+\exp(-x)}
</span></p>
<p>has the nice property that</p>
<p><span class="math display">
    \sigma'(x) = \sigma(x) \, (1 - \sigma(x) )
</span></p>
</section>

<section id="logistic-regression-2" class="title-slide slide level1 center">
<h1>Logistic regression</h1>
<ul>
<li>We can perform a logistic regression with the same online LMS method as in the linear case:</li>
</ul>
<p><span class="math display">l_i(w, b) = (t_i - \sigma(w \, x_i + b) )^2 </span></p>
<ul>
<li>The partial derivative of the individual loss is easy to find using the chain rule:</li>
</ul>
<p><span class="math display">
\begin{aligned}
    \frac{\partial l_i(w, b)}{\partial w}
        &amp;= 2 \, (t_i - y_i)  \, \frac{\partial}{\partial w}  (t_i - \sigma(w \, x_i + b ))\\
        &amp;\\
        &amp;= - 2 \, (t_i - y_i) \, \sigma'(w \, x_i + b ) \,  x_i \\
\end{aligned}
</span></p>
<ul>
<li>The non-linear transfer function <span class="math inline">\sigma(x)</span> adds its derivative into the gradient:</li>
</ul>
<p><span class="math display">
    \Delta w = \eta \, (t_i - y_i) \, \sigma'(w \, x_i + b ) \, x_i
</span></p>
<ul>
<li>With the property <span class="math inline">\sigma'(x) = \sigma(x) \, (1 - \sigma(x) )</span>, it even becomes:</li>
</ul>
<p><span class="math display">
    \Delta w = \eta \, (t_i - y_i) \, y_i \, ( 1 - y_i ) \, x_i
</span></p>
<p>so we do not even need to compute the derivative!</p>
</section>

<section id="logistic-regression-3" class="title-slide slide level1 center">
<h1>Logistic regression</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/artificialneuron.svg"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>Model:</li>
</ul>
<p><span class="math display">
    y = \sigma(w \, x + b )  = \frac{1}{1+\exp(-w \, x - b )}
</span></p>
<ul>
<li>The delta learning rule in case of logistic regression is:</li>
</ul>
<p><span class="math display">
\begin{cases}
    \Delta w = \eta \, (t_i - y_i) \, y_i \, ( 1 - y_i ) \, x_i \\
\\
    \Delta b = \eta \, (t_i - y_i) \, y_i \, ( 1 - y_i ) \\
\end{cases}
</span></p>
</div>
</div>
</section>

<section id="generalized-form-of-the-delta-learning-rule" class="title-slide slide level1 center">
<h1>Generalized form of the delta learning rule</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/linearperceptron.svg" style="width:90.0%"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>Model:</li>
</ul>
<p><span class="math display">
    \mathbf{y} = f(W \times \mathbf{x} + \mathbf{b} )  
</span></p>
<ul>
<li>Loss function (mse):</li>
</ul>
<p><span class="math display">
    \mathcal{L}(W, \mathbf{b}) = \mathbb{E}_{\mathbf{x}, \mathbf{t} \in \mathcal{D}}[||\mathbf{t} - \mathbf{y}||^2]
</span></p>
<ul>
<li>Delta learning rule:</li>
</ul>
<p><span class="math display">
\begin{cases}
    \Delta W = \eta \, [(\mathbf{t} - \mathbf{y}) \odot f'(W \times \mathbf{x} + \mathbf{b}) ] \times \mathbf{x}^T \\
\\
    \Delta \mathbf{b} = \eta \, (\mathbf{t} - \mathbf{y}) \odot f'(W \times \mathbf{x} + \mathbf{b}) \\
\end{cases}
</span></p>
</div>
</div>
<ul>
<li><p><span class="math inline">\odot</span> denotes element-wise multiplication, i.e.&nbsp;<span class="math inline">(\mathbf{t} - \mathbf{y}) \odot f'(W \times \mathbf{x} + \mathbf{b})</span> is also a vector.</p></li>
<li><p>In the linear case, <span class="math inline">f'(x) = 1</span>.</p></li>
<li><p>One can use any non-linear function, e.g hyperbolic tangent tanh(), ReLU, etc.</p></li>
<li><p>Transfer functions are chosen for neural networks so that we can compute their derivative easily.</p></li>
</ul>
</section>

<section id="polynomial-regression" class="title-slide slide level1 center">
<h1>4 - Polynomial regression</h1>

</section>

<section id="polynomial-regression-1" class="title-slide slide level1 center">
<h1>Polynomial regression</h1>

<img data-src="img/polynomialregression.png" style="width:50.0%" class="r-stretch quarto-figure-center"><ul>
<li><p>The functions underlying real data are rarely linear plus some noise around the ideal value.</p></li>
<li><p>In the figure, the input/output function would be better modeled by a second-order polynomial (or higher):</p></li>
</ul>
<p><span class="math display">y = f_{\mathbf{w}, b}(x) = w_1 \, x + w_2 \, x^2 +b</span></p>
</section>

<section id="polynomial-regression-2" class="title-slide slide level1 center">
<h1>Polynomial regression</h1>
<ul>
<li>Model:</li>
</ul>
<p><span class="math display">y = f_{\mathbf{w}, b}(x) = w_1 \, x + w_2 \, x^2 +b</span></p>
<ul>
<li>We can transform the input into a vector of coordinates:</li>
</ul>
<p><span class="math display">\mathbf{x} = \begin{bmatrix} x \\ x^2 \\ \end{bmatrix} \qquad \mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \end{bmatrix}</span></p>
<ul>
<li>The problem becomes:</li>
</ul>
<p><span class="math display">y = \langle \mathbf{w} . \mathbf{x} \rangle + b = \sum_j w_j \, x_j + b</span></p>
<ul>
<li>We can simply apply multiple linear regression (MLR) to find <span class="math inline">\mathbf{w}</span> and b:</li>
</ul>
<p><span class="math display">
\begin{cases}
\Delta \mathbf{w} =  \eta \, (t - y) \, \mathbf{x} \\
\\
\Delta b =  \eta \, (t - y) \\
\end{cases}
</span></p>
</section>

<section id="polynomial-regression-3" class="title-slide slide level1 center">
<h1>Polynomial regression</h1>
<ul>
<li>This generalizes to polynomials of any order <span class="math inline">p</span>:</li>
</ul>
<p><span class="math display">y = f_{\mathbf{w}, b}(x) = w_1 \, x + w_2 \, x^2 + \ldots + w_p \, x^p + b</span></p>
<ul>
<li>We create a vector of powers of <span class="math inline">x</span> (called <strong>polynomial features</strong>):</li>
</ul>
<p><span class="math display">\mathbf{x} = \begin{bmatrix} x \\ x^2 \\ \ldots \\ x^p \end{bmatrix} \qquad \mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \ldots \\ w_p \end{bmatrix}</span></p>
<ul>
<li>And apply multiple linear regression (MLR) to find <span class="math inline">\mathbf{w}</span> and b:</li>
</ul>
<p><span class="math display">
\begin{cases}
\Delta \mathbf{w} =  \eta \, (t - y) \, \mathbf{x} \\
\\
\Delta b =  \eta \, (t - y) \\
\end{cases}
</span></p>
<ul>
<li><p>Non-linear problem solved! The only unknown is which order for the polynomial matches best the data.</p></li>
<li><p>One can perform regression with any kind of parameterized function using gradient descent.</p></li>
</ul>
</section>

<section id="a-bit-of-learning-theory" class="title-slide slide level1 center">
<h1>5 - A bit of learning theory</h1>

</section>

<section id="what-matters-during-training" class="title-slide slide level1 center">
<h1>What matters during training?</h1>
<ul>
<li>Before going further, let’s think about what we have been doing so far. We had a bunch of data samples <span class="math inline">\mathcal{D} = (x_i, t_i)_{i=1..N}</span> (the <strong>training set</strong>) and we decided to apply a (linear) model on it:</li>
</ul>
<p><span class="math display">y_i = w \, x_i + b</span></p>
<ul>
<li>We then minimized the mean square error (mse) on that training set using gradient descent. At the end of learning, we can measure the <strong>residual error</strong> of the model on the data:</li>
</ul>
<p><span class="math display">
    \epsilon_\mathcal{D} = \frac{1}{N} \, \sum_{i=1}^{N} (t_i - y_i )^2
</span></p>
<ul>
<li>We get a number, for example 0.04567. Is that good?</li>
</ul>

<img data-src="img/regression-animation-mse-dual.png" style="width:70.0%" class="r-stretch quarto-figure-center"></section>

<section id="regression-error" class="title-slide slide level1 center">
<h1>Regression error</h1>
<ul>
<li>The <strong>mean square error</strong> mse is not very informative, as its value depends on how the outputs are scaled:</li>
</ul>
<p><span class="math display">
    \epsilon_\mathcal{D} = \frac{1}{N} \, \sum_{i=1}^N (t_i - y_i)^2
</span></p>
<ul>
<li>If you multiply both the data <span class="math inline">t</span> and the prediction <span class="math inline">y</span> by 10, the residual error will be 100 times higher, without any change to the quality of the model.</li>
</ul>

<img data-src="img/regression-animation-mse-dual.png" style="width:90.0%" class="r-stretch quarto-figure-center"></section>

<section id="coefficient-of-determination" class="title-slide slide level1 center">
<h1>Coefficient of determination</h1>
<ul>
<li>The <strong>coefficient of determination</strong> <span class="math inline">R^2</span> is a rescaled variant of the mse comparing the variance of the residuals to the variance of the data around its mean <span class="math inline">\hat{t}</span>:</li>
</ul>
<p><span class="math display">
    R^2 = 1 - \frac{\text{Var}(\text{residuals})}{\text{Var}(\text{data})} = 1 - \frac{\sum_{i=1}^N (t_i- y_i)^2}{\sum_{i=1}^N (t_i - \hat{t})^2}
</span></p>
<ul>
<li><span class="math inline">R^2</span> should be as close from 1 as possible. For example, if <span class="math inline">R^2 = 0.8</span>, we can say that the <strong>model explains 80% of the variance of the data</strong>.</li>
</ul>

<img data-src="img/r2.png" style="width:60.0%" class="r-stretch quarto-figure-center"><div class="footer">
<p>Source: <a href="https://towardsdatascience.com/introduction-to-linear-regression-in-python-c12a072bedf0" class="uri">https://towardsdatascience.com/introduction-to-linear-regression-in-python-c12a072bedf0</a></p>
</div>
</section>

<section id="sensibility-to-outliers" class="title-slide slide level1 center">
<h1>Sensibility to outliers</h1>
<ul>
<li>Suppose we have a training set with one <strong>outlier</strong> (bad measurement, bad luck, etc).</li>
</ul>

<img data-src="img/regression-outlier.png" class="r-stretch quarto-figure-center"></section>

<section id="sensibility-to-outliers-1" class="title-slide slide level1 center">
<h1>Sensibility to outliers</h1>
<ul>
<li>LMS would find the minimum of the mse, but it is clearly a bad fit for most points.</li>
</ul>

<img data-src="img/regression-outlier-fit.png" class="r-stretch quarto-figure-center"></section>

<section id="sensibility-to-outliers-2" class="title-slide slide level1 center">
<h1>Sensibility to outliers</h1>
<ul>
<li>This model feels much better, but its residual mse is higher…</li>
</ul>

<img data-src="img/regression-outlier-fit-corrected.png" class="r-stretch quarto-figure-center"></section>

<section id="polynomial-regression-4" class="title-slide slide level1 center">
<h1>Polynomial regression</h1>

<img data-src="img/polynomialregression-animation.gif" class="r-stretch quarto-figure-center"></section>

<section id="polynomial-regression-5" class="title-slide slide level1 center">
<h1>Polynomial regression</h1>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/polynomialregression-mse.png"></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<ul>
<li><p>When only looking at the residual mse on the training data, one could think that the higher the order of the polynomial, the better.</p></li>
<li><p>But it is obvious that the interpolation quickly becomes very bad when the order is too high.</p></li>
<li><p>A <strong>complex</strong> model (with a lot of parameters) is useless for predicting new values.</p></li>
<li><p>We actually do <strong>not</strong> care about the error on the training set.</p></li>
<li><p>We care about <strong>generalization</strong>.</p></li>
</ul>
</div>
</div>
</section>

<section id="cross-validation" class="title-slide slide level1 center">
<h1>Cross-validation</h1>
<ul>
<li><p>Let’s suppose we dispose of <span class="math inline">m</span> models <span class="math inline">\mathcal{M} = \{ M_1, ..., M_m\}</span> that could be used to fit (or classify) some data <span class="math inline">\mathcal{D} = \{x_i, t_i\}_{i=1}^N</span>.</p></li>
<li><p>Such a class could be the ensemble of polynomes with different orders, different algorithms (NN, SVM) or the same algorithm with different values for the hyperparameters (learning rate, regularization parameters…).</p></li>
<li><p>The naive and <strong>wrong</strong> method to find the best hypothesis would be:</p></li>
</ul>
<div class="callout callout-warning callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Wrong method!</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p>For all models <span class="math inline">M_i</span>:</p>
<ul>
<li><p>Train <span class="math inline">M_i</span> on <span class="math inline">\mathcal{D}</span> to obtain an hypothesis <span class="math inline">h_i</span>.</p></li>
<li><p>Compute the training error <span class="math inline">\epsilon_\mathcal{D}(h_i)</span> of <span class="math inline">h_i</span> on <span class="math inline">\mathcal{D}</span> :</p></li>
</ul>
<p><span class="math display">
      \epsilon_\mathcal{D}(h_i) =  \mathbb{E}_{(\mathbf{x}, t) \in \mathcal{D}} [(h_i(\mathbf{x}) - t)^2]
  </span></p></li>
<li><p>Select the hypothesis <span class="math inline">h_{i}^*</span> with the minimal training error : <span class="math inline">h_{i}^* = \text{argmin}_{h_i \in \mathcal{M}} \quad \epsilon_\mathcal{D}(h_i)</span></p></li>
</ul>
</div>
</div>
</div>
<ul>
<li>This method leads to <strong>overfitting</strong>, as only the training error is used.</li>
</ul>
</section>

<section id="cross-validation-training-and-test-sets" class="title-slide slide level1 center">
<h1>Cross-validation: training and test sets</h1>
<div class="columns">
<div class="column" style="width:60%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/polynomialregression-traintest.png"></p>
</figure>
</div>
</div><div class="column" style="width:40%;">
<ul>
<li><p>The solution is randomly take some samples out of the training set to form the <strong>test set</strong>.</p></li>
<li><p>Typical values are 20 or 30 % of the samples in the test set.</p></li>
<li><p>Method:</p>
<ol type="1">
<li><p>Train the model on the training set (70% of the data).</p></li>
<li><p>Test the performance of the model on the test set (30% of the data).</p></li>
</ol></li>
<li><p>The test performance will better measure how well the model generalizes to new examples.</p></li>
</ul>
</div>
</div>
</section>

<section id="simple-hold-out-cross-validation" class="title-slide slide level1 center">
<h1>Simple hold-out cross-validation</h1>
<div class="callout callout-tip callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Algorithm</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p>Split the training data <span class="math inline">\mathcal{D}</span> into <span class="math inline">\mathcal{S}_{\text{train}}</span> and <span class="math inline">\mathcal{S}_{\text{test}}</span>.</p></li>
<li><p>For all models <span class="math inline">M_i</span>:</p>
<ul>
<li><p>Train <span class="math inline">M_i</span> on <span class="math inline">\mathcal{S}_{\text{train}}</span> to obtain an hypothesis <span class="math inline">h_i</span>.</p></li>
<li><p>Compute the empirical error <span class="math inline">\epsilon_{\text{test}}(h_i)</span> of <span class="math inline">h_i</span> on <span class="math inline">\mathcal{S}_{\text{test}}</span> :</p></li>
</ul>
<p><span class="math display">\epsilon_{\text{test}}(h_i) = \mathbb{E}_{(\mathbf{x}, t) \in  \mathcal{S}_{\text{test}}} [(h_i(\mathbf{x}) - t)^2]</span></p></li>
<li><p>Select the hypothesis <span class="math inline">h_{i}^*</span> with the minimal empirical error : <span class="math inline">h_{i}^* = \text{argmin}_{h_i \in \mathcal{M}} \quad \epsilon_{\text{test}}(h_i)</span></p></li>
</ul>
</div>
</div>
</div>
<ul>
<li><p>Disadvantages:</p>
<ul>
<li><p>20 or 30% of the data is wasted and not used for learning. It may be a problem when data is rare or expensive.</p></li>
<li><p>The test set must be representative of the difficulty of the training set (same distribution).</p></li>
</ul></li>
</ul>
</section>

<section id="k-fold-cross-validation" class="title-slide slide level1 center">
<h1>k-fold cross-validation</h1>
<ul>
<li><p><strong>Idea:</strong></p>
<ul>
<li><p>build several different training/test sets with the same data.</p></li>
<li><p>train and test each model repeatedly on each partition.</p></li>
<li><p>choose the hypothesis that works best on average.</p></li>
</ul></li>
</ul>
<div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/kfold.jpg" style="width:70.0%"></p>
</figure>
</div>
</div>
<div class="footer">
<p>Source <a href="https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg" class="uri">https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg</a></p>
</div>
</section>

<section id="k-fold-cross-validation-1" class="title-slide slide level1 center">
<h1>k-fold cross-validation</h1>
<div class="callout callout-tip callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Algorithm</strong></p>
</div>
<div class="callout-content">
<ul>
<li><p>Randomly split the data <span class="math inline">\mathcal{D}</span> into <span class="math inline">k</span> subsets of <span class="math inline">\frac{N}{k}</span> examples <span class="math inline">\{ \mathcal{S}_{1}, \dots , \mathcal{S}_{k}\}</span></p></li>
<li><p>For all models <span class="math inline">M_i</span>:</p>
<ul>
<li><p>For all <span class="math inline">k</span> subsets <span class="math inline">\mathcal{S}_j</span>:</p>
<ul>
<li><p>Train <span class="math inline">M_i</span> on <span class="math inline">\mathcal{D} - \mathcal{S}_j</span> to obtain an hypothesis <span class="math inline">h_{ij}</span></p></li>
<li><p>Compute the empirical error <span class="math inline">\epsilon_{\mathcal{S}_j}(h_{ij})</span> of <span class="math inline">h_{ij}</span> on <span class="math inline">\mathcal{S}_j</span></p></li>
</ul></li>
<li><p>The empirical error of the model <span class="math inline">M_i</span> on <span class="math inline">\mathcal{D}</span> is the average of empirical errors made on <span class="math inline">(\mathcal{S}_j)_{j=1}^{k}</span></p>
<p><span class="math display">
      \epsilon_{\mathcal{D}} (M_i) = \frac{1}{k} \cdot \sum_{j=1}^{k} \epsilon_{\mathcal{S}_j}(h_{ij})
  </span></p></li>
</ul></li>
<li><p>Select the model <span class="math inline">M_{i}^*</span> with the minimal empirical error on <span class="math inline">\mathcal{D}</span>.</p></li>
</ul>
</div>
</div>
</div>
<ul>
<li><p>In general <span class="math inline">k=10</span>. Extreme cases take <span class="math inline">k=N</span>: <strong>leave-one-out cross-validation</strong>.</p></li>
<li><p>k-fold cross-validation works well, but needs a lot of repeated learning.</p></li>
</ul>
</section>

<section id="training-and-test-errors" class="title-slide slide level1 center">
<h1>Training and test errors</h1>
<div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/polynomialregression-mse-traintest.png" style="width:70.0%"></p>
</figure>
</div>
</div>
<ul>
<li><p>While the training mse always decrease with more complex models, the test mse increases after a while.</p></li>
<li><p>This is called <strong>overfitting</strong>: learning by heart the data without caring about generalization.</p></li>
<li><p>The two curves suggest that we should chose a polynomial order between 2 and 9.</p></li>
</ul>
</section>

<section id="underfitting-overfitting" class="title-slide slide level1 center">
<h1>Underfitting / Overfitting</h1>

<img data-src="img/underfitting-overfitting.png" class="r-stretch quarto-figure-center"><ul>
<li><p>A model not complex enough for the data will <strong>underfit</strong>: its training error is high.</p></li>
<li><p>A model too complex for the data will <strong>overfit</strong>: its test error is high.</p></li>
<li><p>In between, there is the right complexity for the model: it learns the data correctly but does not overfit.</p></li>
</ul>
</section>

<section id="what-does-complexity-mean" class="title-slide slide level1 center">
<h1>What does complexity mean?</h1>
<ul>
<li>In polynomial regression, the complexity is related to the order of the polynomial, i.e.&nbsp;the number of coefficients to estimate:</li>
</ul>
<p><span class="math display">y = f_{\mathbf{w}, b}(x) = \sum_{k=1}^p w_k \, x^k + b</span></p>
<p><span class="math display">\mathbf{x} = \begin{bmatrix} x \\ x^2 \\ \ldots \\ x^p \end{bmatrix} \qquad \mathbf{w} = \begin{bmatrix} w_1 \\ w_2 \\ \ldots \\ w_p \end{bmatrix}</span></p>
<ul>
<li><p>A polynomial of order <span class="math inline">p</span> has <span class="math inline">p+1</span> unknown parameters (<strong>free parameters</strong>): the <span class="math inline">p</span> weights and the bias.</p></li>
<li><p>Generally, the <strong>complexity of a model</strong> relates to its <strong>number of free parameters</strong>:</p>
<ul>
<li><strong>The more free parameters, the more complex the model is, the more likely it will overfit.</strong></li>
</ul></li>
</ul>
<!--
# Bias - variance trade-off

::: {.columns}
::: {.column width=50%}


![](img/biasvariance3.png)

::: footer
<http://scott.fortmann-roe.com/docs/BiasVariance.html>
:::

:::
::: {.column width=50%}


* Under-/Over-fitting relates to the statistical concept of **bias-variance trade-off**.

* The **bias** is the training error that the hypothesis would make if the training set was infinite (accuracy, flexibility of the model).

    * A model with high bias is underfitting.

* The **variance** is the error that will be made by the hypothesis on new examples taken from the same distribution (spread, the model is correct on average, but not for individual samples).

    * A model with high variance is overfitting.

:::
:::


# Bias - variance trade-off

::: {.columns}
::: {.column width=50%}


![](img/biasvariance2.png)

::: footer
<http://scott.fortmann-roe.com/docs/BiasVariance.html>
:::

:::
::: {.column width=50%}


* The bias decreases when the model becomes complex.

* The variance increases when the model becomes complex.

* The **generalization error** is a combination of the bias and variance:

$$
    \text{generalization error} = \text{bias}^2 + \text{variance}
$$

:::
:::


* We search for the model with the **optimum complexity** realizing the trade-off between bias and variance.

* It is better to have a model with a slightly higher bias (training error) but with a smaller variance (generalization error).

-->
</section>

<section id="regularized-regression" class="title-slide slide level1 center">
<h1>6 - Regularized regression</h1>

</section>

<section id="linear-regression-can-either-underfit-or-overfit-depending-on-the-data" class="title-slide slide level1 center">
<h1>Linear regression can either underfit or overfit depending on the data</h1>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Underfitting</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/underfitting-overfitting-linear.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p><strong>Overfitting</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/regression-outlier-fit.png" style="width:85.0%"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><p>When linear regression <strong>underfits</strong> (both training and test errors are high), the data is not linear: we need to use a <strong>neural network</strong>.</p></li>
<li><p>When linear regression <strong>overfits</strong> (the test error is higher than the training error), we would like to <strong>decrease its complexity</strong>.</p></li>
</ul>
</section>

<section id="complexity-of-a-linear-regression" class="title-slide slide level1 center">
<h1>Complexity of a linear regression</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/artificialneuron.svg" style="width:80.0%"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>The problem is that the number of free parameters in linear regression only depends on the number of inputs (dimensions of the input space).</li>
</ul>
<p><span class="math display">
    y = \sum_{i=1}^d w_i \, x_i + b
</span></p>
<ul>
<li>For <span class="math inline">d</span> inputs, there are <span class="math inline">d+1</span> free parameters: the <span class="math inline">d</span> weights and the bias.</li>
</ul>
</div>
</div>
<ul>
<li><p>We must find a way to reduce the complexity of the linear regression without changing the number of parameters, which is impossible.</p></li>
<li><p>The solution is to <strong>constrain</strong> the values that the parameters can take: <strong>regularization</strong>.</p></li>
<li><p>Regularization reduces the variance at the cost of increasing the bias.</p></li>
</ul>
</section>

<section id="l2-regularization---ridge-regression" class="title-slide slide level1 center">
<h1>L2 regularization - Ridge regression</h1>
<ul>
<li><p>Using <strong>L2 regularization</strong> for linear regression leads to the <strong>Ridge regression</strong> algorithm.</p></li>
<li><p>The individual loss function is defined as:</p></li>
</ul>
<p><span class="math display">
    \mathcal{l}_i(\mathbf{w}, b) = (t_i - y_i)^2 + \lambda \, ||\mathbf{w}||^2
</span></p>
<ul>
<li><p>The first part of the loss function is the classical <strong>mse</strong> on the training set: its role is to reduce the <strong>bias</strong>.</p></li>
<li><p>The second part minimizes the L2 norm of the weight vector (or matrix), reducing the variance:</p></li>
</ul>
<p><span class="math display">
    ||\mathbf{w}||^2 = \sum_{i=1}^d w_i^2
</span></p>
<ul>
<li>Deriving the regularized delta learning rule is straightforward:</li>
</ul>
<p><span class="math display">
    \Delta w_i = \eta \, ((t_i - y_i) \ x_i - \lambda \, w_i)
</span></p>
<ul>
<li>Ridge regression is also called <strong>weight decay</strong>: even if there is no error, all weights will decay to 0.</li>
</ul>
</section>

<section id="l1-regularization---lasso-regression" class="title-slide slide level1 center">
<h1>L1 regularization - LASSO regression</h1>
<ul>
<li><p>Using <strong>L1 regularization</strong> for linear regression leads to the <strong>LASSO regression</strong> algorithm (least absolute shrinkage and selection operator).</p></li>
<li><p>The individual loss function is defined as:</p></li>
</ul>
<p><span class="math display">
    \mathcal{l}_i(\mathbf{w}, b) =  (t_i - y_i)^2 + \lambda \, |\mathbf{w}|
</span></p>
<ul>
<li>The second part minimizes this time the L1 norm of the weight vector, i.e.&nbsp;its absolute value:</li>
</ul>
<p><span class="math display">
    |\mathbf{w}| = \sum_{i=1}^d |w_i|
</span></p>
<ul>
<li>Regularized delta learning rule with LASSO:</li>
</ul>
<p><span class="math display">
    \Delta w_i = \eta \, ((t_i - y_i) \ x_i - \lambda \, \text{sign}(w_i))
</span></p>
<ul>
<li><strong>Weight decay</strong> does not depend on the value of the weight, only its sign. Weights can decay very fast to 0.</li>
</ul>
</section>

<section>
<section id="ridge-and-lasso-regression" class="title-slide slide level1 center">
<h1>Ridge and Lasso regression</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/ridge-effect.png"></p>
</figure>
</div>
<ul>
<li><strong>Ridge regression</strong> finds the smallest value for the weights that minimize the mse.</li>
</ul>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/lasso-effect.png"></p>
</figure>
</div>
<ul>
<li><strong>LASSO regression</strong> tries to set as many weight to 0 as possible (sparse code).</li>
</ul>
</div>
</div>
<ul>
<li><p>Both methods depend on the <strong>regularization parameter</strong> <span class="math inline">\lambda</span>. Its value determines how important the regularization term should.</p></li>
<li><p>Regularization introduce a <strong>bias</strong>, as the solution found is <strong>not</strong> the minimum of the mse, but reduces the variance of the estimation, as small weights are less sensible to noise.</p></li>
</ul>
<div class="footer">
<p><a href="https://www.mlalgorithms.org/articles/l1-l2-regression/" class="uri">https://www.mlalgorithms.org/articles/l1-l2-regression/</a></p>
</div>
</section>
<section class="slide level2">

<ul>
<li>LASSO allows <strong>feature selection</strong>: features with a zero weight can be removed from the training set.</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<p><strong>Linear regression</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/linearregression-withoutregularization.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p><strong>LASSO</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/linearregression-withregularization.png"></p>
</figure>
</div>
</div>
</div>
<div class="footer">
<p><a href="https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/" class="uri">https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/</a></p>
</div>
</section></section>
<section id="l1l2-regularization---elasticnet" class="title-slide slide level1 center">
<h1>L1+L2 regularization - ElasticNet</h1>
<div class="columns">
<div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/ridge-effect.png"></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="img/lasso-effect.png"></p>
</figure>
</div>
</div>
</div>
<ul>
<li>An <strong>ElasticNet</strong> is a linear regression using both L1 and L2 regression:</li>
</ul>
<p><span class="math display">
    \mathcal{l}_i(\mathbf{w}, b) =  (t_i - y_i)^2 + \lambda_1 \, |\mathbf{w}| + \lambda_2 \, ||\mathbf{w}||^2
</span></p>
<ul>
<li>It combines the advantages of Ridge and LASSO, at the cost of having now two regularization parameters to determine.</li>
</ul>
<div class="footer">
<p><a href="https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/" class="uri">https://www.analyticsvidhya.com/blog/2017/06/a-comprehensive-guide-for-linear-ridge-and-lasso-regression/</a></p>
</div>
<div class="footer footer-default">

</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="2.2-LinearRegression_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="2.2-LinearRegression_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="2.2-LinearRegression_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="2.2-LinearRegression_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="2.2-LinearRegression_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="2.2-LinearRegression_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="2.2-LinearRegression_files/libs/revealjs/plugin/reveal-pointer/pointer.js"></script>
  <script src="2.2-LinearRegression_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="2.2-LinearRegression_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="2.2-LinearRegression_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="2.2-LinearRegression_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, RevealPointer, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        setTimeout(function() {
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const cites = ref.parentNode.getAttribute('data-cites').split(' ');
        tippyHover(ref, function() {
          var popup = window.document.createElement('div');
          cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    });
    </script>
    

</body></html>